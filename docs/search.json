[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayesian Logistic Regression - Application in Probability Prediction of disease (Diabetes)",
    "section": "",
    "text": "Slides: slides.html ( Go to slides.qmd to edit)"
  },
  {
    "objectID": "index.html#aims",
    "href": "index.html#aims",
    "title": "Bayesian Logistic Regression - Application in Probability Prediction of disease (Diabetes)",
    "section": "Aims",
    "text": "Aims\nThe present study aims performs Bayesian logistic regression to predict diabetes status and evaluate the associations between diabetes and predictors (body mass index (BMI), age (≥20 years), gender, and race). The study anakyzes a retrospective dataset (2013–2014 NHANES survey data). It is based on a complex sampling design, characterized by stratification, clustering, and oversampling of specific population subgroups, rather than uniform random sampling. A Bayesian analytical approach addresses challenges posed by dataset anomalies such as missing data, complete case analysis, and separation that limit the efficiency and reliability of traditional logistic regression in predicting health outcomes."
  },
  {
    "objectID": "index.html#logical-flow-for-analysis",
    "href": "index.html#logical-flow-for-analysis",
    "title": "Bayesian Logistic Regression - Application in Probability Prediction of disease (Diabetes)",
    "section": "Logical Flow for Analysis",
    "text": "Logical Flow for Analysis\n\nData Preparation Preparation and analysis of the observed or imputed dataset (e.g., adult_imp1.csv) to handle missing values and ensure completeness. (2)Train-Test Split Splitting the dataset into training (to fit the model) and testing (to evaluate model performance) sets to ensures the model is generalizable and avoids overfitting.\nFrequentist Approach Fit classical regression or machine learning models on complete dataset to obtain point estimates (e.g., coefficients, odds ratios) and confidence intervals.\nBayesian Approach Fit a Bayesian model using on the imputed dataset to obtain posterior distributions (posterior draws) for parameters to quantify uncertainty. Use posterior draws to generate posterior predictive distributions on the complete data set.\nTargeted Intervention Analysis Compare the two models Use of Bayesian models to simulate or assess interventions Identify modifiable risk factors (e.g., BMI, lifestyle factors). Predict how change in a risk factor affects outcomes (e.g., diabetes risk). Bayesian analysis to quantify uncertainty in intervention effects using posterior predictive distributions.\nInference & Decision-Making Combining insights from both approaches to guide data-driven decisions or public health recommendations. Frequentist results provide point estimates, while Bayesian results provide full uncertainty quantification."
  },
  {
    "objectID": "index.html#statistical-tool",
    "href": "index.html#statistical-tool",
    "title": "Bayesian Logistic Regression - Application in Probability Prediction of disease (Diabetes)",
    "section": "Statistical Tool",
    "text": "Statistical Tool\n\nR, R packages and libraries are used to import data, perform data wrangling and analysis. ## Data source\nNHANES 2-year data (2013-2014) - a cross-sectional weighted data Center for Health Statistics (1999).\n\n\n\nCode\n# loading packages \noptions(repos = c(CRAN = \"https://cloud.r-project.org\"))\ninstall.packages(\"nhanesA\", dependencies = TRUE)\n\ninstall.packages(\"nhanesA\")\nlibrary (\"nhanesA\")    \nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(ggthemes)\nlibrary(ggrepel)\nlibrary(dslabs)\nlibrary(Hmisc)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(forcats)\nlibrary(ggplot2)\nlibrary(classpackage)\nlibrary(janitor)\ninstall.packages(\"gt\")   \nlibrary(gt)\nlibrary(survey)\nlibrary(DataExplorer)\nlibrary(logistf)"
  },
  {
    "objectID": "index.html#data-pre-processing-and-cleaning",
    "href": "index.html#data-pre-processing-and-cleaning",
    "title": "Bayesian Logistic Regression - Application in Probability Prediction of disease (Diabetes)",
    "section": "Data pre-processing and cleaning",
    "text": "Data pre-processing and cleaning\n\nThree datasets: demographics, exam, questionnaire in.XPT format files are imported (Haven package) in R. After selecting variables of interest, a merged dataset is created from the original weighted datasets (demographics, exam, questionnaire) and merged using ID to create a single merged dataframe.\n\n\nResponse Variable: Binary\n\n\nType 2 / diagnosed diabetes(excluding gestational diabetes) -“Doctor told you have diabetes?” DIQ010 combined with DIQ050 a secondary variable describing treatment status (insulin use) to exclude those cases\n\n\nPredictor Variables\n\n\nBody Mass Index, factor, 4 levels are analyzed after standardization).\no Underweight (&lt;5th percentile)\no Normal (5th–&lt;85th)\no Overweight (85th–&lt;95th) o Obese (≥95th percentile)\no Missing We kept it as it is as categorization provides clinically interpretable groups\n\n\nCovariates:\n\nGender (factor, 2 levels): Male: Female\nEthnicity (factor, 5 levels): Mexican American, Non-Hispanic, White Non-Hispanic, Black Other Hispanic, Other Race - Including multi-racial\nAge (number, continuous)\n\n\nMerged dataset is cleaned and exploratory data analysis conducted to report results and visualizations for 10175 observations and 10 variables where - - race categorized in 5 levels - age range (0-80 years) - gender (male and female) - Diabetes grouped from (DIQ010 and DIQ050) BMI as continuous.\n\n\nCode\n# weighted means of each variable                       \nstr(merged_data)\n\n\n'data.frame':   10175 obs. of  10 variables:\n $ SEQN    : num  73557 73558 73559 73560 73561 ...\n $ RIDAGEYR: num  69 54 72 9 73 56 0 61 42 56 ...\n $ RIAGENDR: Factor w/ 2 levels \"Male\",\"Female\": 1 1 1 1 2 1 1 2 1 2 ...\n $ RIDRETH1: Factor w/ 5 levels \"Mexican American\",..: 4 3 3 3 3 1 3 3 2 3 ...\n $ SDMVPSU : num  1 1 1 2 2 1 1 1 2 1 ...\n $ SDMVSTRA: num  112 108 109 109 116 111 105 114 106 112 ...\n $ WTMEC2YR: num  13481 24472 57193 55767 65542 ...\n $ BMXBMI  : num  26.7 28.6 28.9 17.1 19.7 41.7 NA 35.7 NA 26.5 ...\n $ DIQ010  : Factor w/ 5 levels \"Yes\",\"No\",\"Borderline\",..: 1 1 1 2 2 2 NA 2 2 2 ...\n $ DIQ050  : Factor w/ 4 levels \"Yes\",\"No\",\"Refused\",..: 1 1 1 2 2 2 NA 2 2 2 ...\n\n\nCode\nplot_str(merged_data)\nhead(merged_data)\n\n\n   SEQN RIDAGEYR RIAGENDR           RIDRETH1 SDMVPSU SDMVSTRA WTMEC2YR BMXBMI\n1 73557       69     Male Non-Hispanic Black       1      112 13481.04   26.7\n2 73558       54     Male Non-Hispanic White       1      108 24471.77   28.6\n3 73559       72     Male Non-Hispanic White       1      109 57193.29   28.9\n4 73560        9     Male Non-Hispanic White       2      109 55766.51   17.1\n5 73561       73   Female Non-Hispanic White       2      116 65541.87   19.7\n6 73562       56     Male   Mexican American       1      111 25344.99   41.7\n  DIQ010 DIQ050\n1    Yes    Yes\n2    Yes    Yes\n3    Yes    Yes\n4     No     No\n5     No     No\n6     No     No\n\n\nCode\nsummary(merged_data)\n\n\n      SEQN          RIDAGEYR       RIAGENDR   \n Min.   :73557   Min.   : 0.00   Male  :5003  \n 1st Qu.:76100   1st Qu.:10.00   Female:5172  \n Median :78644   Median :26.00                \n Mean   :78644   Mean   :31.48                \n 3rd Qu.:81188   3rd Qu.:52.00                \n Max.   :83731   Max.   :80.00                \n                                              \n                                RIDRETH1       SDMVPSU         SDMVSTRA    \n Mexican American                   :1730   Min.   :1.000   Min.   :104.0  \n Other Hispanic                     : 960   1st Qu.:1.000   1st Qu.:107.0  \n Non-Hispanic White                 :3674   Median :1.000   Median :111.0  \n Non-Hispanic Black                 :2267   Mean   :1.484   Mean   :110.9  \n Other Race - Including Multi-Racial:1544   3rd Qu.:2.000   3rd Qu.:115.0  \n                                            Max.   :2.000   Max.   :118.0  \n                                                                           \n    WTMEC2YR          BMXBMI             DIQ010            DIQ050    \n Min.   :     0   Min.   :12.10   Yes       : 737   Yes       : 220  \n 1st Qu.: 12562   1st Qu.:19.70   No        :8841   No        :9545  \n Median : 20175   Median :24.70   Borderline: 185   Refused   :   1  \n Mean   : 30585   Mean   :25.68   Refused   :   1   Don't know:   2  \n 3rd Qu.: 36748   3rd Qu.:30.20   Don't know:   5   NA's      : 407  \n Max.   :171395   Max.   :82.90   NA's      : 406                    \n                  NA's   :1120                                       \n\n\nCode\nlibrary(dplyr)\nlibrary(knitr)\n\nplot_intro(merged_data, title=\"Figure 1 (Merged dataset). Structure of variables and missing observations.\")\n\n\n\n\n\n\n\n\n\nCode\nplot_missing(merged_data, title=\"Figure 2(Merged dataset). Breakdown of missing observations.\")\n\n\n\n\n\n\n\n\n\n\n\nCode\n# print(glimpse(merged_data))\nprint(table(merged_data$BMDBMIC, useNA = \"ifany\"))\n\n\n&lt; table of extent 0 &gt;\n\n\nCode\nprint(table(merged_data$DIQ010,  useNA = \"ifany\"))\n\n\n\n       Yes         No Borderline    Refused Don't know       &lt;NA&gt; \n       737       8841        185          1          5        406 \n\n\nCode\n #  ---- Coercion helpers (handle labelled/character) ----\nto_num &lt;- function(x) {\n  if (is.numeric(x)) return(x)\n  xc &lt;- as.character(x)\n  n &lt;- suppressWarnings(readr::parse_number(xc))\n  if (mean(is.na(n)) &gt; 0.80) {\n    xlow &lt;- tolower(trimws(xc))\n    n &lt;- dplyr::case_when(\n      xlow %in% c(\"1\",\"yes\",\"yes, told\") ~ 1,\n      xlow %in% c(\"2\",\"no\",\"no, not told\") ~ 2,\n      xlow %in% c(\"3\",\"borderline\") ~ 3,\n      xlow %in% c(\"7\",\"refused\") ~ 7,\n      xlow %in% c(\"9\",\"don't know\",\"dont know\",\"unknown\") ~ 9,\n      TRUE ~ NA_real_\n    )\n  }\n  as.numeric(n)\n}\n\nmerged_data &lt;- merged_data %&gt;%\n  mutate(\n    DIQ010   = to_num(DIQ010),\n    DIQ050   = to_num(if (!\"DIQ050\" %in% names(.)) NA_real_ else DIQ050),\n    BMXBMI   = suppressWarnings(as.numeric(BMXBMI)),\n    RIDAGEYR = suppressWarnings(as.numeric(RIDAGEYR)),\n    RIAGENDR = suppressWarnings(as.numeric(RIAGENDR)),\n    RIDRETH1 = suppressWarnings(as.numeric(RIDRETH1)),\n    SDMVPSU  = suppressWarnings(as.numeric(SDMVPSU)),\n    SDMVSTRA = suppressWarnings(as.numeric(SDMVSTRA)),\n    WTMEC2YR = suppressWarnings(as.numeric(WTMEC2YR))\n  )\n\n# ---- Diagnostics BEFORE save ----\ncat(\"DIQ010 counts BEFORE save:\\n\")\n\n\nDIQ010 counts BEFORE save:\n\n\nCode\nprint(table(merged_data$DIQ010, useNA = \"ifany\"))\n\n\n\n   1    2    3    7    9 &lt;NA&gt; \n 737 8841  185    1    5  406 \n\n\nCode\ncat(\"Count with DIQ010 in {1,2}:\", sum(merged_data$DIQ010 %in% c(1,2), na.rm = TRUE), \"\\n\")\n\n\nCount with DIQ010 in {1,2}: 9578 \n\n\nCode\n# ---- Save to file for reuse ----\ndir.create(\"data\", showWarnings = FALSE)\n# ---- Save ----\ndir.create(\"data\", showWarnings = FALSE, recursive = TRUE)\nsaveRDS(merged_data, \"data/merged_2013_2014.rds\")\nmessage(\"Saved: data/merged_2013_2014.rds\")"
  },
  {
    "objectID": "index.html#exploratory-data-analysis",
    "href": "index.html#exploratory-data-analysis",
    "title": "Bayesian Logistic Regression - Application in Probability Prediction of disease (Diabetes)",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\nUsing library(survey) we calculated weighted means and sd of all the variables. The BMI and age were standardized.\nAge categorized was recoded into different variables, to create age range (20-80 years) &gt;20 years\nBMI is recoded and categorized as-“18.5,18.5–&lt;25,25–&lt;30,30–&lt;35,35–&lt;40,≥40 years).\nEthnicity is recoded as “Mexican American” = “1”, “Other Hispanic” = “2”, “NH White” = “3”, “NH Black” = “4”, “Other/Multi” = “5”\nSince special codes are not random, cannot be dropped; the informative missingness if ignored (MAR or MNAR) could introduce bias.\n\nWe transformed special codes (3,7,) to NA and included all NAs in the analysis. Visualization of missing data is presented below.\nA final analytic dataset created (‘adult’) with “NH White” and “Male” as the reference group for analysis\n\n\nCode\n## \n# ---------------- Basic Exploration (adults) ----------------\n\n# Keep adults only and build analysis variables\nadult &lt;- merged_data %&gt;%\n  dplyr::filter(RIDAGEYR &gt;= 20) %&gt;%\n  dplyr::transmute(\n    # --- keep survey design variables so svydesign() can see them ---\n    SDMVPSU, SDMVSTRA, WTMEC2YR,\n\n    # --- outcome: DIQ010 (1 yes, 2 no; 3/7/9 -&gt; NA) ---\n    diabetes_dx = dplyr::case_when(\n      DIQ010 == 1 ~ 1,\n      DIQ010 == 2 ~ 0,\n      DIQ010 %in% c(3, 7, 9) ~ NA_real_,\n      TRUE ~ NA_real_\n    ),\n\n    # --- predictors (raw) ---\n    bmi  = BMXBMI,\n    age  = RIDAGEYR,\n\n    # sex (1=Male, 2=Female)\n    sex  = forcats::fct_recode(factor(RIAGENDR), Male = \"1\", Female = \"2\"),\n\n    # race (5-level)\n    race = forcats::fct_recode(\n      factor(RIDRETH1),\n      \"Mexican American\" = \"1\",\n      \"Other Hispanic\"   = \"2\",\n      \"NH White\"         = \"3\",\n      \"NH Black\"         = \"4\",\n      \"Other/Multi\"      = \"5\"\n    ),\n\n    # keep DIQ050 so we can safely reference it (may be absent/NA in some rows)\n    \n    DIQ050 = DIQ050\n  ) %&gt;%\n  # standardize continuous predictors\n  dplyr::mutate(\n    age_c = as.numeric(scale(age)),\n    bmi_c = as.numeric(scale(bmi)),\n    bmi_cat = cut(\n      bmi,\n      breaks = c(-Inf, 18.5, 25, 30, 35, 40, Inf),\n      labels = c(\"&lt;18.5\",\"18.5–&lt;25\",\"25–&lt;30\",\"30–&lt;35\",\"35–&lt;40\",\"≥40\"),\n      right = FALSE\n    )\n  ) %&gt;%\n  # adjust outcome: if female & DIQ050==1 (\"only when pregnant\"), set to 0 (not diabetes)\n  dplyr::mutate(\n    diabetes_dx = ifelse(sex == \"Female\" & !is.na(DIQ050) & DIQ050 == 1, 0, diabetes_dx)\n  )\n\n# Make NH White the reference level for race (clearer interpretation)\nadult &lt;- adult %&gt;%\n  dplyr::mutate(\n    race = forcats::fct_relevel(race, \"NH White\")\n  )\n\n# --- sanity checks ---\ncat(\"Adults n =\", nrow(adult), \"\\n\")\n\n\nAdults n = 5769 \n\n\nCode\nglimpse(adult)\n\n\nRows: 5,769\nColumns: 12\n$ SDMVPSU     &lt;dbl&gt; 1, 1, 1, 2, 1, 1, 2, 1, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 2, 2…\n$ SDMVSTRA    &lt;dbl&gt; 112, 108, 109, 116, 111, 114, 106, 112, 112, 113, 116, 114…\n$ WTMEC2YR    &lt;dbl&gt; 13481.04, 24471.77, 57193.29, 65541.87, 25344.99, 61758.65…\n$ diabetes_dx &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ bmi         &lt;dbl&gt; 26.7, 28.6, 28.9, 19.7, 41.7, 35.7, NA, 26.5, 22.0, 20.3, …\n$ age         &lt;dbl&gt; 69, 54, 72, 73, 56, 61, 42, 56, 65, 26, 76, 33, 32, 38, 50…\n$ sex         &lt;fct&gt; Male, Male, Male, Female, Male, Female, Male, Female, Male…\n$ race        &lt;fct&gt; NH Black, NH White, NH White, NH White, Mexican American, …\n$ DIQ050      &lt;dbl&gt; 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ age_c       &lt;dbl&gt; 1.13241831, 0.27835981, 1.30323001, 1.36016725, 0.39223428…\n$ bmi_c       &lt;dbl&gt; -0.33588609, -0.07028101, -0.02834336, -1.31443114, 1.7609…\n$ bmi_cat     &lt;fct&gt; 25–&lt;30, 25–&lt;30, 25–&lt;30, 18.5–&lt;25, ≥40, 35–&lt;40, NA, 25–&lt;30,…\n\n\n##Exploratory Description and Visualization of Adult dataset (&gt; 20 years)**\nObservations - 5769 observations Survey design: SDMVPSU, SDMVSTRA, WTMEC2YR Outcome: diabetes_dx (numeric 0/1) Covariates: bmi, age, sex, race, DIQ050 Centered covariates: age_c, bmi_c BMI categories: bmi_cat\nNHANES is a national surveys based on complex sampling designs (oversampling certain groups (e.g., minorities, older adults) to ensure representation. They use multistage sampling to represent the U.S. population, so we apply sampling weights, strata, and PSU (primary sampling units) for valid estimates.\nWe use survey design in regression anlaysis to avoid - - bias prevalence estimates (e.g., mean BMI or diabetes %) - underestimation of standard errors - incorrect inference for population-level parameters.\n\n\nCode\n# data exploration\n\nprint(table(adult$diabetes_dx, useNA = \"ifany\"))\n\n\n\n   0    1 &lt;NA&gt; \n4974  618  177 \n\n\nCode\nprint(table(adult$sex, useNA = \"ifany\"))\n\n\n\n  Male Female \n  2758   3011 \n\n\nCode\nprint(table(adult$race, useNA = \"ifany\"))\n\n\n\n        NH White Mexican American   Other Hispanic         NH Black \n            2472              767              508             1177 \n     Other/Multi \n             845 \n\n\nCode\nif (sum(!is.na(adult$diabetes_dx)) == 0) {\n  stop(\"Too few non-missing outcomes for modeling (n = 0). Check DIQ010 upstream.\")\n}\n\n# (optional plots omitted for brevity)\n\n# save for downstream\nif (!dir.exists(\"data\")) dir.create(\"data\", recursive = TRUE)\nsaveRDS(adult, \"data/adult_cleaned_2013_2014.rds\")\n\n\n\n\nCode\nggplot(adult, aes(x = age)) +\n  geom_histogram(binwidth = 5, fill = \"skyblue\", color = \"white\") +\n  labs(\n    title = \"Distribution of Age &gt;20 years\",\n    x = \"Age (years)\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\nggplot(adult, aes(factor(diabetes_dx))) +\n  geom_bar(fill = \"steelblue\") +\n  labs(title=\"Diabetes Outcome Distribution in &gt;20 years age group\", x=\"diabetes_dx (0=No, 1=Yes)\", y=\"Count\")\n\n\n\n\n\n\n\n\n\nCode\nggplot(adult, aes(factor(bmi_cat))) +\n  geom_bar(fill = \"steelblue\") +\n  labs(title=\"Diabetes Outcome Distribution by BMI in &gt;20 years age group\", x=\"bmi_cat\")\n\n\n\n\n\n\n\n\n\nCode\nggplot(adult, aes(x = factor(diabetes_dx), y = bmi)) +\n  geom_boxplot(fill = \"skyblue\") +\n  labs(\n    title = \"BMI Distribution by Diabetes Diagnosis in &gt;20 years age group\",\n    x = \"Diabetes Diagnosis (0 = No, 1 = Yes)\",\n    y = \"BMI\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# plots for adult data bmi categories and race categories\n\nggplot(adult, aes(x = factor(race), fill = factor(diabetes_dx))) +\n  geom_bar(position = \"dodge\") +\n  labs(\n    title = \"Diabetes Diagnosis by Race in &gt;20 years age group\",\n    x = \"Race/Ethnicity\",\n    y = \"Count\",\n    fill = \"Diabetes Diagnosis\\n(0 = No, 1 = Yes)\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nCode\nggplot(adult, aes(x = factor(bmi_cat), fill = factor(diabetes_dx))) +\n  geom_bar(position = \"dodge\") +\n  labs(\n    title = \"Diabetes Diagnosis by BMI in &gt;20 years age group\",\n    x = \"BMI\",\n    y = \"Count\",\n    fill = \"Diabetes Diagnosis\\n(0 = No, 1 = Yes)\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "index.html#multiple-logistic-regression-on-survey-weighted-dataset",
    "href": "index.html#multiple-logistic-regression-on-survey-weighted-dataset",
    "title": "Bayesian Logistic Regression - Application in Probability Prediction of disease (Diabetes)",
    "section": "Multiple Logistic regression on survey weighted dataset",
    "text": "Multiple Logistic regression on survey weighted dataset\nWe conducted frequentist method Multiple Logistic regression on a survey-weighted dataset, for complete case analysis and data exploration"
  },
  {
    "objectID": "index.html#multivariate-imputation-by-chained-equations",
    "href": "index.html#multivariate-imputation-by-chained-equations",
    "title": "Bayesian Logistic Regression - Application in Probability Prediction of disease (Diabetes)",
    "section": "Multivariate Imputation by Chained Equations",
    "text": "Multivariate Imputation by Chained Equations\n\nWe conducted MICE to manage missiging data as an alternative to the Bayesian Approach Buuren and Groothuis-Oudshoorn (2011)\nFlatness of the density, heavy tails, non-zero peakedness, skewness and multimodality do not hamper the good performance of multiple imputation for the mean structure in samples n &gt; 400 even for high percentages (75%) of missing data in one variable Van Buuren and Van Buuren (2012).\nMultiple Imputation (MI) can be performed using mice package in R\nIterative mice imputes missing values of one variable at a time, using regression models based on the other variables in the dataset.\nIn the chain process, each imputed variable become a predictor for the subsequent imputation, and the entire process is repeated multiple times to create several complete datasets, each reflecting different possibilities for the missing data."
  },
  {
    "objectID": "index.html#bayesian-logistic-regression",
    "href": "index.html#bayesian-logistic-regression",
    "title": "Bayesian Logistic Regression - Application in Probability Prediction of disease (Diabetes)",
    "section": "Bayesian Logistic Regression",
    "text": "Bayesian Logistic Regression\n\nWe conduct bayesian logisitic regression to estimate the association between BMI, age, sex, and race/ethnicity and predict doctor-diagnosed diabetes (DIQ010).\nBayesian statistics is about updating beliefs with evidence: Posterior ∝ Likelihood × Prior\nPrior (p(θ)): Your initial belief about a parameter before seeing the data.\nLikelihood (p(y|θ)): How probable the observed data are given the parameters. This is derived from the model (e.g., logistic regression likelihood).\nPosterior (p(θ|y)): Your updated belief about the parameter after seeing the data.\nWe selected Bayesian Logistic Regression since our study response variable is a binary outcome (Diabetes:yes/no)\nBayesian Logistic Regression is based on binomial probability Bayes’ rules, and predicts probability of disease outcome\nBayes analyzes linear relation between the predictor (Age, Race, BMI, Gender) and outcome response variable (Diabetes).\nBayes considers that predictors and response variables are independent.\nRegression a of a discrete variable (0 or 1) is a Bernoulli probability model that classifies categorical response variables - predicting Diabetes.\nLogit link provides probabilities for the response variable.\nPrior\nWe used student_t(3, 0, 10) prior R. V. D. Schoot et al. (2013) - student_t(ν,μ,σ) Student’s t-distribution with: ν=3: degrees of freedom (controls tail heaviness) μ=0: location (center, like the mean) σ=10: scale (spread, like standard deviation) student_t(3, 0, 10) means: It’s centered at 0 It allows large values (± several times 10) It has heavy tails, since ν=3, allows for outliers or unexpected large parameter values Helps the model remain stable, especially with: # Small datasets # High correlation between predictors # Potential outliers in the data\nIn Bayesian statistics, every unknown parameter (like a regression coefficient, mean, or variance) is treated as a random variable with a probability distribution that reflects uncertainty.\n\n##Summary of Bayesian regression presented under following headings - Posterior Predictive Probabilities - Posterior Mean, Median, credible Intervals - Posterior Probability (Outcome=1) - Comparison with External Prevalence (population prevalence) - Posterior Model Fit Metrics - Prior versus Posterior Coefficient Distributions - Posterior Predictive Checks - Uncertainty Quantification\n##Bayesian Logistic Regression model Equation\n\\[ \\text{logit}(P(Y_i=1)) = \\beta_0 + \\beta_1 \\cdot Age_i + \\beta_2 \\cdot BMI_i + \\beta_3 \\cdot Race_i + \\beta_4 \\cdot Gender_i \\] Linear Regression equation:\n\\[ y_i = \\beta_0 + \\beta_1 X_1 +\\varepsilon_i \\] ##Diagnostics for Bayesian Logictic Regression\n\nTrace Plots for Markov Chain Monte Carlo Convergence\nAutocorrelation Plots\nPosterior Predictive Checks\nResidual Analysis\nPrior Sensitivity Analysis\nModel Fit Assessment\nPosterior Predictive Probability Plots\nPosterior Interval Coverage Evaluation\nConvergence Diagnostics across Chains\n\n\n\nCode\n# Modeling\n\nlibrary(broom)\nlibrary(mice)\nlibrary(brms)\nlibrary(posterior)\nlibrary(bayesplot)\nlibrary(knitr)\n\n# --- Guardrails for modeling ---\nn_outcome &lt;- sum(!is.na(adult$diabetes_dx))\nif (n_outcome == 0) stop(\"Too few non-missing outcomes for modeling. n = 0\")\n\n# Ensure factors and &gt;=2 observed levels among complete outcomes\nadult &lt;- adult %&gt;%\n  dplyr::mutate(\n    sex  = if (!is.factor(sex))  factor(sex)  else sex,\n    race = if (!is.factor(race)) factor(race) else race\n  )\n\nif (nlevels(droplevels(adult$sex[!is.na(adult$diabetes_dx)]))  &lt; 2)\n  stop(\"sex has &lt;2 observed levels after filtering; check data availability.\")\nif (nlevels(droplevels(adult$race[!is.na(adult$diabetes_dx)])) &lt; 2)\n  stop(\"race has &lt;2 observed levels after filtering; check Data Prep.\")\n\n   #  Survey-weighted complete-case \n# Build a logical filter on the original adult data (same length as design$data)\nkeep_cc &lt;- with(\n  adult,\n  !is.na(diabetes_dx) & !is.na(age_c) & !is.na(bmi_c) &\n  !is.na(sex) & !is.na(race)\n)\n\n# Subset the survey design using the logical vector (same length as original)\ndes_cc &lt;- subset(nhanes_design_adult, keep_cc)\n\n# Corresponding complete-case data (optional)\ncc &lt;- adult[keep_cc, ] |&gt; droplevels()\ncat(\"\\nComplete-case N for survey-weighted model:\", nrow(cc), \"\\n\")\n\n\n\nComplete-case N for survey-weighted model: 5349 \n\n\nCode\nprint(table(cc$race))\n\n\n\n        NH White Mexican American   Other Hispanic         NH Black \n            2293              713              470             1101 \n     Other/Multi \n             772 \n\n\nCode\nprint(table(cc$diabetes_dx))\n\n\n\n   0    1 \n4752  597 \n\n\nCode\nprint(table(cc$sex))\n\n\n\n  Male Female \n  2551   2798 \n\n\nCode\nform_cc &lt;- diabetes_dx ~ age_c + bmi_c + sex + race\nsvy_fit &lt;- survey::svyglm(formula = form_cc, design = des_cc, family = quasibinomial())\nsummary(svy_fit)\n\n\n\nCall:\nsvyglm(formula = form_cc, design = des_cc, family = quasibinomial())\n\nSurvey design:\nsubset(nhanes_design_adult, keep_cc)\n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          -2.67143    0.11935 -22.383 1.68e-08 ***\nage_c                 1.10833    0.05042  21.981 1.94e-08 ***\nbmi_c                 0.63412    0.05713  11.099 3.88e-06 ***\nsexFemale            -0.63844    0.10926  -5.843 0.000386 ***\nraceMexican American  0.71091    0.13681   5.196 0.000826 ***\nraceOther Hispanic    0.46469    0.13474   3.449 0.008712 ** \nraceNH Black          0.51221    0.15754   3.251 0.011677 *  \nraceOther/Multi       0.84460    0.17756   4.757 0.001433 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasibinomial family taken to be 0.8455444)\n\nNumber of Fisher Scoring iterations: 6\n\n\nCode\n## plot(residuals(svy_fit, type='deviance'))\n\n# Survey-weighted OR table (no intercept)\nsvy_or &lt;- broom::tidy(svy_fit, conf.int = TRUE) %&gt;%\n  dplyr::mutate(OR = exp(estimate), LCL = exp(conf.low), UCL = exp(conf.high)) %&gt;%\n  dplyr::select(term, OR, LCL, UCL, p.value) %&gt;%\n  dplyr::filter(term != \"(Intercept)\")\nknitr::kable(svy_or, caption = \"Survey-weighted odds ratios (per 1 SD)\")\n\n\n\nSurvey-weighted odds ratios (per 1 SD)\n\n\nterm\nOR\nLCL\nUCL\np.value\n\n\n\n\nage_c\n3.0292807\n2.6967690\n3.4027912\n0.0000000\n\n\nbmi_c\n1.8853571\n1.6526296\n2.1508579\n0.0000039\n\n\nsexFemale\n0.5281132\n0.4104905\n0.6794397\n0.0003857\n\n\nraceMexican American\n2.0358434\n1.4850041\n2.7910081\n0.0008262\n\n\nraceOther Hispanic\n1.5915182\n1.1664529\n2.1714810\n0.0087119\n\n\nraceNH Black\n1.6689718\n1.1605895\n2.4000450\n0.0116773\n\n\nraceOther/Multi\n2.3270527\n1.5451752\n3.5045697\n0.0014331\n\n\n\n\n\nThe Survey-weighted odds ratios (per 1 SD) are presented. The residual plot looks okay and does not show any pattern.\nMICE performed on the imputed dataset - model summary of the pooled imputed dataset presents odds ratio.\n\n\nCode\n# ----- Multiple Imputation (predictors only) \nmi_dat &lt;- adult %&gt;%\n  dplyr::select(diabetes_dx, age, bmi, sex, race, WTMEC2YR, SDMVPSU, SDMVSTRA)\n\nmeth &lt;- mice::make.method(mi_dat)\npred &lt;- mice::make.predictorMatrix(mi_dat)\n\n# Do not impute outcome\nmeth[\"diabetes_dx\"] &lt;- \"\"\npred[\"diabetes_dx\", ] &lt;- 0\npred[,\"diabetes_dx\"] &lt;- 1\n\n# Imputation methods\nmeth[\"age\"]  &lt;- \"norm\"\nmeth[\"bmi\"]  &lt;- \"pmm\"\nmeth[\"sex\"]  &lt;- \"polyreg\"\nmeth[\"race\"] &lt;- \"polyreg\"\n\n# Survey design vars as auxiliaries only\nmeth[c(\"WTMEC2YR\",\"SDMVPSU\",\"SDMVSTRA\")] &lt;- \"\"\npred[, c(\"WTMEC2YR\",\"SDMVPSU\",\"SDMVSTRA\")] &lt;- 1\n\nglimpse(mi_dat)\n\n\nRows: 5,769\nColumns: 8\n$ diabetes_dx &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ age         &lt;dbl&gt; 69, 54, 72, 73, 56, 61, 42, 56, 65, 26, 76, 33, 32, 38, 50…\n$ bmi         &lt;dbl&gt; 26.7, 28.6, 28.9, 19.7, 41.7, 35.7, NA, 26.5, 22.0, 20.3, …\n$ sex         &lt;fct&gt; Male, Male, Male, Female, Male, Female, Male, Female, Male…\n$ race        &lt;fct&gt; NH Black, NH White, NH White, NH White, Mexican American, …\n$ WTMEC2YR    &lt;dbl&gt; 13481.04, 24471.77, 57193.29, 65541.87, 25344.99, 61758.65…\n$ SDMVPSU     &lt;dbl&gt; 1, 1, 1, 2, 1, 1, 2, 1, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 2, 2…\n$ SDMVSTRA    &lt;dbl&gt; 112, 108, 109, 116, 111, 114, 106, 112, 112, 113, 116, 114…\n\n\nCode\nimp &lt;- mice::mice(mi_dat, m = 5, method = meth, predictorMatrix = pred, seed = 123)\n\n\n\n iter imp variable\n  1   1  bmi\n  1   2  bmi\n  1   3  bmi\n  1   4  bmi\n  1   5  bmi\n  2   1  bmi\n  2   2  bmi\n  2   3  bmi\n  2   4  bmi\n  2   5  bmi\n  3   1  bmi\n  3   2  bmi\n  3   3  bmi\n  3   4  bmi\n  3   5  bmi\n  4   1  bmi\n  4   2  bmi\n  4   3  bmi\n  4   4  bmi\n  4   5  bmi\n  5   1  bmi\n  5   2  bmi\n  5   3  bmi\n  5   4  bmi\n  5   5  bmi\n\n\n\n\nCode\nfit_mi &lt;- with(imp, {\n  age_c &lt;- as.numeric(scale(age))\n  bmi_c &lt;- as.numeric(scale(bmi))\n  glm(diabetes_dx ~ age_c + bmi_c + sex + race, family = binomial())\n})\npool_mi &lt;- pool(fit_mi)\nsummary(pool_mi)\n\n\n                  term   estimate  std.error  statistic       df       p.value\n1          (Intercept) -2.6895645 0.09941301 -27.054453 5566.204 1.486581e-151\n2                age_c  1.0660265 0.05594733  19.054108 5520.446  1.911564e-78\n3                bmi_c  0.5468538 0.04473386  12.224604 5148.557  6.751227e-34\n4            sexFemale -0.6178297 0.09379129  -6.587282 5551.660  4.892566e-11\n5 raceMexican American  0.8877355 0.13750463   6.456041 5472.583  1.167455e-10\n6   raceOther Hispanic  0.5606621 0.17485537   3.206433 5573.987  1.351505e-03\n7         raceNH Black  0.6809629 0.11981185   5.683602 5576.734  1.385727e-08\n8      raceOther/Multi  0.7476406 0.15300663   4.886328 4749.963  1.061140e-06\n\n\nCode\n## table \n\nmi_or &lt;- summary(pool_mi, conf.int = TRUE, exponentiate = TRUE) %&gt;%\n  dplyr::rename(\n    term = term, OR = estimate, LCL = `2.5 %`, UCL = `97.5 %`, p.value = p.value\n  ) %&gt;%\n  dplyr::filter(term != \"(Intercept)\")\nknitr::kable(mi_or, caption = \"MI pooled odds ratios (per 1 SD)\")\n\n\n\nMI pooled odds ratios (per 1 SD)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nOR\nstd.error\nstatistic\ndf\np.value\nLCL\nUCL\nconf.low\nconf.high\n\n\n\n\n2\nage_c\n2.9038183\n0.0559473\n19.054108\n5520.446\n0.0000000\n2.6021752\n3.2404277\n2.6021752\n3.2404277\n\n\n3\nbmi_c\n1.7278084\n0.0447339\n12.224604\n5148.557\n0.0000000\n1.5827382\n1.8861754\n1.5827382\n1.8861754\n\n\n4\nsexFemale\n0.5391132\n0.0937913\n-6.587282\n5551.660\n0.0000000\n0.4485669\n0.6479368\n0.4485669\n0.6479368\n\n\n5\nraceMexican American\n2.4296216\n0.1375046\n6.456041\n5472.583\n0.0000000\n1.8555327\n3.1813298\n1.8555327\n3.1813298\n\n\n6\nraceOther Hispanic\n1.7518320\n0.1748554\n3.206433\n5573.987\n0.0013515\n1.2434346\n2.4680953\n1.2434346\n2.4680953\n\n\n7\nraceNH Black\n1.9757793\n0.1198118\n5.683602\n5576.734\n0.0000000\n1.5621842\n2.4988753\n1.5621842\n2.4988753\n\n\n8\nraceOther/Multi\n2.1120110\n0.1530066\n4.886328\n4749.963\n0.0000011\n1.5646727\n2.8508138\n1.5646727\n2.8508138"
  },
  {
    "objectID": "index.html#training-and-testing",
    "href": "index.html#training-and-testing",
    "title": "Bayesian Logistic Regression - Application in Probability Prediction of disease (Diabetes)",
    "section": "Training and testing",
    "text": "Training and testing\nTo evaluate how well the model learned patterns from the training dataset, and how reliably it will perform on new data in real-world scenarios, we decided to perform Training/Testing on the imputed data\nTesting set to evaluate model performance on unseen data. - Model Accuracy / Performance - Generalizability - Helps check for overfitting or underfitting - Reliability of Predictions - Confidence on model predictions for new individuals - Distribution and Bias Check - to ensure the training and testing sets are representative of the same population\nResults from training and test data- Training (n=4473) & Testing (n=1119) sets: Age ~48 yrs, BMI ~28, bmi_c ~0, wt_norm ~1; ranges similar across sets.\n\n\nCode\n#Code here\n\nimport pandas as pd\n\n# Load the data exported from R\nadult_data_py = pd.read_csv(\"adult_imp1.csv\")  # your R dataset saved as CSV\n\n# Now you can rename it freely in Python\ndf = adult_data_py  # just create a new variable pointing to the same DataFrame\n\n# Check the data\ndf.head()\n\n\n   diabetes_dx  age   bmi     sex  ... SDMVSTRA     age_c     bmi_c   wt_norm\n0            1   69  26.7    Male  ...      112  1.132418 -0.333192  0.339392\n1            1   54  28.6    Male  ...      108  0.278360 -0.067558  0.616088\n2            1   72  28.9    Male  ...      109  1.303230 -0.025616  1.439868\n3            0   73  19.7  Female  ...      116  1.360167 -1.311843  1.650048\n4            0   56  41.7    Male  ...      111  0.392234  1.763918  0.638072\n\n[5 rows x 11 columns]\n\n\nCode\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Optional: rename columns if needed\ndf = df.rename(columns={\"diabetes_dx\": \"diabetes_status\"})\n\n# Separate predictors (X) and outcome (y)\nX = df.drop(columns=[\"diabetes_status\"])\ny = df[\"diabetes_status\"]\n\n# Create 80/20 train-test split, stratifying by outcome to keep class balance\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=1234, stratify=y\n)\n\n# Combine X and y if you want full train/test datasets\ntrain_data = pd.concat([X_train, y_train], axis=1)\ntest_data  = pd.concat([X_test, y_test], axis=1)\n\n# Check sizes\nprint(\"Training set:\", train_data.shape)\n\n\nTraining set: (4473, 11)\n\n\nCode\nprint(\"Testing set: \", test_data.shape)\n\n\nTesting set:  (1119, 11)\n\n\nCode\n# Check class balance\nprint(\"\\nTraining set class distribution:\\n\", y_train.value_counts(normalize=True))\n\n\n\nTraining set class distribution:\n diabetes_status\n0    0.88956\n1    0.11044\nName: proportion, dtype: float64\n\n\nCode\nprint(\"\\nTesting set class distribution:\\n\", y_test.value_counts(normalize=True))\n\n\n\nTesting set class distribution:\n diabetes_status\n0    0.889187\n1    0.110813\nName: proportion, dtype: float64\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n# Combine train and test distributions into a DataFrame\ntrain_dist = pd.DataFrame({'Class': y_train.unique(), \n                           'Proportion': y_train.value_counts(normalize=True).values,\n                           'Set': 'Train'})\n\ntest_dist = pd.DataFrame({'Class': y_test.unique(), \n                          'Proportion': y_test.value_counts(normalize=True).values,\n                          'Set': 'Test'})\n\ndist_df = pd.concat([train_dist, test_dist])\n\n# Plot\nplt.figure(figsize=(8,5))\nsns.barplot(x='Class', y='Proportion', hue='Set', data=dist_df)\nplt.title('Class Distribution in Training and Testing Sets')\nplt.ylabel('Proportion')\nplt.xlabel('Diabetes Status')\nplt.ylim(0,1)\n\n\n(0.0, 1.0)\n\n\nCode\nplt.show()\n\n\n\n\n\n\n\n\n\nCode\n# Numeric summaries for predictors\nprint(\"Training set numeric summary:\\n\", X_train.describe())\n\n\nTraining set numeric summary:\n                age          bmi  ...        bmi_c      wt_norm\ncount  4473.000000  4473.000000  ...  4473.000000  4473.000000\nmean     48.926895    28.933736  ...    -0.020899     1.001234\nstd      17.571583     6.935321  ...     0.969609     0.786057\nmin      20.000000    14.100000  ...    -2.094764     0.000000\n25%      34.000000    24.100000  ...    -0.696691     0.477152\n50%      48.000000    27.700000  ...    -0.193384     0.700259\n75%      63.000000    32.300000  ...     0.449729     1.286732\nmax      80.000000    77.500000  ...     6.769021     4.314957\n\n[8 rows x 8 columns]\n\n\nCode\nprint(\"\\nTesting set numeric summary:\\n\", X_test.describe())\n\n\n\nTesting set numeric summary:\n                age          bmi  ...        bmi_c      wt_norm\ncount  1119.000000  1119.000000  ...  1119.000000  1119.000000\nmean     48.475424    29.275782  ...     0.026921     1.002283\nstd      17.568193     7.744067  ...     1.082677     0.785208\nmin      20.000000    15.200000  ...    -1.940976     0.000000\n25%      33.000000    23.900000  ...    -0.724652     0.453099\n50%      47.000000    27.800000  ...    -0.179404     0.697037\n75%      63.000000    32.900000  ...     0.533614     1.378142\nmax      80.000000    82.900000  ...     7.523981     3.877294\n\n[8 rows x 8 columns]\n\n\nTraining (n=4473) & Testing (n=1119) sets: Age ~48 yrs, BMI ~28, bmi_c ~0, wt_norm ~1; ranges similar across sets.\nPrior - We used student_t(3, 0, 10) prior R. V. D. Schoot et al. (2013) - student_t(ν,μ,σ)\nStudent’s t-distribution with: ν=3: degrees of freedom (controls tail heaviness) μ=0: location (center, like the mean) σ=10: scale (spread, like standard deviation) student_t(3, 0, 10) means: It’s centered at 0 It allows large values (± several times 10) It has heavy tails, since ν=3, allows for outliers or unexpected large parameter values Helps the model remain stable, especially with: Small datasets High correlation between predictors Potential outliers in the data\nBayesian Logistic Regression Analysis\n\n\nCode\n###\nlibrary(ggplot2)\n\n# priors for two coefficients (age and bmi) prior = N(0,2.5)\nprior_draws &lt;- tibble(\n  term = rep(c(\"Age (per 1 SD)\", \"BMI (per 1 SD)\"), each = 4000),\n  value = c(rnorm(4000, 0, 2.5), rnorm(4000, 0, 2.5))\n)\n\nggplot(prior_draws, aes(x = value, fill = term)) +\n  geom_density(alpha = 0.5) +\n  theme_minimal() +\n  labs(title = \"Prior Distributions for Coefficients\",\n       x = \"Coefficient Value\", y = \"Density\") +\n  scale_fill_manual(values = c(\"skyblue\", \"orange\"))\n\n\n\n\n\n\n\n\n\nCode\n# Diabetes vs BMI\n\nlibrary(ggplot2)\n\n# Create the plot\nggplot(adult_imp1, aes(x = factor(diabetes_dx), y = bmi, fill = factor(diabetes_dx))) +\n  geom_boxplot(alpha = 0.7) +\n  scale_x_discrete(labels = c(\"0\" = \"No Diabetes\", \"1\" = \"Diabetes\")) +\n  labs(\n    x = \"Diabetes Diagnosis\",\n    y = \"BMI\",\n    title = \"BMI Distribution by Diabetes Status\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nCode\n# logistic regression curve\nggplot(adult_imp1, aes(x = bmi, y = diabetes_dx)) +\n  geom_point(aes(y = diabetes_dx), alpha = 0.2, position = position_jitter(height = 0.02)) +\n  geom_smooth(method = \"glm\", method.args = list(family = \"binomial\"), se = TRUE, color = \"blue\") +\n  labs(\n    x = \"BMI\",\n    y = \"Probability of Diabetes\",\n    title = \"Predicted Probability of Diabetes vs BMI\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "index.html#comparative-visualizations",
    "href": "index.html#comparative-visualizations",
    "title": "Bayesian Logistic Regression - Application in Probability Prediction of disease (Diabetes)",
    "section": "Comparative Visualizations",
    "text": "Comparative Visualizations\n\nPredicted vs observed - to check how well the model’s predictions align with reality where mean(y_rep) = average predicted probability of diabetes for each individual, across all posterior draws of the parameters. y = the actual observed diabetes status (0 = non-diabetic, 1 = diabetic).\nmcmc plot (dens plots) comparing observed and posterior parameter values (estimates) for bmi_c, age_c, sex_female, and by race categories\nFitted (Predicted) vs observed for bmi using point and error bars\nFitted (Predicted) vs observed for bmi using line plot\n\n\n\nCode\n# Combine observed and predicted into one data frame\nplot_data &lt;- adult_imp1 %&gt;%\n  mutate(\n    predicted_bmi = predicted[, \"Estimate\"],\n    lower_ci = predicted[, \"Q2.5\"],\n    upper_ci = predicted[, \"Q97.5\"],\n    obs_index = 1:nrow(adult_imp1)  # index for x-axis\n  )\n\n# Line plot\nggplot(plot_data, aes(x = obs_index)) +\n  geom_line(aes(y = bmi, color = \"Observed\")) +               # observed BMI\n  geom_line(aes(y = predicted_bmi, color = \"Predicted\")) +   # predicted BMI\n  geom_ribbon(aes(ymin = lower_ci, ymax = upper_ci), alpha = 0.2) +  # uncertainty\n  labs(x = \"Observation\", y = \"BMI\", color = \"Legend\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\nsummary(adult_imp1$bmi)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   14.1    24.1    27.7    29.0    32.4    82.9 \n\n\nCode\nsummary(plot_data$bmi_c)\n\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-2.09476 -0.69669 -0.19338 -0.01133  0.46371  7.52398 \n\n\nCentering for bmi - Summary of original bmi (observed data) and centered version of BMI. - Centering doesn’t change the distribution shape, only shifts it so the mean is zero. - Centering is useful in regression/Bayesian models to improve numerical stability and interpretability of intercepts - Plots showing predicted values of bmi and age (prior vs predicted) and the proportion of diabetes=1 for each draw\n\n\nCode\nprior_summary(bayes_fit)\n\n\n               prior     class                coef group resp dpar nlpar lb ub\n      normal(0, 2.5)         b                                                \n      normal(0, 2.5)         b               age_c                            \n      normal(0, 2.5)         b               bmi_c                            \n      normal(0, 2.5)         b raceMexicanAmerican                            \n      normal(0, 2.5)         b         raceNHBlack                            \n      normal(0, 2.5)         b     raceOtherDMulti                            \n      normal(0, 2.5)         b   raceOtherHispanic                            \n      normal(0, 2.5)         b           sexFemale                            \n student_t(3, 0, 10) Intercept                                                \n tag       source\n             user\n     (vectorized)\n     (vectorized)\n     (vectorized)\n     (vectorized)\n     (vectorized)\n     (vectorized)\n     (vectorized)\n             user\n\n\nCode\nprior_draws &lt;- tibble(\n  term = rep(c(\"BMI (per 1 SD)\", \"Age (per 1 SD)\"), each = 4000),\n  estimate = c(rnorm(4000, 0, 1), rnorm(4000, 0, 1)),\n  type = \"Prior\"\n)\n\nsummary(prior_draws)\n\n\n     term              estimate             type          \n Length:8000        Min.   :-3.585220   Length:8000       \n Class :character   1st Qu.:-0.697054   Class :character  \n Mode  :character   Median : 0.000046   Mode  :character  \n                    Mean   :-0.007350                     \n                    3rd Qu.: 0.668561                     \n                    Max.   : 3.922817                     \n\n\nCode\npost\n\n\n# A draws_df: 1000 iterations, 4 chains, and 11 variables\n   b_Intercept b_age_c b_bmi_c b_sexFemale b_raceMexicanAmerican\n1         -2.6     1.1    0.70       -0.71                  0.67\n2         -2.7     1.0    0.62       -0.57                  0.65\n3         -2.6     1.1    0.65       -0.76                  0.63\n4         -2.7     1.0    0.65       -0.67                  0.82\n5         -2.6     1.1    0.61       -0.73                  0.75\n6         -2.5     1.0    0.60       -0.77                  0.61\n7         -2.8     1.1    0.66       -0.66                  0.52\n8         -2.8     1.2    0.67       -0.57                  0.94\n9         -2.8     1.1    0.65       -0.52                  0.84\n10        -2.6     1.1    0.67       -0.85                  0.70\n   b_raceOtherHispanic b_raceNHBlack b_raceOtherDMulti\n1                0.605          0.52              0.95\n2                0.338          0.45              0.69\n3                0.566          0.63              0.54\n4                0.453          0.61              0.78\n5                0.090          0.50              0.62\n6                0.015          0.48              0.60\n7                0.736          0.50              0.84\n8                0.913          0.57              1.07\n9                0.570          0.66              0.81\n10               0.467          0.54              0.97\n# ... with 3990 more draws, and 3 more variables\n# ... hidden reserved variables {'.chain', '.iteration', '.draw'}\n\n\nCode\nhead(post)\n\n\n# A draws_df: 6 iterations, 1 chains, and 11 variables\n  b_Intercept b_age_c b_bmi_c b_sexFemale b_raceMexicanAmerican\n1        -2.6     1.1    0.70       -0.71                  0.67\n2        -2.7     1.0    0.62       -0.57                  0.65\n3        -2.6     1.1    0.65       -0.76                  0.63\n4        -2.7     1.0    0.65       -0.67                  0.82\n5        -2.6     1.1    0.61       -0.73                  0.75\n6        -2.5     1.0    0.60       -0.77                  0.61\n  b_raceOtherHispanic b_raceNHBlack b_raceOtherDMulti\n1               0.605          0.52              0.95\n2               0.338          0.45              0.69\n3               0.566          0.63              0.54\n4               0.453          0.61              0.78\n5               0.090          0.50              0.62\n6               0.015          0.48              0.60\n# ... with 3 more variables\n# ... hidden reserved variables {'.chain', '.iteration', '.draw'}\n\n\nCode\nnames(prior_draws)\n\n\n[1] \"term\"     \"estimate\" \"type\"    \n\n\nCode\nlibrary(brms)\nlibrary(tidyr)\n\n# Extract posterior draws\npost &lt;- as_draws_df(bayes_fit) %&gt;%      # bayes_fit = your brms model\n  select(b_bmi_c, b_age_c) %&gt;%               # select your coefficient columns\n  pivot_longer(\n    everything(),\n    names_to = \"term\",\n    values_to = \"estimate\"\n  ) %&gt;%\n  mutate(\n    term = case_when(\n      term == \"b_bmi_c\" ~ \"BMI (per 1 SD)\",\n      term == \"b_age_c\" ~ \"Age (per 1 SD)\"\n    ),\n    type = \"Posterior\"\n  )\n\n## visualization of prior and predicted draws\ncombined_draws &lt;- bind_rows(prior_draws, post) \n\nlibrary(ggplot2)\n\nggplot(combined_draws, aes(x = estimate, fill = type)) +\n  geom_density(alpha = 0.4) +\n  facet_wrap(~ term, scales = \"free\", ncol = 2) +\n  theme_minimal(base_size = 13) +\n  labs(\n    title = \"Prior vs Posterior Distributions\",\n    x = \"Coefficient estimate\",\n    y = \"Density\",\n    fill = \"\"\n  )\n\n\n\n\n\n\n\n\n\nCode\n# Compute proportion of diabetes=1 for each draw\npp_proportion &lt;- rowMeans(pp_samples)  # proportion of 1's in each posterior draw\n\n# Summary of posterior proportions\nsummary(pp_proportion)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n0.09067 0.10533 0.10944 0.10935 0.11320 0.12715 \n\n\nCode\n# Optional: visualize the posterior probability distribution\npp_proportion_df &lt;- tibble(proportion = pp_proportion)\n\nggplot(pp_proportion_df, aes(x = proportion)) +\n  geom_histogram(binwidth = 0.01, fill = \"skyblue\", color = \"black\") +\n  labs(\n    title = \"Posterior Distribution of Proportion of Diabetes = 1\",\n    x = \"Proportion of Diabetes = 1\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nPrior and predicted draws\nPosterior predicted proportion of Diabetes vs NHANES prevalence of Diabetes in the population\n\n\n\nCode\nlibrary(tidyverse)\n\n# Posterior predicted proportion vector\n# pp_proportion &lt;- rowMeans(pp_samples)  # if not already done\n\nknown_prev &lt;- 0.089   # NHANES prevalence\n\n# Posterior summary\nposterior_mean &lt;- mean(pp_proportion)\nposterior_ci &lt;- quantile(pp_proportion, c(0.025, 0.975))  # 95% credible interval\n\n# Create a data frame for plotting\npp_df &lt;- tibble(proportion = pp_proportion)\n\n# Plot\nggplot(pp_df, aes(x = proportion)) +\n  geom_histogram(binwidth = 0.005, fill = \"skyblue\", color = \"black\") +\n  geom_vline(xintercept = known_prev, color = \"red\", linetype = \"dashed\", size = 1) +\n  geom_vline(xintercept = posterior_mean, color = \"blue\", linetype = \"solid\", size = 1) +\n  geom_rect(aes(xmin = posterior_ci[1], xmax = posterior_ci[2], ymin = 0, ymax = Inf),\n            fill = \"blue\", alpha = 0.1, inherit.aes = FALSE) +\n  labs(\n    title = \"Posterior Predicted Diabetes Proportion vs NHANES Prevalence\",\n    subtitle = paste0(\"Red dashed = NHANES prevalence (\", known_prev, \n                      \"), Blue solid = Posterior mean (\", round(posterior_mean,3), \")\"),\n    x = \"Proportion of Diabetes = 1\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(dplyr)\n\n# Posterior predicted proportion\nposterior_mean &lt;- mean(pp_proportion)\nposterior_ci &lt;- quantile(pp_proportion, c(0.025, 0.975))  # 95% credible interval\n\n# NHANES prevalence with SE from survey::svymean\n# Suppose you already have:\n# svymean(~diabetes_dx, nhanes_design_adult, na.rm = TRUE)\nknown_prev &lt;- 0.089        # Mean prevalence\nknown_se   &lt;- 0.0048       # Standard error from survey\n\n# Calculate 95% confidence interval\nknown_ci &lt;- c(\n  known_prev - 1.96 * known_se,\n  known_prev + 1.96 * known_se\n)\n\n# Print results\ndata.frame(\n  Type = c(\"Posterior Prediction\", \"NHANES Prevalence\"),\n  Mean = c(posterior_mean, known_prev),\n  Lower_95 = c(posterior_ci[1], known_ci[1]),\n  Upper_95 = c(posterior_ci[2], known_ci[2])\n)\n\n\n                     Type      Mean   Lower_95  Upper_95\n2.5% Posterior Prediction 0.1093519 0.09781831 0.1212446\n        NHANES Prevalence 0.0890000 0.07959200 0.0984080\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Create a data frame for plotting\nci_df &lt;- data.frame(\n  Type = c(\"Posterior Prediction\", \"NHANES Prevalence\"),\n  Mean = c(0.1096674, 0.089),\n  Lower_95 = c(0.09772443, 0.079592),\n  Upper_95 = c(0.1210658, 0.098408)\n)\n\n# Plot\nggplot(ci_df, aes(x = Type, y = Mean, color = Type)) +\n  geom_point(size = 4) +\n  geom_errorbar(aes(ymin = Lower_95, ymax = Upper_95), width = 0.2) +\n  ylim(0, max(ci_df$Upper_95) + 0.02) +\n  labs(\n    title = \"Comparison of Posterior Predicted Diabetes Proportion vs NHANES Prevalence\",\n    y = \"Proportion of Diabetes\",\n    x = \"\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "index.html#targeted-bmi",
    "href": "index.html#targeted-bmi",
    "title": "Bayesian Logistic Regression - Application in Probability Prediction of disease (Diabetes)",
    "section": "Targeted bmi",
    "text": "Targeted bmi\nIn this analysis, a grid of body mass index (BMI) values is generated to examine the relationship between BMI and the predicted probability of diabetes while holding other covariates (age, sex, and race) constant. - Using the fitted Bayesian logistic regression model, posterior predictive probabilities computed for each BMI value and the mean of posterior draws estimated the average predicted probability of diabetes across the BMI range. - We then tried to identify the BMI value whose predicted probability was closest to a predefined target probability (here, 0.3). - This enabled interpretation of the model in a clinically meaningful way—specifically, by determining the BMI level associated with a 30% predicted probability of diabetes for a given demographic profile. - We were able to link statistical inference to practical health thresholds relevant for risk communication and prevention strategies in translational research."
  },
  {
    "objectID": "index.html#implications",
    "href": "index.html#implications",
    "title": "Bayesian Logistic Regression - Application in Probability Prediction of disease (Diabetes)",
    "section": "Implications",
    "text": "Implications\n\nage and BMI as robust and independent predictors of diabetes, underscore the importance of early targeted interventions in mitigating diabetes risk.\nLongitudinal studies and combining other statistical analytical methods with Bayesian can further enhance and provide better informed precision prevention strategies."
  },
  {
    "objectID": "slides.html#type-2-diabetes-t2d-a-public-health-concern-worldwide",
    "href": "slides.html#type-2-diabetes-t2d-a-public-health-concern-worldwide",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)",
    "section": "Type 2 Diabetes (T2D), a public health concern worldwide",
    "text": "Type 2 Diabetes (T2D), a public health concern worldwide\n\nBody doesn’t make enough insulin or is not used properly → rise in blood sugar\nAffects organs and can lead to multi-organ failure\nUnderstanding risk factors (age, BMI, genetics) allows early intervention"
  },
  {
    "objectID": "slides.html#statistical-problem",
    "href": "slides.html#statistical-problem",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)",
    "section": "Statistical Problem",
    "text": "Statistical Problem\n\nTraditional statistical approaches fail to analyze complex relationships or uncertainty present in healthcare data\nMissing data in survey-collected datasets reduces sample size and may bias estimates"
  },
  {
    "objectID": "slides.html#study-goal",
    "href": "slides.html#study-goal",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)",
    "section": "Study Goal",
    "text": "Study Goal\n\nAddress statistical challenges where traditional methods fail\nCompare Frequentist and Bayesian methods on NHANES 2013–2014\nIdentify associations between risk factors and diabetes (dichotomous outcome)\nDemonstrate model performance and insights for healthcare data analysis"
  },
  {
    "objectID": "slides.html#methods",
    "href": "slides.html#methods",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)",
    "section": "Methods",
    "text": "Methods\n\nExploratory Data Analysis (EDA) of weighted NHANES dataset revealed missing values\nMultivariate linear regression on complete cases resulted in reduced sample size\nMultiple Imputation by Chained Equations (MICE) was applied to handle missing data\nBayesian regression was conducted on imputed data after normalizing age and BMI"
  },
  {
    "objectID": "slides.html#bayesian-regression-principles",
    "href": "slides.html#bayesian-regression-principles",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)",
    "section": "Bayesian Regression Principles",
    "text": "Bayesian Regression Principles\n\nPriors stabilize estimates and reduce extreme values\nHandles uncertainty fully through posterior distributions\nIncorporates prior knowledge for robust inference\nWorks effectively with imputed datasets"
  },
  {
    "objectID": "slides.html#data-source",
    "href": "slides.html#data-source",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)",
    "section": "Data Source",
    "text": "Data Source\n\nNHANES 2013–2014 (CDC)\n\nFiles merged: demographics (DEMO_H), exam (BMX_H), questionnaire (DIQ_H)\n\nCleaned dataset: n = 10,175 observations, 10 variables\n\n\n\n\nDataset\nKey variables\nDimensions\n\n\n\n\ndemo1\nage, race, gender\n10175 × 47\n\n\nexam1\nBMI\n9813 × 224\n\n\nquest1\nDiabetes\n9813 × 953"
  },
  {
    "objectID": "slides.html#data-exploration",
    "href": "slides.html#data-exploration",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)",
    "section": "Data Exploration",
    "text": "Data Exploration\n\nSmall sample for certain BMI subgroups (pre-diabetic n=132/9813)\nOnly 14 complete cases across all variables → imbalance\nSurvey-weighted proportions approximate US population (Male:Female ~48.9%:51%)\nPredictor correlations are hierarchical and complex → suitable for Bayesian modeling"
  },
  {
    "objectID": "slides.html#our-question",
    "href": "slides.html#our-question",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)",
    "section": "Our Question",
    "text": "Our Question\n\nCan Bayesian logistic regression provide more stable and transparent inference than classical MLE for diabetes outcomes in NHANES 2013–2014?"
  },
  {
    "objectID": "slides.html#data-pipeline-reproducible",
    "href": "slides.html#data-pipeline-reproducible",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)",
    "section": "Data Pipeline (Reproducible)",
    "text": "Data Pipeline (Reproducible)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSEQN\nRIDAGEYR\nRIAGENDR\nRIDRETH1\nSDMVPSU\nSDMVSTRA\nWTMEC2YR\nBMXBMI\nDIQ010\nDIQ050\n\n\n\n\n73557\n69\n1\n4\n1\n112\n13481.04\n26.7\n1\n1\n\n\n73558\n54\n1\n3\n1\n108\n24471.77\n28.6\n1\n1\n\n\n73559\n72\n1\n3\n1\n109\n57193.29\n28.9\n1\n1\n\n\n73560\n9\n1\n3\n2\n109\n55766.51\n17.1\n2\n2\n\n\n73561\n73\n2\n3\n2\n116\n65541.87\n19.7\n2\n2\n\n\n73562\n56\n1\n1\n1\n111\n25344.99\n41.7\n2\n2\n\n\n\n\n\n\nAdults &gt;20 years filtered: n = 5,769\nStandardized predictors: age_c, bmi_c\nOutcome adjusted for pregnancy when female\nNH White set as reference level for race\n\n```"
  },
  {
    "objectID": "slides.html#references",
    "href": "slides.html#references",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013–2014)",
    "section": "References",
    "text": "References"
  }
]