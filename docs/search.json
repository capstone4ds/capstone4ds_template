[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayesian Logistic Regression - Application in Probability Prediction of disease (Diabetes)",
    "section": "",
    "text": "Slides: slides.html ( Go to slides.qmd to edit)"
  },
  {
    "objectID": "index.html#aims",
    "href": "index.html#aims",
    "title": "Bayesian Logistic Regression - Application in Probability Prediction of disease (Diabetes)",
    "section": "Aims",
    "text": "Aims\nThis study aims to apply Bayesian logistic regression to predict diabetes status and examine its association with body mass index (BMI), age (‚â•20 years), gender, and race using data from the 2013‚Äì2014 NHANES survey. The NHANES dataset employs a complex sampling design involving stratification, clustering, and oversampling of specific population subgroups. By adopting a Bayesian analytical framework, this study seeks to overcome challenges commonly encountered in traditional logistic regression‚Äîsuch as missing data, separation, and biases introduced by complete case analysis‚Äîthereby improving the efficiency, robustness, and interpretability of model estimates in predicting population-level health outcomes."
  },
  {
    "objectID": "index.html#bayesian-logistic-regression",
    "href": "index.html#bayesian-logistic-regression",
    "title": "Bayesian Logistic Regression - Application in Probability Prediction of disease (Diabetes)",
    "section": "Bayesian Logistic Regression",
    "text": "Bayesian Logistic Regression\nTo estimate the associations between predictors and outcome probabilities. Bayesian framework integrates prior information with observed data to generate posterior distributions, allowing direct probabilistic interpretation of parameters.\nThis approach provides flexibility in model specification, accounts for parameter uncertainty, and produces credible intervals that fully reflect uncertainty in the estimates.\nUnlike traditional frequentist methods, the Bayesian method enables inference through posterior probabilities, supporting more nuanced decision-making and interpretation.\n\nModel Structure\n\nBayesian logistic regression is a probabilistic modeling framework used to estimate the relationship between one or more predictors (continuous or categorical) and a binary outcome (e.g., presence/absence of disease).\n\nIt extends classical logistic regression by combining it with Bayesian inference, treating model parameters as random variables with probability distributions rather than fixed point estimates Leeuw and Klugkist (2012) and Klauenberg et al. (2015)\nBayesian logistic regression models the log-odds of a binary outcome as a linear combination of predictors:\n\n\n\nIn the Bayesian framework, model parameters ( ) are treated as random variables and assigned prior distributions that reflect existing knowledge or plausible ranges before observing the data. After incorporating the observed evidence, the priors are updated through Bayes‚Äô theorem (Leeuw and Klugkist 2012; Klauenberg et al.¬†2015):\nIn the Bayesian framework, the coefficients () are assigned prior distributions, which are updated in light of the observed data to yield posterior distributions.\nThe Bayesian approach naturally incorporates uncertainty in all model parameters.\n\nIt combines prior beliefs with observed data to produce posterior distributions according to Bayes‚Äô theorem: [ ]\n\nLikelihood: is the probability of the observed data given the model parameters (as in classical logistic regression).\n\nPrior: Encodes prior knowledge or beliefs about parameter values before observing the data.\n\nPosterior: is the updated beliefs about parameters after observing the data.\nPosterior ‚ãâ Likelihood * Prior\n\n\nPrior Specification - Regression coefficients prior: normal(0, 2.5) ‚Äî providing weakly informative regularization provide gentle regularization, constraining extreme values without overpowering the data R. van de Schoot et al. (2021) - Intercept prior: Student‚Äôs t-distribution prior, student_t(3, 0, 10) (van de Schoot et al., 2013).\nThis weakly informative prior:\n- Has 3 degrees of freedom** (( = 3 )), producing heavy tails that allow for occasional large effects.\n- Is centered at 0 (( = 0 )), reflecting no initial bias toward positive or negative associations.\n- Has a scale parameter of 10 (( = 10 )), allowing broad variation in possible coefficient values.\nSuch priors improve stability in models with small sample sizes, high collinearity, or potential outliers.\n\n\nAdvantages of Bayesian Logistic Regression\n\nUncertainty quantification: Produces full posterior distributions instead of single-point estimates.\nCredible intervals: Provide the range within which a parameter lies with a specified probability (e.g., 95%).\nFlexible priors: Allow incorporation of expert knowledge or results from previous studies.\nProbabilistic predictions: Posterior predictive distributions yield direct probabilities for future observations.\nComprehensive model checking: Posterior predictive checks (PPCs) evaluate how well simulated outcomes reproduce observed data.\n\n\n\nPosterior Predictions\n\nPosterior distributions of the coefficients are used to estimate the probability of the outcome for given predictor values. This enables statements such as:\n¬†‚ÄúGiven the predictors, the probability of the outcome lies between X% and Y%.‚Äù\nPosterior predictions incorporate two sources of uncertainty:\n\nParameter uncertainty: Variability in estimated model coefficients.\nPredictive uncertainty: Variability in future outcomes given those parameters.\n\nIn Bayesian analysis, every unknown parameter ‚Äî such as a regression coefficient, mean, or variance ‚Äî is treated as a random variable with a probability distribution that expresses uncertainty given the observed data.\n\n\n\nModel Evaluation and Diagnostics\n\nModel quality and convergence are assessed using standard Bayesian diagnostics:\nPosterior inference performed using Markov Chain Monte Carlo (MCMC) sampling via the No-U-Turn Sampler (NUTS), a variant of Hamiltonian Monte Carlo (HMC). Four chains run with sufficient warm-up iterations to ensure convergence. Austin et al. (2021)\nConvergence diagnostics: Markov Chain Monte Carlo (MCMC) performance was evaluated using ( ) (R-hat) and effective sample size (ESS).\nAutocorrelation checks: Ensure independence between successive MCMC draws.\nPosterior predictive checks (PPC): Compare simulated data from posterior distributions to observed outcomes.\nBayesian R¬≤: Quantified the proportion of variance explained by the predictors, incorporating uncertainty."
  },
  {
    "objectID": "index.html#model-structure",
    "href": "index.html#model-structure",
    "title": "Bayesian Logistic Regression - Application in Probability Prediction of disease (Diabetes)",
    "section": "Model Structure",
    "text": "Model Structure\nBayesian logistic regression\n\nBayesian logistic regression is a probabilistic modeling framework used to estimate the relationship between one or more predictors (continuous or categorical) and a binary outcome (e.g., presence/absence of disease).\n\nIt extends classical logistic regression by combining it with Bayesian inference, treating model parameters as random variables with probability distributions rather than fixed point estimates Leeuw and Klugkist (2012) and Klauenberg et al. (2015)\nBayesian logistic regression models the log-odds of a binary outcome as a linear combination of predictors:\n\n\n\nIn the Bayesian framework, model parameters ( ) are treated as random variables and assigned prior distributions that reflect existing knowledge or plausible ranges before observing the data. After incorporating the observed evidence, the priors are updated through Bayes‚Äô theorem (Leeuw and Klugkist 2012; Klauenberg et al.¬†2015):\nIn the Bayesian framework, the coefficients () are assigned prior distributions, which are updated in light of the observed data to yield posterior distributions.\nThe Bayesian approach naturally incorporates uncertainty in all model parameters.\n\nIt combines prior beliefs with observed data to produce posterior distributions according to Bayes‚Äô theorem: [ ]\n\nLikelihood: is the probability of the observed data given the model parameters (as in classical logistic regression).\n\nPrior: Encodes prior knowledge or beliefs about parameter values before observing the data.\n\nPosterior: is the updated beliefs about parameters after observing the data.\nPosterior ‚ãâ Likelihood * Prior\n\n\n\nPrior Specification\nRegression coefficients prior: normal(0, 2.5) ‚Äî providing weakly informative regularization provide gentle regularization, constraining extreme values without overpowering the data R. van de Schoot et al. (2021)\nIntercept prior: Student‚Äôs t-distribution prior, student_t(3, 0, 10) (van de Schoot et al., 2013).\nThis weakly informative prior:\n- Has 3 degrees of freedom (( = 3 )), producing heavy tails that allow for occasional large effects.\n- Is centered at 0 (( = 0 )), reflecting no initial bias toward positive or negative associations.\n- Has a scale parameter of 10 (( = 10 )), allowing broad variation in possible coefficient values.\nSuch priors improve stability in models with small sample sizes, high collinearity, or potential outliers."
  },
  {
    "objectID": "index.html#advantages-of-bayesian-logistic-regression",
    "href": "index.html#advantages-of-bayesian-logistic-regression",
    "title": "Bayesian Logistic Regression - Application in Probability Prediction of disease (Diabetes)",
    "section": "Advantages of Bayesian Logistic Regression",
    "text": "Advantages of Bayesian Logistic Regression\n\nUncertainty quantification: Produces full posterior distributions instead of single-point estimates.\nCredible intervals: Provide the range within which a parameter lies with a specified probability (e.g., 95%).\nFlexible priors: Allow incorporation of expert knowledge or results from previous studies.\nProbabilistic predictions: Posterior predictive distributions yield direct probabilities for future observations.\nComprehensive model checking: Posterior predictive checks (PPCs) evaluate how well simulated outcomes reproduce observed data."
  },
  {
    "objectID": "index.html#posterior-predictions",
    "href": "index.html#posterior-predictions",
    "title": "Bayesian Logistic Regression - Application in Probability Prediction of disease (Diabetes)",
    "section": "Posterior Predictions",
    "text": "Posterior Predictions\n\nPosterior distributions of the coefficients are used to estimate the probability of the outcome for given predictor values. This enables statements such as:\n¬†‚ÄúGiven the predictors, the probability of the outcome lies between X% and Y%.‚Äù\nPosterior predictions incorporate two sources of uncertainty:\n\nParameter uncertainty: Variability in estimated model coefficients.\nPredictive uncertainty: Variability in future outcomes given those parameters.\n\nIn Bayesian analysis, every unknown parameter ‚Äî such as a regression coefficient, mean, or variance ‚Äî is treated as a random variable with a probability distribution that expresses uncertainty given the observed data."
  },
  {
    "objectID": "index.html#model-evaluation-and-diagnostics",
    "href": "index.html#model-evaluation-and-diagnostics",
    "title": "Bayesian Logistic Regression - Application in Probability Prediction of disease (Diabetes)",
    "section": "Model Evaluation and Diagnostics",
    "text": "Model Evaluation and Diagnostics\nModel quality and convergence are assessed using standard Bayesian diagnostics: - Posterior inference performed using Markov Chain Monte Carlo (MCMC) sampling via the No-U-Turn Sampler (NUTS), a variant of Hamiltonian Monte Carlo (HMC). Four chains run with sufficient warm-up iterations to ensure convergence. Austin et al. (2021) - Convergence diagnostics: Markov Chain Monte Carlo (MCMC) performance was evaluated using ( ) (R-hat) and effective sample size (ESS). - Autocorrelation checks: Ensure independence between successive MCMC draws. - Posterior predictive checks (PPC): Compare simulated data from posterior distributions to observed outcomes. - Bayesian R¬≤: Quantified the proportion of variance explained by the predictors, incorporating uncertainty."
  },
  {
    "objectID": "index.html#exploratory-data-analysis-adult-20---80-years",
    "href": "index.html#exploratory-data-analysis-adult-20---80-years",
    "title": "Bayesian Logistic Regression - Application in Probability Prediction of disease (Diabetes)",
    "section": "Exploratory Data Analysis (Adult, 20 - 80 years)",
    "text": "Exploratory Data Analysis (Adult, 20 - 80 years)\n\nExploratory data with initial readings (TABLE) is presented.\n\n\n\n      mean     SE\nage 47.496 0.3805\n\n\n                mean     SE\ndiabetes_dx 0.089016 0.0048\n\n\n            variance     SE\ndiabetes_dx   4759.9 0.0039\n\n\nEffective sample size for diabetes_dx: 48142 \n\n\n\n\nCode\nlibrary(dplyr)\nlibrary(skimr)\nlibrary(knitr)\nlibrary(tidyr)\nlibrary(purrr)\nlibrary(forcats)\nlibrary(kableExtra)\n\nstr(adult)\n\n\n'data.frame':   5769 obs. of  12 variables:\n $ SDMVPSU    : num  1 1 1 2 1 1 2 1 2 2 ...\n $ SDMVSTRA   : num  112 108 109 116 111 114 106 112 112 113 ...\n $ WTMEC2YR   : num  13481 24472 57193 65542 25345 ...\n $ diabetes_dx: num  1 1 1 0 0 0 0 0 0 0 ...\n $ bmi        : num  26.7 28.6 28.9 19.7 41.7 35.7 NA 26.5 22 20.3 ...\n $ age        : num  69 54 72 73 56 61 42 56 65 26 ...\n $ sex        : Factor w/ 2 levels \"Male\",\"Female\": 1 1 1 2 1 2 1 2 1 2 ...\n $ race       : Factor w/ 5 levels \"NH White\",\"Mexican American\",..: 4 1 1 1 2 1 3 1 1 1 ...\n $ DIQ050     : num  1 1 1 2 2 2 2 2 2 2 ...\n $ age_c      : num  1.132 0.278 1.303 1.36 0.392 ...\n $ bmi_c      : num  -0.3359 -0.0703 -0.0283 -1.3144 1.761 ...\n $ bmi_cat    : Factor w/ 6 levels \"&lt;18.5\",\"18.5‚Äì&lt;25\",..: 3 3 3 2 6 5 NA 3 2 2 ...\n\n\nCode\nplot_str(adult)\nhead(adult)\n\n\n  SDMVPSU SDMVSTRA WTMEC2YR diabetes_dx  bmi age    sex             race DIQ050\n1       1      112 13481.04           1 26.7  69   Male         NH Black      1\n2       1      108 24471.77           1 28.6  54   Male         NH White      1\n3       1      109 57193.29           1 28.9  72   Male         NH White      1\n4       2      116 65541.87           0 19.7  73 Female         NH White      2\n5       1      111 25344.99           0 41.7  56   Male Mexican American      2\n6       1      114 61758.65           0 35.7  61 Female         NH White      2\n      age_c       bmi_c  bmi_cat\n1 1.1324183 -0.33588609   25‚Äì&lt;30\n2 0.2783598 -0.07028101   25‚Äì&lt;30\n3 1.3032300 -0.02834336   25‚Äì&lt;30\n4 1.3601672 -1.31443114 18.5‚Äì&lt;25\n5 0.3922343  1.76099614      ‚â•40\n6 0.6769204  0.92224325   35‚Äì&lt;40\n\n\nCode\nplot_intro(adult, title=\"Figure 1 (Adult dataset). Structure of variables and missing observations.\")\n\n\n\n\n\n\n\n\n\nCode\nplot_missing(adult, title=\"Figure 2(Adult dataset). Breakdown of missing observations.\")\n\n\n\n\n\n\n\n\n\n\nData Characteristics and Missingness\nThe merged dataset contains 10,175 participants. We restricted the sample to adults (age ‚â• 20) for subsequent analyses that included demographic and anthropometric measures to provide a representative sample for assessing diabetes risk across the U.S. adult population reflecting the complex, multistage sampling design of NHANES.\nStudy Cohort - 25% of columns are discrete (categorical), 75% are continuous (age and BMI). - 92.7% of rows have complete information for all variables with fully observed data across predictors and outcomes for most participants. - The adult cohort includes standardized variables for age and BMI (age_c, bmi_c), categorical indicators for sex and race/ethnicity, and a binary doctor-diagnosed diabetes. - The adult cohort included 5,769 adults aged 20‚Äì80 years with the mean age of participants was 49.1 years (SD = 17.6), and participants are fairly evenly distributed across adult age groups, with no sharp skewness. - The mean BMI was 29.1 kg/m¬≤ (SD = 7.15), with values ranging from 14.1 to 82.9 kg/m¬≤.A small proportion of missing values in variables (BMI and diabetes status). BMI data were available for 5,520 participants, with 249 missing values. BMI: Most participants have BMI values within the normal to overweight range, with fewer in the obese category. - Sex: Sample includes a higher proportion of females than males.\n\n\nVisualization on prevalence of diabetes\n(1) Diabetes by BMI categories: Individuals diagnosed with diabetes\n    tend to have higher BMI values compared to non-diabetics.\n(2) Diabetesby Age Group: The proportion of diabetes increases with\n    advancing age, highlighting age as a strong risk factor.\n(3) Diabetes by Race/Ethnicity: Differences are observed across\n    racial/ethnic groups, with some showing higher prevalence rates\n    than others.\n(4) Diabetes diagnosis by sex across different racial groups. Bars\n    are side by side for each sex, with counts displayed on top\n\n\nCode\nggplot(adult, aes(x = age)) +\n  geom_histogram(binwidth = 5, fill = \"skyblue\", color = \"white\") +\n  labs(\n    title = \"Distribution of Age &gt;20 years\",\n    x = \"Age (years)\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\nggplot(adult, aes(factor(diabetes_dx))) +\n  geom_bar(fill = \"steelblue\") +\n  labs(title=\"Diabetes Outcome Distribution in &gt;20 years age group\", x=\"diabetes_dx (0=No, 1=Yes)\", y=\"Count\")\n\n\n\n\n\n\n\n\n\nCode\nggplot(adult, aes(factor(bmi_cat))) +\n  geom_bar(fill = \"steelblue\") +\n  labs(title=\"Diabetes Outcome Distribution by BMI in &gt;20 years age group\", x=\"bmi_cat\")\n\n\n\n\n\n\n\n\n\nCode\nggplot(adult, aes(x = factor(diabetes_dx), y = bmi)) +\n  geom_boxplot(fill = \"skyblue\") +\n  labs(\n    title = \"BMI Distribution by Diabetes Diagnosis in &gt;20 years age group\",\n    x = \"Diabetes Diagnosis (0 = No, 1 = Yes)\",\n    y = \"BMI\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# plots for adult data bmi categories and race categories\n\nggplot(adult, aes(x = factor(race), fill = factor(diabetes_dx))) +\n  geom_bar(position = \"dodge\") +\n  labs(\n    title = \"Diabetes Diagnosis by Race in &gt;20 years age group\",\n    x = \"Race/Ethnicity\",\n    y = \"Count\",\n    fill = \"Diabetes Diagnosis\\n(0 = No, 1 = Yes)\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nCode\nggplot(adult, aes(x = factor(bmi_cat), fill = factor(diabetes_dx))) +\n  geom_bar(position = \"dodge\") +\n  labs(\n    title = \"Diabetes Diagnosis by BMI in &gt;20 years age group\",\n    x = \"BMI\",\n    y = \"Count\",\n    fill = \"Diabetes Diagnosis\\n(0 = No, 1 = Yes)\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Example: create your dataset\nadult1 &lt;- data.frame(\n  race = rep(c(\"NH White\",\"Mexican American\",\"Other Hispanic\",\"NH Black\",\"Other/Multi\"), each = 6),\n  sex = rep(c(\"Male\",\"Male\",\"Male\",\"Female\",\"Female\",\"Female\"), times = 5),\n  diabetes_dx = rep(c(0,1,NA,0,1,NA), times = 5),\n  count = c(\n    1019,119,38,1164,96,36,\n    304,60,14,329,49,11,\n    183,26,10,255,25,9,\n    461,100,19,515,65,17,\n    351,46,8,393,32,15\n  )\n)\n\n# Clean NA for plotting or convert to \"Missing\"\nadult1$diabetes_dx &lt;- as.character(adult1$diabetes_dx)\nadult1$diabetes_dx[is.na(adult1$diabetes_dx)] &lt;- \"Missing\"\n\n# Plot grouped bar chart\nggplot(adult1, aes(x = diabetes_dx, y = count, fill = sex)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  facet_wrap(~race) +\n  labs(title = \"Diabetes Diagnosis by Sex and Race\",\n       x = \"Diabetes Diagnosis\",\n       y = \"Count\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"skyblue\", \"orange\"))\n\n\n\n\n\n\n\n\n\n\n\nMissingness in Adult dataset\n\nOnly 1.3% of individual data points are missing across the dataset, reflecting minimal missingness.\nNo column is entirely missing (0%), indicating all variables have at least some observed data.\nOverall missingness: ~4% ‚Üí low, but non-trivial given the small number of variables involved."
  },
  {
    "objectID": "index.html#abnormalities-detected-in-adult-dataset",
    "href": "index.html#abnormalities-detected-in-adult-dataset",
    "title": "Bayesian Logistic Regression - Application in Probability Prediction of disease (Diabetes)",
    "section": "Abnormalities detected in Adult dataset",
    "text": "Abnormalities detected in Adult dataset\n\nMissingness\n\nOnly 1.3% of individual data points are missing across the dataset, reflecting minimal missingness.\nNo column is entirely missing (0%), indicating all variables have at least some observed data.\nOverall missingness: ~4% ‚Üí low, but non-trivial given the small number of variables involved.\nMissingness is not completely at random (MNAR or MAR) - If the probability of missingness depends on other observed variables (e.g., older adults missing BMI due to illness), imputation helps reduce bias. It is possible and should consider MICE and test with logistic regression of missingness indicators\nMissingness affects outcome or key covariates - Even small missingness in important variables can bias posterior estimates. Since BMI and diabetes are central we should perform MICE\nSufficient auxiliary variables available - MICE works best when you have other correlated variables to inform imputation (e.g., age, sex, race, WTMEC2YR).\n\nBayesian model assumes complete data - Standard Bayesian logistic models (e.g., brms, rstanarm) cannot directly handle NAs ‚Äî you must impute or model missingness."
  },
  {
    "objectID": "index.html#multiple-logistic-regression-model-survey-weighted-modeling-for-complete-case-analysis",
    "href": "index.html#multiple-logistic-regression-model-survey-weighted-modeling-for-complete-case-analysis",
    "title": "Bayesian Logistic Regression - Application in Probability Prediction of disease (Diabetes)",
    "section": "Multiple Logistic Regression model (Survey weighted Modeling for complete-case Analysis)",
    "text": "Multiple Logistic Regression model (Survey weighted Modeling for complete-case Analysis)\n\n\nCode\n# Modeling\n\nlibrary(broom)\nlibrary(mice)\nlibrary(brms)\nlibrary(posterior)\nlibrary(bayesplot)\nlibrary(knitr)\n\n# --- Guardrails for modeling ---\nn_outcome &lt;- sum(!is.na(adult$diabetes_dx))\nif (n_outcome == 0) stop(\"Too few non-missing outcomes for modeling. n = 0\")\n\n# Ensure factors and &gt;=2 observed levels among complete outcomes\nadult &lt;- adult %&gt;%\n  dplyr::mutate(\n    sex  = if (!is.factor(sex))  factor(sex)  else sex,\n    race = if (!is.factor(race)) factor(race) else race\n  )\n\nif (nlevels(droplevels(adult$sex[!is.na(adult$diabetes_dx)]))  &lt; 2)\n  stop(\"sex has &lt;2 observed levels after filtering; check data availability.\")\nif (nlevels(droplevels(adult$race[!is.na(adult$diabetes_dx)])) &lt; 2)\n  stop(\"race has &lt;2 observed levels after filtering; check Data Prep.\")\n\n   #  Survey-weighted complete-case \n# Build a logical filter on the original adult data (same length as design$data)\nkeep_cc &lt;- with(\n  adult,\n  !is.na(diabetes_dx) & !is.na(age_c) & !is.na(bmi_c) &\n  !is.na(sex) & !is.na(race)\n)\n\n# Subset the survey design using the logical vector (same length as original)\ndes_cc &lt;- subset(nhanes_design_adult, keep_cc)\n\n# Corresponding complete-case data (optional)\ncc &lt;- adult[keep_cc, ] |&gt; droplevels()\ncat(\"\\nComplete-case N for survey-weighted model:\", nrow(cc), \"\\n\")\n\n\n\nComplete-case N for survey-weighted model: 5349 \n\n\nCode\nprint(table(cc$race))\n\n\n\n        NH White Mexican American   Other Hispanic         NH Black \n            2293              713              470             1101 \n     Other/Multi \n             772 \n\n\nCode\nprint(table(cc$diabetes_dx))\n\n\n\n   0    1 \n4752  597 \n\n\nCode\nprint(table(cc$sex))\n\n\n\n  Male Female \n  2551   2798 \n\n\nCode\nform_cc &lt;- diabetes_dx ~ age_c + bmi_c + sex + race\nsvy_fit &lt;- survey::svyglm(formula = form_cc, design = des_cc, family = quasibinomial())\nsummary(svy_fit)\n\n\n\nCall:\nsvyglm(formula = form_cc, design = des_cc, family = quasibinomial())\n\nSurvey design:\nsubset(nhanes_design_adult, keep_cc)\n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          -2.67143    0.11935 -22.383 1.68e-08 ***\nage_c                 1.10833    0.05042  21.981 1.94e-08 ***\nbmi_c                 0.63412    0.05713  11.099 3.88e-06 ***\nsexFemale            -0.63844    0.10926  -5.843 0.000386 ***\nraceMexican American  0.71091    0.13681   5.196 0.000826 ***\nraceOther Hispanic    0.46469    0.13474   3.449 0.008712 ** \nraceNH Black          0.51221    0.15754   3.251 0.011677 *  \nraceOther/Multi       0.84460    0.17756   4.757 0.001433 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasibinomial family taken to be 0.8455444)\n\nNumber of Fisher Scoring iterations: 6\n\n\n\nStrongest predictors: Age and BMI show the largest ORs among continuous variables‚Äîboth strongly linked to diabetes risk.\nProtective factor: Being female reduces the odds of diabetes.\nRace disparities: All racial/ethnic minority groups show significantly higher odds compared to Non-Hispanic Whites, consistent with known health disparities in diabetes prevalence.\nSignificance: All p-values &lt; 0.05, so all predictors are statistically significant.\n\n\nHandling Missing Data: Multivariate Imputation by Chained Equations (MICE)\n\nVariables with missing data are imputed conditionally on all others through iterative regression models.\nMultiple (m = 5‚Äì10) imputed datasets generated (MICE) are analyzed separately, and combined using Rubin‚Äôs rules to obtain pooled parameter estimates and standard errors.\n\n\n\nBayesian Logistic Regression\n\nBayesian logistic regression model applied to the imputed datasets, with prior distributions incorporated and direct estimation of posterior distributions, credible intervals, and posterior predictive checks. Bayesian inference provided a probabilistic interpretation of parameter uncertainty, complementing the frequentist findings.\n\n\n\nModel Validation and Interpretation\n\nDiagnostic checks performed below evaluate model convergence, goodness-of-fit, and predictive accuracy.\n\n\n\nModel Comparison\n\nThe results from both frameworks (frequentist and Bayesian) are compared to ensure robustness of conclusions regarding predictors of diabetes.\n\n\n\nCode\nsvy_or &lt;- broom::tidy(svy_fit, conf.int = TRUE) %&gt;%\n  dplyr::mutate(OR = exp(estimate), LCL = exp(conf.low), UCL = exp(conf.high)) %&gt;%\n  dplyr::select(term, OR, LCL, UCL, p.value) %&gt;%\n  dplyr::filter(term != \"(Intercept)\")\nknitr::kable(svy_or, caption = \"Survey-weighted odds ratios (per 1 SD)\")\n\n\n\nSurvey-weighted odds ratios (per 1 SD)\n\n\nterm\nOR\nLCL\nUCL\np.value\n\n\n\n\nage_c\n3.0292807\n2.6967690\n3.4027912\n0.0000000\n\n\nbmi_c\n1.8853571\n1.6526296\n2.1508579\n0.0000039\n\n\nsexFemale\n0.5281132\n0.4104905\n0.6794397\n0.0003857\n\n\nraceMexican American\n2.0358434\n1.4850041\n2.7910081\n0.0008262\n\n\nraceOther Hispanic\n1.5915182\n1.1664529\n2.1714810\n0.0087119\n\n\nraceNH Black\n1.6689718\n1.1605895\n2.4000450\n0.0116773\n\n\nraceOther/Multi\n2.3270527\n1.5451752\n3.5045697\n0.0014331"
  },
  {
    "objectID": "index.html#multivariate-imputation-by-chained-equations-pooled-logistic",
    "href": "index.html#multivariate-imputation-by-chained-equations-pooled-logistic",
    "title": "Bayesian Logistic Regression - Application in Probability Prediction of disease (Diabetes)",
    "section": "Multivariate Imputation by Chained Equations (Pooled Logistic",
    "text": "Multivariate Imputation by Chained Equations (Pooled Logistic\nRegression) - We conducted MICE to manage missiging data as an alternative to the Bayesian Approach Buuren and Groothuis-Oudshoorn (2011) - Flatness of the density, heavy tails, non-zero peakedness, skewness and multimodality do not hamper the good performance of multiple imputation for the mean structure in samples n &gt; 400 even for high percentages (75%) of missing data in one variable Van Buuren and Van Buuren (2012). - Multiple Imputation (MI) can be performed using mice package in R - Iterative mice imputes missing values of one variable at a time, using regression models based on the other variables in the dataset. - In the chain process, each imputed variable become a predictor for the subsequent imputation, and the entire process is repeated multiple times to create several complete datasets, each reflecting different possibilities for the missing data.\n\n\nCode\n# ----- Multiple Imputation (predictors only) \nmi_dat &lt;- adult %&gt;%\n  dplyr::select(diabetes_dx, age, bmi, sex, race, WTMEC2YR, SDMVPSU, SDMVSTRA)\n\nmeth &lt;- mice::make.method(mi_dat)\npred &lt;- mice::make.predictorMatrix(mi_dat)\n\n# Do not impute outcome\nmeth[\"diabetes_dx\"] &lt;- \"\"\npred[\"diabetes_dx\", ] &lt;- 0\npred[,\"diabetes_dx\"] &lt;- 1\n\n# Imputation methods\nmeth[\"age\"]  &lt;- \"norm\"\nmeth[\"bmi\"]  &lt;- \"pmm\"\nmeth[\"sex\"]  &lt;- \"polyreg\"\nmeth[\"race\"] &lt;- \"polyreg\"\n\n# Survey design vars as auxiliaries only\nmeth[c(\"WTMEC2YR\",\"SDMVPSU\",\"SDMVSTRA\")] &lt;- \"\"\npred[, c(\"WTMEC2YR\",\"SDMVPSU\",\"SDMVSTRA\")] &lt;- 1\n\nglimpse(mi_dat)\n\n\nRows: 5,769\nColumns: 8\n$ diabetes_dx &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ age         &lt;dbl&gt; 69, 54, 72, 73, 56, 61, 42, 56, 65, 26, 76, 33, 32, 38, 50‚Ä¶\n$ bmi         &lt;dbl&gt; 26.7, 28.6, 28.9, 19.7, 41.7, 35.7, NA, 26.5, 22.0, 20.3, ‚Ä¶\n$ sex         &lt;fct&gt; Male, Male, Male, Female, Male, Female, Male, Female, Male‚Ä¶\n$ race        &lt;fct&gt; NH Black, NH White, NH White, NH White, Mexican American, ‚Ä¶\n$ WTMEC2YR    &lt;dbl&gt; 13481.04, 24471.77, 57193.29, 65541.87, 25344.99, 61758.65‚Ä¶\n$ SDMVPSU     &lt;dbl&gt; 1, 1, 1, 2, 1, 1, 2, 1, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 2, 2‚Ä¶\n$ SDMVSTRA    &lt;dbl&gt; 112, 108, 109, 116, 111, 114, 106, 112, 112, 113, 116, 114‚Ä¶\n\n\nCode\nimp &lt;- mice::mice(mi_dat, m = 5, method = meth, predictorMatrix = pred, seed = 123)\n\n\n\n iter imp variable\n  1   1  bmi\n  1   2  bmi\n  1   3  bmi\n  1   4  bmi\n  1   5  bmi\n  2   1  bmi\n  2   2  bmi\n  2   3  bmi\n  2   4  bmi\n  2   5  bmi\n  3   1  bmi\n  3   2  bmi\n  3   3  bmi\n  3   4  bmi\n  3   5  bmi\n  4   1  bmi\n  4   2  bmi\n  4   3  bmi\n  4   4  bmi\n  4   5  bmi\n  5   1  bmi\n  5   2  bmi\n  5   3  bmi\n  5   4  bmi\n  5   5  bmi\n\n\nMICE Results: - After MICE, the final pooled imputed dataset consisted of 5,769 participants with 8 variables - with missing values were addressed - Five imputations across five iterations each, with BMI imputed conditionally based on other predictors (age, sex, race, and diabetes status). - The iterative process showed stable convergence, indicating reliable estimation of missing BMI values for subsequent survey-weighted and Bayesian modeling analyses.\n\n\nCode\nfit_mi &lt;- with(imp, {\n  age_c &lt;- as.numeric(scale(age))\n  bmi_c &lt;- as.numeric(scale(bmi))\n  glm(diabetes_dx ~ age_c + bmi_c + sex + race, family = binomial())\n})\npool_mi &lt;- pool(fit_mi)\nsummary(pool_mi)\n\n\n                  term   estimate  std.error  statistic       df       p.value\n1          (Intercept) -2.6895645 0.09941301 -27.054453 5566.204 1.486581e-151\n2                age_c  1.0660265 0.05594733  19.054108 5520.446  1.911564e-78\n3                bmi_c  0.5468538 0.04473386  12.224604 5148.557  6.751227e-34\n4            sexFemale -0.6178297 0.09379129  -6.587282 5551.660  4.892566e-11\n5 raceMexican American  0.8877355 0.13750463   6.456041 5472.583  1.167455e-10\n6   raceOther Hispanic  0.5606621 0.17485537   3.206433 5573.987  1.351505e-03\n7         raceNH Black  0.6809629 0.11981185   5.683602 5576.734  1.385727e-08\n8      raceOther/Multi  0.7476406 0.15300663   4.886328 4749.963  1.061140e-06\n\n\nCode\n## table \n\nmi_or &lt;- summary(pool_mi, conf.int = TRUE, exponentiate = TRUE) %&gt;%\n  dplyr::rename(\n    term = term, OR = estimate, LCL = `2.5 %`, UCL = `97.5 %`, p.value = p.value\n  ) %&gt;%\n  dplyr::filter(term != \"(Intercept)\")\nknitr::kable(mi_or, caption = \"MI pooled odds ratios (per 1 SD)\")\n\n\n\nMI pooled odds ratios (per 1 SD)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nOR\nstd.error\nstatistic\ndf\np.value\nLCL\nUCL\nconf.low\nconf.high\n\n\n\n\n2\nage_c\n2.9038183\n0.0559473\n19.054108\n5520.446\n0.0000000\n2.6021752\n3.2404277\n2.6021752\n3.2404277\n\n\n3\nbmi_c\n1.7278084\n0.0447339\n12.224604\n5148.557\n0.0000000\n1.5827382\n1.8861754\n1.5827382\n1.8861754\n\n\n4\nsexFemale\n0.5391132\n0.0937913\n-6.587282\n5551.660\n0.0000000\n0.4485669\n0.6479368\n0.4485669\n0.6479368\n\n\n5\nraceMexican American\n2.4296216\n0.1375046\n6.456041\n5472.583\n0.0000000\n1.8555327\n3.1813298\n1.8555327\n3.1813298\n\n\n6\nraceOther Hispanic\n1.7518320\n0.1748554\n3.206433\n5573.987\n0.0013515\n1.2434346\n2.4680953\n1.2434346\n2.4680953\n\n\n7\nraceNH Black\n1.9757793\n0.1198118\n5.683602\n5576.734\n0.0000000\n1.5621842\n2.4988753\n1.5621842\n2.4988753\n\n\n8\nraceOther/Multi\n2.1120110\n0.1530066\n4.886328\n4749.963\n0.0000011\n1.5646727\n2.8508138\n1.5646727\n2.8508138\n\n\n\n\n\nSurvey-weighted logistic regression coefficients (log-odds scale) -Regression coefficientsfor predictors of diabetes diagnosis (diabetes_dx) with the reference group (Male, Non-Hispanic White, average BMI and age). - Each coefficient (estimate) represents the change in log-odds of diabetes associated with a one-unit increase in the predictor (or compared to the reference group), controlling for all other variables. - Baseline log-odds of diabetes = -2.69 - For each 1 SD increase in age, the log-odds of diabetes increase by 1.07 ‚Üí odds increase by exp(1.07) = 2.9√ó (‚âà3√ó higher odds). - For each 1 SD increase in BMI, odds of diabetes increase by exp(0.55) = 1.73√ó (‚âà73% higher). - Females have exp(-0.62) = 0.54√ó the odds of diabetes compared to males ‚Üí about 46% lower odds. - Mexican Americans have exp(0.89) = 2.43√ó higher odds of diabetes vs.¬†Non-Hispanic Whites. Other Hispanics have exp(0.56) = 1.75√ó higher odds Non-Hispanic Blacks have exp(0.68) = 1.97√ó higher odds. Those identifying as ‚ÄúOther/Multi-racial‚Äù have exp(0.75) = 2.12√ó higher odds of diabetes.\nInterpretation - Age and BMI are strong positive predictors of diabetes ‚Äî each 1 SD increase in these variables substantially raises the odds. - Sex: Females show significantly lower odds compared to males. - Race/Ethnicity: All non-White racial groups have significantly higher odds of diabetes, highlighting persistent disparities in diabetes risk. - Model significance: All predictors are statistically significant (p &lt; 0.01), suggesting a robust association across demographic and health variables.\nGlimpse and statistics (Imputed dataset): - age (mean (SD)) = 48.84 (17.57) - females 2923 (52.3%) &gt; males 2669 (47.7%) - majority being NH White count = 2398 (42.9%) - non-diabetics: Diabetics :: 4974 (88.9%): 618 (11.1%)\n\n\nCode\nlibrary(gt)\n\n# Bayesian Logistic Regression (formula weights) \nadult_imp1 &lt;- complete(imp, 1) %&gt;%\n  dplyr::mutate(\n    age_c  = as.numeric(scale(age)),\n    bmi_c  = as.numeric(scale(bmi)),\n    wt_norm = WTMEC2YR / mean(WTMEC2YR, na.rm = TRUE),\n    # ensure factor refs match survey/mice:\n    race = forcats::fct_relevel(race, \"NH White\"),\n    sex  = forcats::fct_relevel(sex,  \"Male\")\n  ) %&gt;%\n  dplyr::filter(!is.na(diabetes_dx), !is.na(age_c), !is.na(bmi_c),\n                !is.na(sex), !is.na(race)) %&gt;%\n  droplevels()\n\nstopifnot(all(is.finite(adult_imp1$wt_norm)))\n\nglimpse(adult_imp1)\n\n\nRows: 5,592\nColumns: 11\n$ diabetes_dx &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ age         &lt;dbl&gt; 69, 54, 72, 73, 56, 61, 42, 56, 65, 26, 76, 33, 32, 38, 50‚Ä¶\n$ bmi         &lt;dbl&gt; 26.7, 28.6, 28.9, 19.7, 41.7, 35.7, 23.6, 26.5, 22.0, 20.3‚Ä¶\n$ sex         &lt;fct&gt; Male, Male, Male, Female, Male, Female, Male, Female, Male‚Ä¶\n$ race        &lt;fct&gt; NH Black, NH White, NH White, NH White, Mexican American, ‚Ä¶\n$ WTMEC2YR    &lt;dbl&gt; 13481.04, 24471.77, 57193.29, 65541.87, 25344.99, 61758.65‚Ä¶\n$ SDMVPSU     &lt;dbl&gt; 1, 1, 1, 2, 1, 1, 2, 1, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 2, 2‚Ä¶\n$ SDMVSTRA    &lt;dbl&gt; 112, 108, 109, 116, 111, 114, 106, 112, 112, 113, 116, 114‚Ä¶\n$ age_c       &lt;dbl&gt; 1.13241831, 0.27835981, 1.30323001, 1.36016725, 0.39223428‚Ä¶\n$ bmi_c       &lt;dbl&gt; -0.33319172, -0.06755778, -0.02561558, -1.31184309, 1.7639‚Ä¶\n$ wt_norm     &lt;dbl&gt; 0.3393916, 0.6160884, 1.4398681, 1.6500477, 0.6380722, 1.5‚Ä¶\n\n\nCode\nlibrary(tableone)\n\nvars &lt;- c(\"age\", \"bmi\", \"age_c\", \"bmi_c\", \"wt_norm\", \"sex\", \"race\", \"diabetes_dx\")\n\ntable1 &lt;- CreateTableOne(vars = vars, data = adult_imp1, factorVars = c(\"sex\", \"race\", \"diabetes_dx\"))\nprint(table1, showAllLevels = TRUE)\n\n\n                     \n                      level            Overall      \n  n                                     5592        \n  age (mean (SD))                      48.84 (17.57)\n  bmi (mean (SD))                      29.00 (7.11) \n  age_c (mean (SD))                    -0.02 (1.00) \n  bmi_c (mean (SD))                    -0.01 (0.99) \n  wt_norm (mean (SD))                   1.00 (0.79) \n  sex (%)             Male              2669 (47.7) \n                      Female            2923 (52.3) \n  race (%)            NH White          2398 (42.9) \n                      Mexican American   742 (13.3) \n                      Other Hispanic     489 ( 8.7) \n                      NH Black          1141 (20.4) \n                      Other/Multi        822 (14.7) \n  diabetes_dx (%)     0                 4974 (88.9) \n                      1                  618 (11.1) \n\n\n\n\nCode\n## correlation matrix\nlibrary(ggplot2)\nlibrary(reshape2)\n\ncorrelation_matrix &lt;- cor(adult_imp1[, c(\"diabetes_dx\", \"age\", \"bmi\")], use = \"complete.obs\", method = \"pearson\")\ncorrelation_melted &lt;- melt(correlation_matrix)\n\nggplot(correlation_melted, aes(Var1, Var2, fill = value)) +\n  geom_tile(color = \"white\") +\n  scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", midpoint = 0,\n                       limit = c(-1, 1), space = \"Lab\", name = \"Correlation\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(title = \"Correlation Heatmap\", x = \"Features\", y = \"Features\")\n\n\n\n\n\n\n\n\n\nVisualization of the imputed dataset:\n\nCorrelation matrix: Pairwise correlations heatmap: show the strength and direction of correlations (Pearson correlation) which measures linear association between diabetes_dx, age, and bmi\nDiabetes Diagnosis Distribution\nBMI Distribution by Diabetes Status\nPredicted Probability of Diabetes vs BMI\n\n\n\nCode\n# Class distribution\n\nggplot(adult_imp1, aes(x = factor(diabetes_dx))) +\n  geom_bar(fill = \"steelblue\") +\n  labs(\n    title = \"Diabetes Diagnosis Distribution\",\n    x = \"Diabetes Diagnosis (0 = No, 1 = Yes)\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\nprop.table(table(adult_imp1$diabetes_dx))\n\n\n\n       0        1 \n0.889485 0.110515 \n\n\nCode\n# Visualization of Diabetes vs BMI (adult_data1)\n\nlibrary(ggplot2)\n\n# Create the plot\nggplot(adult_imp1, aes(x = factor(diabetes_dx), y = bmi, fill = factor(diabetes_dx))) +\n  geom_boxplot(alpha = 0.7) +\n  scale_x_discrete(labels = c(\"0\" = \"No Diabetes\", \"1\" = \"Diabetes\")) +\n  labs(\n    x = \"Diabetes Diagnosis\",\n    y = \"BMI\",\n    title = \"BMI Distribution by Diabetes Status\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nCode\n# logistic regression curve\nggplot(adult_imp1, aes(x = bmi, y = diabetes_dx)) +\n  geom_point(aes(y = diabetes_dx), alpha = 0.2, position = position_jitter(height = 0.02)) +\n  geom_smooth(method = \"glm\", method.args = list(family = \"binomial\"), se = TRUE, color = \"blue\") +\n  labs(\n    x = \"BMI\",\n    y = \"Probability of Diabetes\",\n    title = \"Predicted Probability of Diabetes vs BMI\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nNumber of study population accross the three datasets\n\nRows: 10175 and Columns: 10 (survey-weighted, merged data)\nRows: 5,769 and Columns: 12 (filtered data, adult)\nRows: 5,592 and Columns: 11 (imputed data, adult_imp1)"
  },
  {
    "objectID": "index.html#predictive-checking-and-validation-of-bayesian-model",
    "href": "index.html#predictive-checking-and-validation-of-bayesian-model",
    "title": "Bayesian Logistic Regression - Application in Probability Prediction of disease (Diabetes)",
    "section": "Predictive checking and validation of Bayesian model",
    "text": "Predictive checking and validation of Bayesian model\n\nPosterior Summaries (mean, median, 95% credible intervals)\nConvergence diagnostics (R-hat, effective sample size)\n\n\nplots to visualizes posterior distributions with high uncertainty, narrow distributions indicating precise estimates.\n\n\nPosterior Odds Ratios provides interpretation of the model coefficients on a multiplicative scale with reference categories: NH White (race), Male (sex).\nPosterior Predictive Checks (PPC) assesses how the model reproduces observed data and validate model fit.\n\n\nVisualizations of generated simulated datasets compared with the observed data show density overlays for both mean and SD. There was no large discrepancies indicating potential misfit; there was good alignment suggesting reliable predictions.\n\n\nMCMC Convergence endures reliable posterior estimates.\n\n\nMCMC Trace plots show chains for each parameter over iterations.\nWell-mixed chains without trends indicate convergence and stable posterior estimates.\n\n\nModel Fit -provided details to quantify predictive performance.\n\n\nThe proportion of variance explained by the model: R¬≤ = 0.13 (13%) shows predictors are relevant but other factors (e.g., genetics, lifestyle, environment) also contribute to outcome variability.\n\n\nCorrelation and Parameter Relationships (Optional)\n\n\nPairwise plots (mcmc_pairs, posterior) ‚Äì explore correlations between parameters.\nHistograms or density plots mcmc_hist() or mcmc_areas() of specific parameters detects no collinearity or dependencies among predictors\n\n\n\nCode\nlibrary(brms)\n\nplot(bayes_fit)   # Posterior distributions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nbayes_R2(bayes_fit)      # Model fit\n\n\n    Estimate  Est.Error      Q2.5    Q97.5\nR2 0.1313342 0.01265055 0.1064607 0.156078\n\n\n\nPosterior Distributions (Left Panels)\n\nAll distributions look smooth and unimodal ‚Üí no multimodality, confirming stable posteriors.\nEach histogram represents the distribution of sampled coefficient values after convergence across all MCMC draws:\nb_raceOtherHispanic: The posterior peaks around 0.4‚Äì0.5, with some spread below 0 and above 1.‚Üí Suggests a modestly positive association with diabetes risk, but some uncertainty (credible interval overlaps 0).\nb_raceNHBlack: Centered around 0.5‚Äì0.6, with a narrower, symmetric shape. ‚Üí Indicates a consistent positive effect‚ÄîNH Black participants have higher odds of diabetes, and uncertainty is low.\nb_raceOtherDMulti: Centered around 0.8‚Äì0.9, with slightly wider spread but entirely above 0. ‚Üí Stronger evidence for increased odds of diabetes among Other/Multi-racial individuals.\n\n\n\nTrace Plots (Right Panels)\n\nEach shows 4 MCMC chains (different colors) across 1000 iterations:\nThe chains mix well and overlap substantially, without visible trends or drifts ‚Üí indicates good convergence.\nThe parameter values oscillate around stable means with no systematic pattern ‚Üí confirms stationarity.\nCombined with Rhat ‚âà 1 and high ESS from your summary, the trace plots visually validate posterior convergence and independence.\n\n\n\nBayesian ùëÖ^2 (model fit statics):\n\nexplains about 13% of the variability in diabetes status, with credible uncertainty bounds suggesting reasonable but modest explanatory power.\nExplains the expected proportion of variance explained, averaged over the posterior distribution of parameters.\n\n\n\nResults from Posterior\n\nBelow is the tabulated format for Bayesian posterior odds ratios (95% CrI) ‚Äî reference: NH White (race), Male (sex)\nThe Bayesian logistic regression model identified significant associations between demographic and anthropometric factors and diabetes diagnosis.\nAge a strong predictor: each standardized unit increase in age was associated with nearly threefold higher odds of diabetes (OR = 2.99; 95% CrI = 2.64‚Äì3.37).\nBMI showed a strong positive association (OR = 1.87; 95% CrI = 1.71‚Äì2.05), higher body mass substantially increased diabetes risk.\nFemale sex had lower odds of diabetes compared to males (OR = 0.52; 95% CrI = 0.42‚Äì0.63).\nCompared with Non-Hispanic Whites (reference group), several racial/ethnic groups had higher odds:\nMexican Americans (OR = 2.00; 95% CrI = 1.41‚Äì2.84)\nNon-Hispanic Blacks (OR = 1.71; 95% CrI = 1.28‚Äì2.27)\nOther/Multi-racial individuals (OR = 2.27; 95% CrI = 1.56‚Äì3.28)\nOther Hispanics showed a positive but non-significant association (OR = 1.54; 95% CrI = 0.93‚Äì2.43).\n\n\n\nCode\n# Posterior ORs (drop intercept, clean labels)\n\nbayes_or &lt;- posterior_summary(bayes_fit, pars = \"^b_\") %&gt;%\n  as.data.frame() %&gt;%\n  tibble::rownames_to_column(\"raw\") %&gt;%\n  dplyr::mutate(\n    term = gsub(\"^b_\", \"\", raw),\n    term = gsub(\"race\", \"race:\", term),\n    term = gsub(\"sex\",  \"sex:\",  term),\n    term = gsub(\"OtherDMulti\", \"Other/Multi\", term),\n    term = gsub(\"OtherHispanic\", \"Other Hispanic\", term),\n    OR   = exp(Estimate),\n    LCL  = exp(Q2.5),\n    UCL  = exp(Q97.5)\n  ) %&gt;%\n  dplyr::select(term, OR, LCL, UCL) %&gt;%\n  dplyr::filter(term != \"Intercept\")\n\nknitr::kable(\n  bayes_or %&gt;%\n    dplyr::mutate(dplyr::across(c(OR,LCL,UCL), ~round(.x, 2))),\n  digits = 2,\n  caption = \"Bayesian posterior odds ratios (95% CrI) ‚Äî reference: NH White (race), Male (sex)\"\n)\n\n\n\nBayesian posterior odds ratios (95% CrI) ‚Äî reference: NH White (race), Male (sex)\n\n\nterm\nOR\nLCL\nUCL\n\n\n\n\nage_c\n2.99\n2.64\n3.37\n\n\nbmi_c\n1.87\n1.71\n2.05\n\n\nsex:Female\n0.52\n0.42\n0.63\n\n\nrace:MexicanAmerican\n2.00\n1.41\n2.84\n\n\nrace:Other Hispanic\n1.54\n0.93\n2.43\n\n\nrace:NHBlack\n1.71\n1.28\n2.27\n\n\nrace:Other/Multi\n2.27\n1.56\n3.28\n\n\n\n\n\n\nAcross all analytic methods‚Äîsurvey-weighted maximum likelihood estimation (MLE), multiple imputation with pooled estimates (MICE), and Bayesian regression‚Äîthe associations between BMI, age, and diabetes diagnosis were consistent in direction and magnitude.\nFor BMI, the odds ratios ranged from 1.73 (95% CI: 1.58‚Äì1.89) in the MICE-pooled model to 1.89 (95% CI: 1.65‚Äì2.15) in the survey-weighted MLE model, and 1.87 (95% CrI: 1.71‚Äì2.05) in the Bayesian model.\nFor age, the estimated odds ratios were 2.90 (95% CI: 2.60‚Äì3.24) using MICE, 3.03 (95% CI: 2.70‚Äì3.40) from the survey-weighted MLE model, and 2.99 (95% CrI: 2.64‚Äì3.37) in the Bayesian analysis.\n\n\n\nCode\n# Results\n\n #Build compact results table (BMI & Age only) \nlibrary(dplyr); \nlibrary(tidyr); \nlibrary(knitr); \nlibrary(stringr)\n\n# pretty \"OR (LCL‚ÄìUCL)\" string\n\n  fmt_or &lt;- function(or, lcl, ucl, digits = 2) {\n  paste0(\n    formatC(or,  format = \"f\", digits = digits), \" (\",\n    formatC(lcl, format = \"f\", digits = digits), \"‚Äì\",\n    formatC(ucl, format = \"f\", digits = digits), \")\"\n  )\n}\n\n# guardrails: require these to exist from Modeling\nstopifnot(exists(\"svy_or\"), exists(\"mi_or\"), exists(\"bayes_or\"))\nfor (nm in c(\"svy_or\",\"mi_or\",\"bayes_or\")) {\n  if (!all(c(\"term\",\"OR\",\"LCL\",\"UCL\") %in% names(get(nm)))) {\n    stop(nm, \" must have columns: term, OR, LCL, UCL\")\n  }\n}\n\nsvy_tbl   &lt;- svy_or   %&gt;% mutate(Model = \"Survey-weighted MLE\")\nmi_tbl    &lt;- mi_or    %&gt;% mutate(Model = \"mice pooled\")\nbayes_tbl &lt;- bayes_or %&gt;% mutate(Model = \"Bayesian\")\n\nall_tbl &lt;- bind_rows(svy_tbl, mi_tbl, bayes_tbl) %&gt;%\n  mutate(term = case_when(\n    str_detect(term, \"bmi_c|\\\\bBMI\\\\b\") ~ \"BMI (per 1 SD)\",\n    str_detect(term, \"age_c|\\\\bAge\\\\b\") ~ \"Age (per 1 SD)\",\n    TRUE ~ term\n  )) %&gt;%\n  filter(term %in% c(\"BMI (per 1 SD)\", \"Age (per 1 SD)\")) %&gt;%\n  mutate(OR_CI = fmt_or(OR, LCL, UCL, digits = 2)) %&gt;%\n  select(Model, term, OR_CI) %&gt;%\n  arrange(\n    factor(Model, levels = c(\"Survey-weighted MLE\",\"mice pooled\",\"Bayesian\")),\n    factor(term,  levels = c(\"BMI (per 1 SD)\",\"Age (per 1 SD)\"))\n  )\n\nres_wide &lt;- all_tbl %&gt;%\n  pivot_wider(names_from = term, values_from = OR_CI) %&gt;%\n  rename(\n    `BMI (per 1 SD) OR (95% CI)` = `BMI (per 1 SD)`,\n    `Age (per 1 SD) OR (95% CI)` = `Age (per 1 SD)`\n  )\n\nkable(\n  res_wide,\n  align = c(\"l\",\"c\",\"c\"),\n  caption = \"Odds ratios (per 1 SD) with 95% CIs across models\"\n)\n\n\n\nOdds ratios (per 1 SD) with 95% CIs across models\n\n\n\n\n\n\n\nModel\nBMI (per 1 SD) OR (95% CI)\nAge (per 1 SD) OR (95% CI)\n\n\n\n\nSurvey-weighted MLE\n1.89 (1.65‚Äì2.15)\n3.03 (2.70‚Äì3.40)\n\n\nmice pooled\n1.73 (1.58‚Äì1.89)\n2.90 (2.60‚Äì3.24)\n\n\nBayesian\n1.87 (1.71‚Äì2.05)\n2.99 (2.64‚Äì3.37)\n\n\n\n\n\n\n\nCode\n# Posterior predictive draws\n\n#Posterior predictive checks (binary outcome)\npp_samples &lt;- posterior_predict(bayes_fit, ndraws = 500)  # 500 draws\n\n# Check dimensions\ndim(pp_samples)  # rows = draws, cols = observations\n\n\n[1]  500 5592"
  },
  {
    "objectID": "index.html#comparative-visualizations-predicted-vs-observed",
    "href": "index.html#comparative-visualizations-predicted-vs-observed",
    "title": "Bayesian Logistic Regression - Application in Probability Prediction of disease (Diabetes)",
    "section": "Comparative Visualizations (Predicted vs observed)",
    "text": "Comparative Visualizations (Predicted vs observed)\nA total of 500 posterior predictive draws were generated for 5,592 observations, producing a simulated distribution of predicted outcomes consistent with the sample size. These draws were used to assess model fit and evaluate how well the Bayesian model reproduced the observed data pattern.\n\n\nCode\n# Plot overlay of observed vs predicted counts (duplicate image)\nppc_dens_overlay(y = adult_imp1$diabetes_dx, yrep = pp_samples[1:50, ]) +\n  labs(title = \"Posterior Predictive Check: Density Overlay\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nPosterior Predictive Check: Density Overlay\n\nmodel‚Äôs predictions align with reality where mean(y_rep) = average predicted probability of diabetes for each individual, across all posterior draws of the parameters. y = the actual observed diabetes status (0 = non-diabetic, 1 = diabetic).\nmcmc dens plots compare observed and posterior parameter values (estimates) for bmi_c, age_c, sex_female, and by race categories (1) Fitted (Predicted) vs observed for bmi using point and error bars (2) Fitted (Predicted) vs observed for bmi using line plot\n\n\n\nCode\nppc_bars(y = adult_imp1$diabetes_dx, yrep = pp_samples[1:50, ])\n\n\n\n\n\n\n\n\n\n\n\nA posterior predictive check (Bar plot)\n- using ppc_bars() compared the observed diabetes outcomes with 50 replicated datasets drawn from the posterior distribution. The replicated distributions closely matched the observed proportions, indicating that the Bayesian model adequately captured the outcome variability and overall data structure\n\n\nCode\n#PP check for proportions (useful for binary) mean comparison to check if the simulated means match the observed mean\n\n## mean\nppc_stat(y = adult_imp1$diabetes_dx, yrep = pp_samples[1:100, ], stat = \"mean\") +\n  labs(title = \"Posterior Predictive Check: Mean of Replicates\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nPosterior Predictive Check: Mean\n-A posterior predictive check was performed on the mean diabetes outcome using 100 replicated datasets from the posterior distribution. The distribution of the simulated means closely aligned with the observed mean, suggesting that the Bayesian model accurately captures the central tendency of the outcome.\n\n\nCode\n#PP check for proportions (useful for binary) mean and sd comparison to check if the simulated means match the observed mean\n\n## sd\nppc_stat(y = adult_imp1$diabetes_dx, yrep = pp_samples[1:100, ], stat = \"sd\") +\n  labs(title = \"PPC: Standard Deviation of Replicates\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nPosterior Predictive Check: Standard Deviation\n- A posterior predictive check was conducted on the standard deviation of diabetes outcomes using 100 replicated datasets from the posterior distribution. The simulated standard deviations closely matched the observed value, indicating that the Bayesian model adequately captures the variability in the outcome data.\n\n\nCode\nlibrary(brms)\nlibrary(dplyr)\nlibrary(posterior)\nlibrary(bayesplot)\nlibrary(ggplot2)\n\n\n# Extract posterior draws as a draws_df # simulate posterior outcomes\npost &lt;- as_draws_df(bayes_fit)\npost\n\n\n# A draws_df: 1000 iterations, 4 chains, and 11 variables\n   b_Intercept b_age_c b_bmi_c b_sexFemale b_raceMexicanAmerican\n1         -2.6     1.1    0.70       -0.71                  0.67\n2         -2.7     1.0    0.62       -0.57                  0.65\n3         -2.6     1.1    0.65       -0.76                  0.63\n4         -2.7     1.0    0.65       -0.67                  0.82\n5         -2.6     1.1    0.61       -0.73                  0.75\n6         -2.5     1.0    0.60       -0.77                  0.61\n7         -2.8     1.1    0.66       -0.66                  0.52\n8         -2.8     1.2    0.67       -0.57                  0.94\n9         -2.8     1.1    0.65       -0.52                  0.84\n10        -2.6     1.1    0.67       -0.85                  0.70\n   b_raceOtherHispanic b_raceNHBlack b_raceOtherDMulti\n1                0.605          0.52              0.95\n2                0.338          0.45              0.69\n3                0.566          0.63              0.54\n4                0.453          0.61              0.78\n5                0.090          0.50              0.62\n6                0.015          0.48              0.60\n7                0.736          0.50              0.84\n8                0.913          0.57              1.07\n9                0.570          0.66              0.81\n10               0.467          0.54              0.97\n# ... with 3990 more draws, and 3 more variables\n# ... hidden reserved variables {'.chain', '.iteration', '.draw'}\n\n\nCode\n# Posterior summary\npost_sum &lt;- posterior_summary(bayes_fit)\npost_sum\n\n\n                           Estimate  Est.Error          Q2.5         Q97.5\nb_Intercept              -2.6644912 0.08841987 -2.840368e+00    -2.4963936\nb_age_c                   1.0936287 0.06218042  9.720147e-01     1.2162894\nb_bmi_c                   0.6267208 0.04755308  5.344481e-01     0.7198901\nb_sexFemale              -0.6586208 0.10156670 -8.592825e-01    -0.4575489\nb_raceMexicanAmerican     0.6916992 0.17744231  3.465940e-01     1.0425831\nb_raceOtherHispanic       0.4314438 0.24442275 -7.159507e-02     0.8867589\nb_raceNHBlack             0.5379213 0.14668730  2.431286e-01     0.8182113\nb_raceOtherDMulti         0.8190024 0.18868276  4.454199e-01     1.1877554\nIntercept                -2.6732989 0.06779455 -2.808774e+00    -2.5477114\nlprior                  -16.5021561 0.05307875 -1.661688e+01   -16.4105283\nlp__                  -1430.3473284 2.03852832 -1.435302e+03 -1427.4169785\n\n\n\n\nPosterior Estimates for BMI and Age (posterior draws analysis)\n\nThe Bayesian model produced posterior estimates for the effects of BMI and age (standardized per 1 SD) on the outcome.\nBMI showed a negative association in most draws, with posterior estimates ranging roughly from 0.61 to 0.70, indicating that higher BMI is associated with lower odds of the outcome in this analysis.\nAge showed a positive association, with posterior estimates ranging roughly from 1.00 to 1.14, suggesting that higher age increases the odds of the outcome.\nThese posterior estimates reflect both the central tendency and variability in the effect of BMI and age, highlighting their roles as important predictors in the model.\n\n\n\nPosterior summary\n\nThe posterior summary of the Bayesian model reports the following for each parameter:\nEstimate: The posterior mean of the coefficient, representing the central tendency of the parameter‚Äôs distribution.\nEst.Error: The posterior standard deviation, quantifying uncertainty around the estimate.\nQ2.5: The 2.5th percentile of the posterior distribution, representing the lower bound of the 95% credible interval.\nQ97.5: The 97.5th percentile of the posterior distribution, representing the upper bound of the 95% credible interval.\n\n\n\nCode\nlibrary(brms)\nlibrary(dplyr)\nlibrary(posterior)\nlibrary(bayesplot)\nlibrary(ggplot2)\n\n# Density overlay for age and bmi\nmcmc_areas(post, pars = c( \"b_age_c\",\"b_bmi_c\",\"b_sexFemale\",\"b_raceMexicanAmerican\", \"b_raceOtherHispanic\",\"b_raceNHBlack\",\"b_raceOtherDMulti\" ))\n\n\n\n\n\n\n\n\n\n\n\nPosterior Distributions of Model Coefficients\n\nUsing 4,000 posterior draws from the Bayesian model (4 chains √ó 1,000 post-warmup draws per chain), the mcmc_areas() plot visualized the posterior distributions of key predictors: age, BMI, sex, and race/ethnicity.\nThe posterior densities show the range and uncertainty of each coefficient.\nThe 95% credible intervals are clearly depicted by the shaded areas, highlighting which predictors have strong evidence of association with diabetes.\nAge and BMI showed positive associations, female sex showed a negative association, and several racial/ethnic groups had elevated odds relative to the reference group.\nThis visualization provides an intuitive overview of both the magnitude and uncertainty of the model‚Äôs estimated effects.\n\n\n\nCode\npredicted &lt;- fitted(bayes_fit, summary = TRUE)\nobserved &lt;- adult_imp1[, c(\"bmi\", \"age\")]\n\n# Plot for **bmi** (obs vs pred)\n\nggplot(data = NULL, aes(x = observed$bmi, y = predicted[, \"Estimate\"])) +\n  geom_point() +\n  geom_errorbar(aes(ymin = predicted[, \"Q2.5\"], ymax = predicted[, \"Q97.5\"])) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"red\") +\n  xlab(\"Observed bmi\") + ylab(\"Predicted bmi\")\n\n\n\n\n\n\n\n\n\n\n\nObserved vs.¬†Predicted BMI\n\nA comparison of observed BMI values with their posterior predicted estimates was performed using the Bayesian model.\nEach point represents an individual‚Äôs observed BMI versus the model‚Äôs predicted mean.\nError bars indicate the 95% credible intervals of the predictions.\nThe dashed red line represents perfect prediction (observed = predicted). - The plot demonstrates that the model‚Äôs predictions generally align with the observed data, with most points closely following the diagonal, indicating good predictive performance for BMI.\n\n\n\nCode\nlibrary(posterior)\nlibrary(dplyr)\nlibrary(tidyr)\n\n# Extract posterior draws as a matrix, then convert to tibble\npost &lt;- as_draws_matrix(bayes_fit) %&gt;%   # safer than as_draws_df for manipulation\n  as.data.frame() %&gt;%\n  select(b_bmi_c, b_age_c) %&gt;%\n  pivot_longer(\n    everything(),\n    names_to = \"term\",\n    values_to = \"estimate\"\n  ) %&gt;%\n  mutate(\n    term = case_when(\n      term == \"b_bmi_c\" ~ \"BMI (per 1 SD)\",\n      term == \"b_age_c\" ~ \"Age (per 1 SD)\"\n    ),\n    type = \"Posterior\"\n  )\nprior_draws &lt;- tibble(\n  term = rep(c(\"BMI (per 1 SD)\", \"Age (per 1 SD)\"), each = 4000),\n  estimate = c(rnorm(4000, 0, 1), rnorm(4000, 0, 1)),\n  type = \"Prior\"\n)\ncombined_draws &lt;- bind_rows(prior_draws, post)\n\n\n\n\nPrior vs Posterior Distributions\nTo assess how the Bayesian model updates beliefs from prior information to posterior estimates, we compared prior vs posterior coefficient distributions for key predictors: BMI and age. 1. Prior Draws - Simulated from a standard normal distribution (mean = 0, SD = 1) for both BMI and age coefficients. Represent initial beliefs about coefficient values before seeing the data. 2. Posterior Draws - Extracted from the fitted model (bayes_fit) for b_bmi_c and b_age_c.¬†- Pivoted to long format and labeled as ‚ÄúPosterior‚Äù. 3. Visualization Combined prior and posterior draws - Plotted density overlays with facets for BMI and age. - Posterior distributions are narrower and often shifted from prior, reflecting information gained from the data. - Differences between prior and posterior highlight the model‚Äôs learning about effect sizes. - Posterior Predictive Proportions of Diabetes - Computed the proportion of diabetes cases (diabetes = 1) for each posterior draw (pp_samples).\nInterpretaion: - Prior vs posterior plots demonstrate that the Bayesian model updates prior beliefs in a data-informed way. - Posterior predictive proportions closely match observed prevalence, supporting model reliability for inference and prediction.\n\n\nCode\nlibrary(ggplot2)\n\nggplot(combined_draws, aes(x = estimate, fill = type)) +\n  geom_density(alpha = 0.4) +\n  facet_wrap(~ term, scales = \"free\", ncol = 2) +\n  theme_minimal(base_size = 13) +\n  labs(\n    title = \"Prior vs Posterior Distributions\",\n    x = \"Coefficient estimate\",\n    y = \"Density\",\n    fill = \"\"\n  )\n\n\n\n\n\n\n\n\n\nPrior vs.¬†Posterior Distributions - We visualized the prior and posterior distributions of the BMI and age coefficients. - Priors were centered at 0, reflecting weak prior beliefs about the direction and magnitude of the effects. - Posteriors were shifted away from 0, indicating that the data provided strong evidence for associations with the outcome. - The density plots highlight the uncertainty and magnitude of the estimated effects, with posterior distributions narrower than the priors, demonstrating that the data meaningfully updated our beliefs. - Faceting by term allows comparison of each predictor‚Äôs prior and posterior distributions.\n\n\nCode\n# Compute proportion of diabetes=1 for each draw\npp_proportion &lt;- rowMeans(pp_samples)  # proportion of 1's in each posterior draw\n\n# Optional: visualize the posterior probability distribution\npp_proportion_df &lt;- tibble(proportion = pp_proportion)\n\nggplot(pp_proportion_df, aes(x = proportion)) +\n  geom_histogram(binwidth = 0.01, fill = \"skyblue\", color = \"black\") +\n  labs(\n    title = \"Posterior Distribution of Proportion of Diabetes = 1\",\n    x = \"Proportion of Diabetes = 1\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nPosterior Distribution of Diabetes Prevalence\n\nA histogram of these values shows the distribution of predicted prevalence calculated for each draw of the Bayesian model (pp_proportion), reflecting uncertainty in the model estimates, central tendency and variability of diabetes prevalence\nMost posterior predictions cluster around 10‚Äì11%, indicating good alignment with the observed/imputed data and demonstrating that the model captures the underlying population pattern.\n\n\n\nCode\nlibrary(dplyr)\nlibrary(knitr)\nsvy_mean &lt;- svymean(~diabetes_dx, nhanes_design_adult, na.rm = TRUE)\n\n# Create summary table\nsummary_table &lt;- tibble(\n  Method = c(\"Survey-weighted mean (NHANES)\", \n             \"Imputed dataset mean\", \n             \"Posterior predictive mean\"),\n  diabetes_mean = c(\n    coef(svy_mean),           # survey-weighted mean\n    mean(adult_imp1$diabetes_dx, na.rm = TRUE),  # imputed dataset\n    mean(pp_proportion)       # posterior predictive mean\n  ),\n  SE = c(\n    SE(svy_mean),             # survey-weighted SE\n    NA,                       # not available for raw mean\n    NA                        # not available for posterior predictive\n  )\n)\n\n# Render table\nkable(summary_table, digits = 4, caption = \"Comparison of Diabetes Prevalence Across Methods\")\n\n\n\nComparison of Diabetes Prevalence Across Methods\n\n\nMethod\ndiabetes_mean\nSE\n\n\n\n\nSurvey-weighted mean (NHANES)\n0.0890\n0.0048\n\n\nImputed dataset mean\n0.1105\nNA\n\n\nPosterior predictive mean\n0.1089\nNA\n\n\n\n\n\n\n\nComparison of Diabetes Prevalence Across Methods\n\nThe estimated prevalence of diabetes was consistent across different analytical approaches:\nSurvey-weighted mean (NHANES): 8.9% (SE = 0.0048)\nImputed dataset mean (MICE): 11.1%\nPosterior predictive mean (Bayesian model): 10.95%\n\nThese results indicate that multiple imputation and Bayesian posterior predictions yield slightly higher prevalence estimates than the raw survey-weighted mean, but all methods are broadly consistent. The posterior predictive distribution closely matches the observed prevalence, suggesting that the Bayesian model is well-calibrated.\n\nBayesian model predicts that about 10‚Äì11% of this population has diabetes, with a relatively narrow range across posterior draws, reflects uncertainty in the estimate\nWhile most predictions cluster around 10‚Äì11%, the model allows for values as low as 8.5% and as high as 12.8%.\nOn comparing this with the raw imputed data proportion show that the the model predictions align with the observed/imputed data.\n\nThe predicted proportion incorporates uncertainty from both the Bayesian model and the imputed data, providing a more robust estimate of diabetes prevalence.\nThese results suggest that approximately 1 in 10 adults in this population may have diabetes, which can help policymakers and clinicians plan and prioritize targeted interventions effectively.\n\n\nCode\nlibrary(tidyverse)\n\n# Posterior predicted proportion vector\n# pp_proportion &lt;- rowMeans(pp_samples)  # if not already done\n\nknown_prev &lt;- 0.089   # NHANES prevalence\n\n# Posterior summary\nposterior_mean &lt;- mean(pp_proportion)\nposterior_ci &lt;- quantile(pp_proportion, c(0.025, 0.975))  # 95% credible interval\n\n# Create a data frame for plotting\npp_df &lt;- tibble(proportion = pp_proportion)\n\n# Plot\nggplot(pp_df, aes(x = proportion)) +\n  geom_histogram(binwidth = 0.005, fill = \"skyblue\", color = \"black\") +\n  geom_vline(xintercept = known_prev, color = \"red\", linetype = \"dashed\", size = 1) +\n  geom_vline(xintercept = posterior_mean, color = \"blue\", linetype = \"solid\", size = 1) +\n  geom_rect(aes(xmin = posterior_ci[1], xmax = posterior_ci[2], ymin = 0, ymax = Inf),\n            fill = \"blue\", alpha = 0.1, inherit.aes = FALSE) +\n  labs(\n    title = \"Posterior Predicted Diabetes Proportion vs NHANES Prevalence\",\n    subtitle = paste0(\"Red dashed = NHANES prevalence (\", known_prev, \n                      \"), Blue solid = Posterior mean (\", round(posterior_mean,3), \")\"),\n    x = \"Proportion of Diabetes = 1\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nPosterior Predicted Diabetes Proportion vs.¬†NHANES Prevalence\n\nWe compared the posterior predicted proportion of diabetes from the Bayesian model with the observed NHANES prevalence (8.9%).\nThe blue solid line represents the posterior mean, while the shaded blue area indicates the 95% credible interval of predicted proportions.\nThe red dashed line shows the NHANES survey-weighted prevalence for reference.\nMost posterior predictions cluster around 10‚Äì11%, slightly higher than the NHANES mean, but the credible interval overlaps the observed prevalence, indicating good model calibration.\nThis visualization highlights the model‚Äôs ability to capture uncertainty in predictions while remaining consistent with the observed data.\n\n\n\nCode\nlibrary(dplyr)\n\n# Posterior predicted proportion\n\nposterior_mean &lt;- mean(pp_proportion)\nposterior_ci &lt;- quantile(pp_proportion, c(0.025, 0.975))  # 95% credible interval\n\n# NHANES prevalence with SE from survey::svymean\n# Suppose you already have:\n# svymean(~diabetes_dx, nhanes_design_adult, na.rm = TRUE)\nknown_prev &lt;- 0.089        # Mean prevalence\nknown_se   &lt;- 0.0048       # Standard error from survey\n\n# Calculate 95% confidence interval\nknown_ci &lt;- c(\n  known_prev - 1.96 * known_se,\n  known_prev + 1.96 * known_se\n)\n\n# Print results\ndata.frame(\n  Type = c(\"Posterior Prediction\", \"NHANES Prevalence\"),\n  Mean = c(posterior_mean, known_prev),\n  Lower_95 = c(posterior_ci[1], known_ci[1]),\n  Upper_95 = c(posterior_ci[2], known_ci[2])\n)\n\n\n                     Type      Mean   Lower_95  Upper_95\n2.5% Posterior Prediction 0.1089181 0.09629381 0.1216962\n        NHANES Prevalence 0.0890000 0.07959200 0.0984080\n\n\nCode\n# Create a data frame for plotting\nci_df &lt;- data.frame(\n  Type = c(\"Posterior Prediction\", \"NHANES Prevalence\"),\n  Mean = c(0.1096674, 0.089),\n  Lower_95 = c(0.09772443, 0.079592),\n  Upper_95 = c(0.1210658, 0.098408)\n)\n\n\n# --- 1. Survey-weighted (Population) prevalence ---\npop_est &lt;- svymean(~diabetes_dx, nhanes_design_adult, na.rm = TRUE)\npop_prev &lt;- as.numeric(pop_est)\npop_se &lt;- as.numeric(SE(pop_est))\npop_ci &lt;- c(pop_prev - 1.96 * pop_se, pop_prev + 1.96 * pop_se)\n\n# --- 2. Bayesian posterior prevalence ---\n# bayes_pred = matrix of posterior draws (iterations √ó individuals)\npp_proportion &lt;- rowMeans(pp_samples)             # prevalence per posterior draw\npost_prev &lt;- mean(pp_proportion)                  # posterior mean prevalence\npost_ci &lt;- quantile(pp_proportion, c(0.025, 0.975))  # 95% credible interval\n\n# --- 3. Combine into one data frame ---\nbar_df &lt;- tibble(\n  Source     = c(\"Survey-weighted (Population)\", \"Bayesian Posterior\"),\n  Prevalence = c(pop_prev, post_prev),\n  CI_low     = c(pop_ci[1], post_ci[1]),\n  CI_high    = c(pop_ci[2], post_ci[2])\n)\n\n# --- 4. Plot ---\nggplot(bar_df, aes(x = Source, y = Prevalence, fill = Source)) +\n  geom_col(alpha = 0.85, width = 0.6) +\n  geom_errorbar(\n    aes(ymin = CI_low, ymax = CI_high),\n    width = 0.15,\n    color = \"black\",\n    linewidth = 0.8\n  ) +\n  guides(fill = \"none\") +\n  labs(\n    title = \"Population vs Posterior Diabetes Prevalence\",\n    subtitle = \"Survey-weighted estimate (design-based) vs Bayesian (model-based)\",\n    y = \"Prevalence (Proportion with Diabetes)\",\n    x = NULL\n  ) +\n  theme_minimal(base_size = 13)\n\n\n\n\n\n\n\n\n\n\n\nComparison of Diabetes Prevalence Across Methods\n\nThe bar plot compares survey-weighted, imputed, and Bayesian posterior estimates of diabetes prevalence:\nSurvey-weighted prevalence: 8.9% (95% CI: 0.080‚Äì0.098), reflecting the NHANES population after accounting for complex sampling.\nImputed (unweighted) prevalence: 11.1%, slightly higher due to unadjusted overrepresentation of subgroups with higher diabetes rates.\nBayesian posterior mean: 10.9% (95% CrI: 0.098‚Äì0.121), closely replicating the imputed data while slightly shrinking toward the population-level mean, consistent with Bayesian regularization.\nThe posterior credible interval overlaps the survey 95% CI, indicating that the Bayesian model reproduces population-level prevalence accurately. This demonstrates good model calibration and predictive validity, while visualizing the uncertainty of both survey-based and model-based estimates.\n\nPractical Implications - Health departments can estimate diabetes burden at the state or county level using Bayesian small-area estimation. - Clinicians and public health researchers can plan targeted screening where predicted prevalence is higher than observed. - Epidemiologists can validate disease models before applying them to regions without survey data.\n\n\nCode\nlibrary(tidyr)\nlibrary(bayesplot)\nlibrary(posterior)\n\n# Convert fitted model to draws array\npost_array &lt;- as_draws_array(bayes_fit)  # draws x chains x parameters\n\n# Plot autocorrelation for age and bmi\nmcmc_acf(post_array, pars = c(\"b_age_c\", \"b_bmi_c\"))\n\n\n\n\n\n\n\n\n\nMCMC Autocorrelation for Key Parameters of Posterior Samples\n\nAutocorrelation plots were generated for the posterior draws of age and BMI coefficients to assess chain mixing and convergence:\nThe plots show the correlation of each draw with its lagged values across iterations.\nRapid decay of autocorrelation toward zero indicates that the Markov chains are mixing well and successive draws are relatively independent.\nBoth age and BMI coefficients exhibited low autocorrelation after a few lags, supporting the reliability of posterior estimates.\nThis diagnostic confirms that the Bayesian model sampling was adequate and stable, ensuring valid inference from the posterior distributions."
  },
  {
    "objectID": "About.html",
    "href": "About.html",
    "title": "About",
    "section": "",
    "text": "Contributors\n\nNamita Mishra ‚Äì analytic coding, content draft, developed project plan, collaborated via GitHub.\nAutumn Wilcox ‚Äì analytic coding, content draft, structured project workflow, collaborated via GitHub.\n\n\nDr.¬†Namita Mishra is a physician, a Head and Neck surgeon and a public health researcher with a strong foundation in medicine, epidemiology, and data science. She is a graduate student in Data Science (Health Analytics).\nHer work focuses on early detection and prevention of non-communicable diseases (cancer, obesity) and on health disparities at community level. She has researched salivary gland tumors, cardiac implants and community based research on healthy food access. Leveraging skills from Data Science, she integrates statistical modeling and Bayesian methods into her analyses. Her Bioinformatics expertise utilizes geodata visualization tools (3D Maps and GIS) for presentations.Passionate about bridging clinical insight with data-driven approaches, dedicated to advancing sustainable, evidence-based solutions in epidemiology and community health.\nOutside work she explores - gardening, cooking, singing, and sewing.\nüìß Contact: nmishra@uwf.edu\n\nAutumn S. Wilcox is a U.S. Navy veteran and Data Science graduate student at the University of West Florida, specializing in Analytics and Modeling. She has over nine years of experience in Network Operations and Technical Writing, including her current role at Navy Federal Credit Union, where she supports enterprise technology and process documentation initiatives. Autumn also holds certification in Clinical Research Quality Management (CRQM) and has contributed to quality oversight and compliance efforts in clinical research settings.\nHer background bridges technology, analytics, and healthcare, with a focus on applying data-driven approaches to improve communication and systems reliability. Outside of work, Autumn enjoys traveling, photography, and finding creative inspiration through music.\nüìß Contact: awr12@students.uwf.edu"
  },
  {
    "objectID": "Introduction.html",
    "href": "Introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Diabetes mellitus (DM) is a major public health concern closely associated with factors such as obesity, age, race, and gender. Identifying these associated risk factors is essential for targeted interventions D‚ÄôAngelo and Ran (2025). Logistic Regression (traditional) that estimates the association between risk factors and outcomes is insufficient in analyzing the complex healthcare data (DNA sequences, imaging, patient-reported outcomes, electronic health records (EHRs), longitudinal health measurements, diagnoses, and treatments. Zeger et al. (2020). Classical maximum likelihood estimation (MLE) yields unstable results in samples that are small, have missing data, or presents quasi- and complete separation.\nBayesian hierarchical models using Markov Chain Monte Carlo (MCMC) allow analysis of multivariate longitudinal healthcare data with repeated measures within individuals and individuals nested in a population. By integrating prior knowledge and including exogenous (e.g., age, clinical history) and endogenous (e.g., current treatment) covariates, Bayesian models provide posterior distributions and risk predictions for conditions such as pneumonia, prostate cancer, and mental disorders. Parametric assumptions remain a limitation of these models.\nIn Bayesian inference Chatzimichail and Hatjimihail (2023), Bayesian inference has shown that parametric models (with fixed parameters) often underperform compared to nonparametric models, which do not assume a prior distribution. Posterior probabilities from Bayesian approaches improve disease classification and better capture heterogeneity in skewed, bimodal, or multimodal data distributions. Bayesian nonparametric models are flexible and robust, integrating multiple diagnostic tests and priors to enhance accuracy and precision, though reliance on prior information and restricted access to resources can limit applicability. Combining Bayesian methods with other statistical or computational approaches helps address systemic biases, incomplete data, and non-representative datasets.\nThe Bayesian framework described by Schoot et al. (2021) highlights the role of priors, data modeling, inference, posterior sampling, variational inference, and variable selection.Proper variable selection mitigates multicollinearity, overfitting, and limited sampling, improving predictive performance. Priors can be informative, weakly informative, or diffuse, and can be elicited from expert opinion, generic knowledge, or data-driven methods. Sensitivity analysis evaluates the alignment of priors with likelihoods, while MCMC simulations (e.g., brms, blavaan in R) empirically estimate posterior distributions. Spatial and temporal Bayesian models have applications in large-scale cancer genomics, identifying molecular interactions, mutational signatures, patient stratification, and cancer evolution, though temporal autocorrelation and subjective prior elicitation can be limiting.\nBayesian normal linear regression has been applied in metrology for instrument calibration using conjugate Normal‚ÄìInverse-Gamma priors Klauenberg et al. (2015). Hierarchical priors add flexibility by modeling uncertainty across multiple levels, improving robustness and interpretability. Bayesian hierarchical/meta-analytic linear regression incorporates both exchangeable and unexchangeable prior information, addressing multiple testing challenges, small sample sizes, and complex relationships among regression parameters across studies Leeuw and Klugkist (2012)\nA sequential clinical reasoning model Liu et al. (2013) Sequential clinical reasoning models demonstrate screening by adding predictors stepwise: (1) demographics, (2) metabolic components, and (3) conventional risk factors, incorporating priors and mimicking clinical evaluation. This approach captures ecological heterogeneity and improves baseline risk estimation, though interactions between predictors and external cross-validation remain limitations.\nBayesian multiple imputation with logistic regression addresses missing data in clinical research Austin et al. (2021) in clinical research by classifying missing values (e.g., patient refusal, loss to follow-up, mechanical errors) as MAR, MNAR, or MCAR. Multiple imputation generates plausible values across datasets and pools results for reliable classification of patient health status and mortality."
  },
  {
    "objectID": "slides.html",
    "href": "slides.html",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013‚Äì2014)",
    "section": "",
    "text": "Body doesn‚Äôt make enough insulin or is not used properly ‚Üí rise in blood sugar\nAffects organs and can lead to multi-organ failure\nUnderstanding risk factors (age, BMI, genetics) allows early intervention"
  },
  {
    "objectID": "slides.html#type-2-diabetes-t2d-a-public-health-concern-worldwide",
    "href": "slides.html#type-2-diabetes-t2d-a-public-health-concern-worldwide",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013‚Äì2014)",
    "section": "Type 2 Diabetes (T2D), a public health concern worldwide",
    "text": "Type 2 Diabetes (T2D), a public health concern worldwide\n\nBody doesn‚Äôt make enough insulin or is not used properly ‚Üí rise in blood sugar\nAffects organs and can lead to multi-organ failure\nUnderstanding risk factors (age, BMI, genetics) allows early intervention"
  },
  {
    "objectID": "slides.html#statistical-problem",
    "href": "slides.html#statistical-problem",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013‚Äì2014)",
    "section": "Statistical Problem",
    "text": "Statistical Problem\n\nTraditional statistical approaches fail to analyze complex relationships or uncertainty present in healthcare data\nMissing data in survey-collected datasets reduces sample size and may bias estimates"
  },
  {
    "objectID": "slides.html#study-goal",
    "href": "slides.html#study-goal",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013‚Äì2014)",
    "section": "Study Goal",
    "text": "Study Goal\n\nAddress statistical challenges where traditional methods fail\nCompare Frequentist and Bayesian methods on NHANES 2013‚Äì2014\nIdentify associations between risk factors and diabetes (dichotomous outcome)\nDemonstrate model performance and insights for healthcare data analysis"
  },
  {
    "objectID": "slides.html#methods",
    "href": "slides.html#methods",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013‚Äì2014)",
    "section": "Methods",
    "text": "Methods\n\nExploratory Data Analysis (EDA) of weighted NHANES dataset revealed missing values\nMultivariate linear regression on complete cases resulted in reduced sample size\nMultiple Imputation by Chained Equations (MICE) was applied to handle missing data\nBayesian regression was conducted on imputed data after normalizing age and BMI"
  },
  {
    "objectID": "slides.html#bayesian-regression-principles",
    "href": "slides.html#bayesian-regression-principles",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013‚Äì2014)",
    "section": "Bayesian Regression Principles",
    "text": "Bayesian Regression Principles\n\nPriors stabilize estimates and reduce extreme values\nHandles uncertainty fully through posterior distributions\nIncorporates prior knowledge for robust inference\nWorks effectively with imputed datasets"
  },
  {
    "objectID": "slides.html#data-source",
    "href": "slides.html#data-source",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013‚Äì2014)",
    "section": "Data Source:",
    "text": "Data Source:\nNational Health and Nutrition Examination Survey (Survey weighted dataset) (2013-2014) (NHANES)\n\nStudy population: Adults (&gt;20 years)(n=5769):\n\nAge: evenly distributed across adult age groups\nBMI: concentrated in the overweight and obese ranges\nFemale¬†&gt; Male¬†participants\nmost participants are non-diabetic, a minority are diabetic: a relatively lower prevalence of diabetes in the sample\n\nPredictors: age, sex, race, BMI\nResponse variable: diabetes_dx (diagnosis: 0 = No, 1 = Yes)\n\n  head(adult)"
  },
  {
    "objectID": "slides.html#data-exploration",
    "href": "slides.html#data-exploration",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013‚Äì2014)",
    "section": "Data Exploration",
    "text": "Data Exploration\n\nSmall sample for certain BMI subgroups (pre-diabetic n=132/9813)\nOnly 14 complete cases across all variables ‚Üí imbalance\nSurvey-weighted proportions approximate US population (Male:Female ~48.9%:51%)\nPredictor correlations are hierarchical and complex ‚Üí suitable for Bayesian modeling"
  },
  {
    "objectID": "slides.html#our-question",
    "href": "slides.html#our-question",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013‚Äì2014)",
    "section": "Our Question",
    "text": "Our Question\n\nCan Bayesian logistic regression provide more stable and transparent inference than classical MLE for diabetes outcomes in NHANES 2013‚Äì2014?"
  },
  {
    "objectID": "slides.html#data-pipeline-reproducible",
    "href": "slides.html#data-pipeline-reproducible",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013‚Äì2014)",
    "section": "Data Pipeline (Reproducible)",
    "text": "Data Pipeline (Reproducible)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSEQN\nRIDAGEYR\nRIAGENDR\nRIDRETH1\nSDMVPSU\nSDMVSTRA\nWTMEC2YR\nBMXBMI\nDIQ010\nDIQ050\n\n\n\n\n73557\n69\n1\n4\n1\n112\n13481.04\n26.7\n1\n1\n\n\n73558\n54\n1\n3\n1\n108\n24471.77\n28.6\n1\n1\n\n\n73559\n72\n1\n3\n1\n109\n57193.29\n28.9\n1\n1\n\n\n73560\n9\n1\n3\n2\n109\n55766.51\n17.1\n2\n2\n\n\n73561\n73\n2\n3\n2\n116\n65541.87\n19.7\n2\n2\n\n\n73562\n56\n1\n1\n1\n111\n25344.99\n41.7\n2\n2\n\n\n\n\n\n\nAdults &gt;20 years filtered: n = 5,769\nStandardized predictors: age_c, bmi_c\nOutcome adjusted for pregnancy when female\nNH White set as reference level for race"
  },
  {
    "objectID": "slides.html#references",
    "href": "slides.html#references",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013‚Äì2014)",
    "section": "References",
    "text": "References\n\nvan Buuren S., Groothuis-Oudshoorn K. (2011). MICE: Multivariate Imputation by Chained Equations in R.\nGelman A., Hill J. (2007). Data Analysis Using Regression and Multilevel/Hierarchical Models.\nNHANES Data Documentation (CDC).\nMcElreath R. (2020). Statistical Rethinking: A Bayesian Course with Examples in R and Stan."
  },
  {
    "objectID": "slides.html#slide-1-introduction",
    "href": "slides.html#slide-1-introduction",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013‚Äì2014)",
    "section": "Slide 1: Introduction",
    "text": "Slide 1: Introduction\n\nDiabetes is a chronic disease with rising global prevalence.\nEarly identification of risk factors is key to prevention and control.\nBayesian methods allow flexible modeling with uncertainty quantification.\nAim: Predict diabetes diagnosis using Bayesian logistic regression with imputed data ."
  },
  {
    "objectID": "slides.html#slide-2-data-source",
    "href": "slides.html#slide-2-data-source",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013‚Äì2014)",
    "section": "Slide 2: Data Source",
    "text": "Slide 2: Data Source\n\nData: National Health and Nutrition Examination Survey (NHANES)\nAdult dataset (&gt;20 years).\nVariables included:\n\nDemographics: age, sex, race\nClinical: BMI\nOutcome: diabetes_dx (diagnosis: 0 = No, 1 = Yes)\nhead(adult)"
  },
  {
    "objectID": "slides.html#slide-3-missing-data-assessment",
    "href": "slides.html#slide-3-missing-data-assessment",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013‚Äì2014)",
    "section": "Slide 3: Missing Data Assessment",
    "text": "Slide 3: Missing Data Assessment\n\nOverall missingness: ~4%, No variable completely missing, Missingness is not uniform\nMissingness pattern: likely MAR (Missing At Random).\nClustered mainly in bmi and diabetes_dx.\nDecision: Apply Multiple Imputation by Chained Equations (MICE)."
  },
  {
    "objectID": "slides.html#slide-4-mice-imputation",
    "href": "slides.html#slide-4-mice-imputation",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013‚Äì2014)",
    "section": "Slide 4: MICE Imputation",
    "text": "Slide 4: MICE Imputation\n\nMethod: Predictive mean matching (PMM) for continuous vars; logistic regression for binary.\nIterations: 5 imputations, 10 iterations each, combined imputed datasets using Rubin‚Äôs rules.\nDistribution plots confirmed consistency with the original data."
  },
  {
    "objectID": "slides.html#slide-5-bayesian-logistic-regression",
    "href": "slides.html#slide-5-bayesian-logistic-regression",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013‚Äì2014)",
    "section": "Slide 5: Bayesian Logistic Regression",
    "text": "Slide 5: Bayesian Logistic Regression\nOutcome: diabetes_dx (0 = non-diabetic, 1 = diabetic)\nPredictors: age_c, bmi_c, sex, race.\n\nIntercept prior: student_t(3, 0, 10) ‚Äî allows heavy tails for flexibility in the intercept estimate. R. V. D. Schoot et al. (2013)\nRegression coefficients prior: normal(0, 2.5) ‚Äî providing weakly informative regularization, constraining extreme values without overpowering the data. R. van de Schoot et al. (2021)\nImplemented in brms (Stan backend), Posterior draws = 4000 (4 chains √ó 1000 iterations). Logistic link function"
  },
  {
    "objectID": "slides.html#slide-6-model-equation",
    "href": "slides.html#slide-6-model-equation",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013‚Äì2014)",
    "section": "Slide 6: Model Equation",
    "text": "Slide 6: Model Equation\n\\[\n\\text{logit}(P(Y=1)) = \\beta_0 + \\beta_1 \\text{age}_c + \\beta_2 \\text{bmi}_c + \\beta_3 \\text{sex} + \\beta_4 \\text{race}\n\\]\n\n( P(Y=1) ): Probability of being diabetic\nCoefficients estimated with posterior means and 95% credible intervals."
  },
  {
    "objectID": "slides.html#slide-7-model-diagnostics",
    "href": "slides.html#slide-7-model-diagnostics",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013‚Äì2014)",
    "section": "Slide 7: Model Diagnostics",
    "text": "Slide 7: Model Diagnostics\n\nRhat ‚âà 1.00 ‚Üí convergence achieved.\nBulk ESS &gt; 3000 for all parameters ‚Üí good mixing.\nTrace plots showed stable sampling across chains.\nPosterior distributions were unimodal and well-centered."
  },
  {
    "objectID": "slides.html#slide-8-posterior-estimates",
    "href": "slides.html#slide-8-posterior-estimates",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013‚Äì2014)",
    "section": "Slide 8: Posterior Estimates",
    "text": "Slide 8: Posterior Estimates\n\n\n\n\n\n\n\n\n\nPredictor\nEstimate\n95% CI\nInterpretation\n\n\n\n\nIntercept\n-2.66\n[-2.84, -2.50]\nBaseline log-odds\n\n\nAge_c\n1.09\n[0.97, 1.22]\n‚Üë age increases diabetes risk\n\n\nBMI_c\n0.88\n[0.76, 1.01]\nHigher BMI linked with higher risk\n\n\nHTN\n0.65\n[0.50, 0.81]\nHypertension predicts diabetes"
  },
  {
    "objectID": "slides.html#slide-9-posterior-predictive-distribution",
    "href": "slides.html#slide-9-posterior-predictive-distribution",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013‚Äì2014)",
    "section": "Slide 9: Posterior Predictive Distribution",
    "text": "Slide 9: Posterior Predictive Distribution\nlibrary(ggplot2)\n\nggplot(combined_draws, aes(x = estimate, fill = type)) +\n  geom_density(alpha = 0.4) +\n  facet_wrap(~ term, scales = \"free\", ncol = 2) +\n  theme_minimal(base_size = 13) +\n  labs(\n    title = \"Prior vs Posterior Distributions\",\n    x = \"Coefficient estimate\",\n    y = \"Density\",\n    fill = \"\"\n  )"
  },
  {
    "objectID": "slides.html#slide-10-model-interpretation",
    "href": "slides.html#slide-10-model-interpretation",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013‚Äì2014)",
    "section": "Slide 10: Model Interpretation",
    "text": "Slide 10: Model Interpretation\n\nStrong positive relationship between age, BMI, and diabetes probability.\nPosterior predictive checks confirm the model captures data patterns well.\nImputation reduced bias and improved model robustness."
  },
  {
    "objectID": "slides.html#slide-11-limitations",
    "href": "slides.html#slide-11-limitations",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013‚Äì2014)",
    "section": "Slide 11: Limitations",
    "text": "Slide 11: Limitations\n\nNHANES data are cross-sectional ‚Üí no causal inference.\nPotential unmeasured confounding (diet, physical activity).\nLimited predictors ‚Üí simplified model structure.\nImputation assumes MAR; violations may introduce bias."
  },
  {
    "objectID": "slides.html#slide-12-conclusion",
    "href": "slides.html#slide-12-conclusion",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013‚Äì2014)",
    "section": "Slide 12: Conclusion",
    "text": "Slide 12: Conclusion\n\nBayesian logistic regression effectively models uncertainty.\nMICE improved data completeness and reliability.\nPosterior predictions provide interpretable probabilities for diabetes risk.\nFramework adaptable to other health outcomes (e.g., hypertension, obesity)."
  },
  {
    "objectID": "slides.html#slide-13-references",
    "href": "slides.html#slide-13-references",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013‚Äì2014)",
    "section": "Slide 13: References",
    "text": "Slide 13: References\n\nvan Buuren S., Groothuis-Oudshoorn K. (2011). MICE: Multivariate Imputation by Chained Equations in R.\nGelman A., Hill J. (2007). Data Analysis Using Regression and Multilevel/Hierarchical Models.\nNHANES Data Documentation (CDC).\nMcElreath R. (2020). Statistical Rethinking: A Bayesian Course with Examples in R and Stan."
  },
  {
    "objectID": "slides.html#slide-14-acknowledgements",
    "href": "slides.html#slide-14-acknowledgements",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013‚Äì2014)",
    "section": "Slide 14: Acknowledgements",
    "text": "Slide 14: Acknowledgements\n\nFaculty: Dr.¬†Ashraf Cohen, PhD, MS\nUniversity of West Florida, Department: Mathematics and Statistics\n        Thanks for the guidance"
  },
  {
    "objectID": "slides.html#slide-5-model-equation",
    "href": "slides.html#slide-5-model-equation",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013‚Äì2014)",
    "section": "Slide 5: Model Equation",
    "text": "Slide 5: Model Equation\nMultivariate Imputation by Chained Equations (Pooled Logistic Regression)\n fit_mi &lt;- with(imp, { age_c &lt;- as.numeric(scale(age)) bmi_c &lt;- as.numeric(scale(bmi)) glm(diabetes_dx ~ age_c + bmi_c + sex + race, family = binomial()) }) pool_mi &lt;- pool(fit_mi) summary(pool_mi)` |\n\n\n\n\n\n\n## Slide 6: Model Equation\n\n\n\\[\n\\text{logit}(P(Y=1)) = \\beta_0 + \\beta_1 \\text{age}_c + \\beta_2 \\text{bmi}_c + \\beta_3 \\text{sex} + \\beta_4 \\text{race}\n\\]\n\n\n- ( P(Y=1) ): Probability of being diabetic - Coefficients estimated with posterior means and 95% credible intervals."
  },
  {
    "objectID": "slides.html#slide-5-multivariate-imputation-by-chained-equations-pooled-logistic-regression",
    "href": "slides.html#slide-5-multivariate-imputation-by-chained-equations-pooled-logistic-regression",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013‚Äì2014)",
    "section": "Slide 5: Multivariate Imputation by Chained Equations (Pooled Logistic Regression)",
    "text": "Slide 5: Multivariate Imputation by Chained Equations (Pooled Logistic Regression)\n  glm(diabetes_dx ~ age_c + bmi_c + sex + race, family = binomial()) }) pool_mi &lt;- pool(fit_mi) \n\nn=5,769 participants\nModel significance: All predictors are statistically significant (p &lt; 0.01), suggesting a robust association across demographic and health variables."
  },
  {
    "objectID": "slides.html#slide-6-bayesian-model-equation",
    "href": "slides.html#slide-6-bayesian-model-equation",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013‚Äì2014)",
    "section": "Slide 6: Bayesian Model Equation",
    "text": "Slide 6: Bayesian Model Equation\nlibrary(gt)\npriors &lt;- c(\n  set_prior(\"normal(0, 2.5)\", class = \"b\"),\n  set_prior(\"student_t(3, 0, 10)\", class = \"Intercept\") \n)\nbayes_fit &lt;- brm(\n  formula = diabetes_dx | weights(wt_norm) ~ age_c + bmi_c + sex + race,\n  data    = adult_imp1,\n  family  = bernoulli(link = \"logit\"),\n  prior   = priors,\n  chains  = 4, iter = 2000, seed = 123,\n  control = list(adapt_delta = 0.95),\n  refresh = 0   # quiet Stan output\n)\n\\[\n\\text{logit}(P(Y=1)) = \\beta_0 + \\beta_1 \\text{age}_c + \\beta_2 \\text{bmi}_c + \\beta_3 \\text{sex} + \\beta_4 \\text{race}\n\\]\n\n( P(Y=1) ): Probability of being diabetic\nCoefficients estimated with posterior means and 95% credible intervals."
  },
  {
    "objectID": "slides.html#slide-7-bayesian-model-diagnostics",
    "href": "slides.html#slide-7-bayesian-model-diagnostics",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013‚Äì2014)",
    "section": "Slide 7: Bayesian Model Diagnostics",
    "text": "Slide 7: Bayesian Model Diagnostics\n\nRhat ‚âà 1.00 ‚Üí convergence achieved.\nBulk ESS &gt; 3000 for all parameters ‚Üí good mixing.\nTrace plots showed stable sampling across chains.\nPosterior distributions were unimodal and well-centered."
  },
  {
    "objectID": "index.html#multivariate-imputation-by-chained-equations-pooled-logistic-regression",
    "href": "index.html#multivariate-imputation-by-chained-equations-pooled-logistic-regression",
    "title": "Bayesian Logistic Regression - Application in Probability Prediction of disease (Diabetes)",
    "section": "Multivariate Imputation by Chained Equations (Pooled Logistic Regression)",
    "text": "Multivariate Imputation by Chained Equations (Pooled Logistic Regression)\nMICE, as an alternative to the Bayesian Approach Buuren and Groothuis-Oudshoorn (2011), manages missiging data.\n\nFlatness of the density, heavy tails, non-zero peakedness, skewness and multimodality do not hamper performance of multiple imputation for the mean structure (n &gt; 400) even for high percentages (75%) of missing data in one variable Van Buuren and Van Buuren (2012).\nMultiple Imputation (MI) performed using mice R package, imputes missing values of one variable at a time, using regression models based on the other variables in the dataset.\nIn the chain process, each imputed variable become a predictor for the subsequent imputation, and the entire process is repeated multiple times to create several complete datasets, each reflecting different possibilities for the missing data.\n\n\n\nCode\n# ----- Multiple Imputation (predictors only) \nmi_dat &lt;- adult %&gt;%\n  dplyr::select(diabetes_dx, age, bmi, sex, race, WTMEC2YR, SDMVPSU, SDMVSTRA)\n\nmeth &lt;- mice::make.method(mi_dat)\npred &lt;- mice::make.predictorMatrix(mi_dat)\n\n# Do not impute outcome\nmeth[\"diabetes_dx\"] &lt;- \"\"\npred[\"diabetes_dx\", ] &lt;- 0\npred[,\"diabetes_dx\"] &lt;- 1\n\n# Imputation methods\nmeth[\"age\"]  &lt;- \"norm\"\nmeth[\"bmi\"]  &lt;- \"pmm\"\nmeth[\"sex\"]  &lt;- \"polyreg\"\nmeth[\"race\"] &lt;- \"polyreg\"\n\n# Survey design vars as auxiliaries only\nmeth[c(\"WTMEC2YR\",\"SDMVPSU\",\"SDMVSTRA\")] &lt;- \"\"\npred[, c(\"WTMEC2YR\",\"SDMVPSU\",\"SDMVSTRA\")] &lt;- 1\n\nglimpse(mi_dat)\n\n\nRows: 5,769\nColumns: 8\n$ diabetes_dx &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ age         &lt;dbl&gt; 69, 54, 72, 73, 56, 61, 42, 56, 65, 26, 76, 33, 32, 38, 50‚Ä¶\n$ bmi         &lt;dbl&gt; 26.7, 28.6, 28.9, 19.7, 41.7, 35.7, NA, 26.5, 22.0, 20.3, ‚Ä¶\n$ sex         &lt;fct&gt; Male, Male, Male, Female, Male, Female, Male, Female, Male‚Ä¶\n$ race        &lt;fct&gt; NH Black, NH White, NH White, NH White, Mexican American, ‚Ä¶\n$ WTMEC2YR    &lt;dbl&gt; 13481.04, 24471.77, 57193.29, 65541.87, 25344.99, 61758.65‚Ä¶\n$ SDMVPSU     &lt;dbl&gt; 1, 1, 1, 2, 1, 1, 2, 1, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 2, 2‚Ä¶\n$ SDMVSTRA    &lt;dbl&gt; 112, 108, 109, 116, 111, 114, 106, 112, 112, 113, 116, 114‚Ä¶\n\n\nCode\nimp &lt;- mice::mice(mi_dat, m = 5, method = meth, predictorMatrix = pred, seed = 123)\n\n\n\n iter imp variable\n  1   1  bmi\n  1   2  bmi\n  1   3  bmi\n  1   4  bmi\n  1   5  bmi\n  2   1  bmi\n  2   2  bmi\n  2   3  bmi\n  2   4  bmi\n  2   5  bmi\n  3   1  bmi\n  3   2  bmi\n  3   3  bmi\n  3   4  bmi\n  3   5  bmi\n  4   1  bmi\n  4   2  bmi\n  4   3  bmi\n  4   4  bmi\n  4   5  bmi\n  5   1  bmi\n  5   2  bmi\n  5   3  bmi\n  5   4  bmi\n  5   5  bmi\n\n\nResults from MICE:\n\nAfter MICE, the final pooled imputed dataset consisted of 5,769 participants with 8 variables with missing values were addressed\nFive imputations across five iterations each, with BMI imputed conditionally based on other predictors (age, sex, race, and diabetes status).\nThe iterative process showed stable convergence, indicating reliable estimation of missing BMI values for subsequent survey-weighted and Bayesian modeling analyses.\n\n\n\nCode\nfit_mi &lt;- with(imp, {\n  age_c &lt;- as.numeric(scale(age))\n  bmi_c &lt;- as.numeric(scale(bmi))\n  glm(diabetes_dx ~ age_c + bmi_c + sex + race, family = binomial())\n})\npool_mi &lt;- pool(fit_mi)\nsummary(pool_mi)\n\n\n                  term   estimate  std.error  statistic       df       p.value\n1          (Intercept) -2.6895645 0.09941301 -27.054453 5566.204 1.486581e-151\n2                age_c  1.0660265 0.05594733  19.054108 5520.446  1.911564e-78\n3                bmi_c  0.5468538 0.04473386  12.224604 5148.557  6.751227e-34\n4            sexFemale -0.6178297 0.09379129  -6.587282 5551.660  4.892566e-11\n5 raceMexican American  0.8877355 0.13750463   6.456041 5472.583  1.167455e-10\n6   raceOther Hispanic  0.5606621 0.17485537   3.206433 5573.987  1.351505e-03\n7         raceNH Black  0.6809629 0.11981185   5.683602 5576.734  1.385727e-08\n8      raceOther/Multi  0.7476406 0.15300663   4.886328 4749.963  1.061140e-06\n\n\nCode\n## table \n\nmi_or &lt;- summary(pool_mi, conf.int = TRUE, exponentiate = TRUE) %&gt;%\n  dplyr::rename(\n    term = term, OR = estimate, LCL = `2.5 %`, UCL = `97.5 %`, p.value = p.value\n  ) %&gt;%\n  dplyr::filter(term != \"(Intercept)\")\nknitr::kable(mi_or, caption = \"MI pooled odds ratios (per 1 SD)\")\n\n\n\nMI pooled odds ratios (per 1 SD)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nOR\nstd.error\nstatistic\ndf\np.value\nLCL\nUCL\nconf.low\nconf.high\n\n\n\n\n2\nage_c\n2.9038183\n0.0559473\n19.054108\n5520.446\n0.0000000\n2.6021752\n3.2404277\n2.6021752\n3.2404277\n\n\n3\nbmi_c\n1.7278084\n0.0447339\n12.224604\n5148.557\n0.0000000\n1.5827382\n1.8861754\n1.5827382\n1.8861754\n\n\n4\nsexFemale\n0.5391132\n0.0937913\n-6.587282\n5551.660\n0.0000000\n0.4485669\n0.6479368\n0.4485669\n0.6479368\n\n\n5\nraceMexican American\n2.4296216\n0.1375046\n6.456041\n5472.583\n0.0000000\n1.8555327\n3.1813298\n1.8555327\n3.1813298\n\n\n6\nraceOther Hispanic\n1.7518320\n0.1748554\n3.206433\n5573.987\n0.0013515\n1.2434346\n2.4680953\n1.2434346\n2.4680953\n\n\n7\nraceNH Black\n1.9757793\n0.1198118\n5.683602\n5576.734\n0.0000000\n1.5621842\n2.4988753\n1.5621842\n2.4988753\n\n\n8\nraceOther/Multi\n2.1120110\n0.1530066\n4.886328\n4749.963\n0.0000011\n1.5646727\n2.8508138\n1.5646727\n2.8508138\n\n\n\n\n\nDescroptive Statistics of Imputed dataset - age (mean (SD)) = 48.84 (17.57) - females 2923 (52.3%) &gt; males 2669 (47.7%) - majority being NH White count = 2398 (42.9%) - non-diabetics: Diabetics :: 4974 (88.9%):618 (11.1%)\n\n\nCode\nlibrary(gt)\n\n# Bayesian Logistic Regression (formula weights) \nadult_imp1 &lt;- complete(imp, 1) %&gt;%\n  dplyr::mutate(\n    age_c  = as.numeric(scale(age)),\n    bmi_c  = as.numeric(scale(bmi)),\n    wt_norm = WTMEC2YR / mean(WTMEC2YR, na.rm = TRUE),\n    # ensure factor refs match survey/mice:\n    race = forcats::fct_relevel(race, \"NH White\"),\n    sex  = forcats::fct_relevel(sex,  \"Male\")\n  ) %&gt;%\n  dplyr::filter(!is.na(diabetes_dx), !is.na(age_c), !is.na(bmi_c),\n                !is.na(sex), !is.na(race)) %&gt;%\n  droplevels()\n\nstopifnot(all(is.finite(adult_imp1$wt_norm)))\n\nglimpse(adult_imp1)\n\n\nRows: 5,592\nColumns: 11\n$ diabetes_dx &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ age         &lt;dbl&gt; 69, 54, 72, 73, 56, 61, 42, 56, 65, 26, 76, 33, 32, 38, 50‚Ä¶\n$ bmi         &lt;dbl&gt; 26.7, 28.6, 28.9, 19.7, 41.7, 35.7, 23.6, 26.5, 22.0, 20.3‚Ä¶\n$ sex         &lt;fct&gt; Male, Male, Male, Female, Male, Female, Male, Female, Male‚Ä¶\n$ race        &lt;fct&gt; NH Black, NH White, NH White, NH White, Mexican American, ‚Ä¶\n$ WTMEC2YR    &lt;dbl&gt; 13481.04, 24471.77, 57193.29, 65541.87, 25344.99, 61758.65‚Ä¶\n$ SDMVPSU     &lt;dbl&gt; 1, 1, 1, 2, 1, 1, 2, 1, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 2, 2‚Ä¶\n$ SDMVSTRA    &lt;dbl&gt; 112, 108, 109, 116, 111, 114, 106, 112, 112, 113, 116, 114‚Ä¶\n$ age_c       &lt;dbl&gt; 1.13241831, 0.27835981, 1.30323001, 1.36016725, 0.39223428‚Ä¶\n$ bmi_c       &lt;dbl&gt; -0.33319172, -0.06755778, -0.02561558, -1.31184309, 1.7639‚Ä¶\n$ wt_norm     &lt;dbl&gt; 0.3393916, 0.6160884, 1.4398681, 1.6500477, 0.6380722, 1.5‚Ä¶\n\n\nCode\nlibrary(tableone)\n\nvars &lt;- c(\"age\", \"bmi\", \"age_c\", \"bmi_c\", \"wt_norm\", \"sex\", \"race\", \"diabetes_dx\")\n\ntable1 &lt;- CreateTableOne(vars = vars, data = adult_imp1, factorVars = c(\"sex\", \"race\", \"diabetes_dx\"))\nprint(table1, showAllLevels = TRUE)\n\n\n                     \n                      level            Overall      \n  n                                     5592        \n  age (mean (SD))                      48.84 (17.57)\n  bmi (mean (SD))                      29.00 (7.11) \n  age_c (mean (SD))                    -0.02 (1.00) \n  bmi_c (mean (SD))                    -0.01 (0.99) \n  wt_norm (mean (SD))                   1.00 (0.79) \n  sex (%)             Male              2669 (47.7) \n                      Female            2923 (52.3) \n  race (%)            NH White          2398 (42.9) \n                      Mexican American   742 (13.3) \n                      Other Hispanic     489 ( 8.7) \n                      NH Black          1141 (20.4) \n                      Other/Multi        822 (14.7) \n  diabetes_dx (%)     0                 4974 (88.9) \n                      1                  618 (11.1) \n\n\n\n\nCode\n## correlation matrix\nlibrary(ggplot2)\nlibrary(reshape2)\n\ncorrelation_matrix &lt;- cor(adult_imp1[, c(\"diabetes_dx\", \"age\", \"bmi\")], use = \"complete.obs\", method = \"pearson\")\ncorrelation_melted &lt;- melt(correlation_matrix)\n\nggplot(correlation_melted, aes(Var1, Var2, fill = value)) +\n  geom_tile(color = \"white\") +\n  scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", midpoint = 0,\n                       limit = c(-1, 1), space = \"Lab\", name = \"Correlation\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(title = \"Correlation Heatmap\", x = \"Features\", y = \"Features\")\n\n\n\n\n\n\n\n\n\nVisualization of the imputed dataset:\n\nCorrelation matrix: Pairwise correlations heatmap: show the strength and direction of correlations (Pearson correlation) which measures linear association between diabetes_dx, age, and bmi\nDiabetes Diagnosis Distribution\nBMI Distribution by Diabetes Status\nPredicted Probability of Diabetes vs BMI\n\n\n\nCode\n# Class distribution\n\nggplot(adult_imp1, aes(x = factor(diabetes_dx))) +\n  geom_bar(fill = \"steelblue\") +\n  labs(\n    title = \"Diabetes Diagnosis Distribution\",\n    x = \"Diabetes Diagnosis (0 = No, 1 = Yes)\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\nprop.table(table(adult_imp1$diabetes_dx))\n\n\n\n       0        1 \n0.889485 0.110515 \n\n\nCode\n# Visualization of Diabetes vs BMI (adult_data1)\n\nlibrary(ggplot2)\n\n# Create the plot\nggplot(adult_imp1, aes(x = factor(diabetes_dx), y = bmi, fill = factor(diabetes_dx))) +\n  geom_boxplot(alpha = 0.7) +\n  scale_x_discrete(labels = c(\"0\" = \"No Diabetes\", \"1\" = \"Diabetes\")) +\n  labs(\n    x = \"Diabetes Diagnosis\",\n    y = \"BMI\",\n    title = \"BMI Distribution by Diabetes Status\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nCode\n# logistic regression curve\nggplot(adult_imp1, aes(x = bmi, y = diabetes_dx)) +\n  geom_point(aes(y = diabetes_dx), alpha = 0.2, position = position_jitter(height = 0.02)) +\n  geom_smooth(method = \"glm\", method.args = list(family = \"binomial\"), se = TRUE, color = \"blue\") +\n  labs(\n    x = \"BMI\",\n    y = \"Probability of Diabetes\",\n    title = \"Predicted Probability of Diabetes vs BMI\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nStudy population accross the three datasets\n\nRows: 10175 and Columns: 10 (survey-weighted, merged data)\nRows: 5,769 and Columns: 12 (filtered data, adult)\nRows: 5,592 and Columns: 11 (imputed data, adult_imp1)"
  },
  {
    "objectID": "index.html#statistical-modeling",
    "href": "index.html#statistical-modeling",
    "title": "Bayesian Logistic Regression - Application in Probability Prediction of disease (Diabetes)",
    "section": "Statistical Modeling",
    "text": "Statistical Modeling\nAnalytic workflow: Three modeling frameworks were compared using identical predictors (standardized age, BMI, sex, and race4) and the binary outcome diabetes_dx: (1) survey-weighted logistic regression to incorporate the NHANES complex sampling design, (2) multiple imputation (MICE) to address missing BMI values, and (3) Bayesian logistic regression with weakly informative priors to quantify uncertainty.\n\n\nCode\n# Modeling\n\nlibrary(broom)\nlibrary(mice)\nlibrary(brms)\nlibrary(posterior)\nlibrary(bayesplot)\nlibrary(knitr)\n\n# --- Guardrails for modeling ---\nn_outcome &lt;- sum(!is.na(adult$diabetes_dx))\nif (n_outcome == 0) stop(\"Too few non-missing outcomes for modeling. n = 0\")\n\n# Ensure factors and &gt;=2 observed levels among complete outcomes\nadult &lt;- adult %&gt;%\n  dplyr::mutate(\n    sex  = if (!is.factor(sex))  factor(sex)  else sex,\n    race = if (!is.factor(race)) factor(race) else race\n  )\n\nif (nlevels(droplevels(adult$sex[!is.na(adult$diabetes_dx)]))  &lt; 2)\n  stop(\"sex has &lt;2 observed levels after filtering; check data availability.\")\nif (nlevels(droplevels(adult$race[!is.na(adult$diabetes_dx)])) &lt; 2)\n  stop(\"race has &lt;2 observed levels after filtering; check Data Prep.\")\n\n   #  Survey-weighted complete-case \n# Build a logical filter on the original adult data (same length as design$data)\nkeep_cc &lt;- with(\n  adult,\n  !is.na(diabetes_dx) & !is.na(age_c) & !is.na(bmi_c) &\n  !is.na(sex) & !is.na(race)\n)\n\n# Subset the survey design using the logical vector (same length as original)\ndes_cc &lt;- subset(nhanes_design_adult, keep_cc)\n\n# Corresponding complete-case data (optional)\ncc &lt;- adult[keep_cc, ] |&gt; droplevels()\ncat(\"\\nComplete-case N for survey-weighted model:\", nrow(cc), \"\\n\")\n\n\n\nComplete-case N for survey-weighted model: 5349 \n\n\nCode\nprint(table(cc$race))\n\n\n\n        NH White Mexican American   Other Hispanic         NH Black \n            2293              713              470             1101 \n     Other/Multi \n             772 \n\n\nCode\nprint(table(cc$diabetes_dx))\n\n\n\n   0    1 \n4752  597 \n\n\nCode\nprint(table(cc$sex))\n\n\n\n  Male Female \n  2551   2798 \n\n\nCode\nform_cc &lt;- diabetes_dx ~ age_c + bmi_c + sex + race\nsvy_fit &lt;- survey::svyglm(formula = form_cc, design = des_cc, family = quasibinomial())\nsummary(svy_fit)\n\n\n\nCall:\nsvyglm(formula = form_cc, design = des_cc, family = quasibinomial())\n\nSurvey design:\nsubset(nhanes_design_adult, keep_cc)\n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          -2.67143    0.11935 -22.383 1.68e-08 ***\nage_c                 1.10833    0.05042  21.981 1.94e-08 ***\nbmi_c                 0.63412    0.05713  11.099 3.88e-06 ***\nsexFemale            -0.63844    0.10926  -5.843 0.000386 ***\nraceMexican American  0.71091    0.13681   5.196 0.000826 ***\nraceOther Hispanic    0.46469    0.13474   3.449 0.008712 ** \nraceNH Black          0.51221    0.15754   3.251 0.011677 *  \nraceOther/Multi       0.84460    0.17756   4.757 0.001433 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasibinomial family taken to be 0.8455444)\n\nNumber of Fisher Scoring iterations: 6\n\n\n\nMultiple Logistic Regression model (Survey weighted Modeling) and complete-case Analysis\n\nStrongest predictors: Age and BMI show the largest ORs among continuous variables‚Äîboth strongly linked to diabetes risk.\nProtective factor: Being female reduces the odds of diabetes.\nRace disparities: All racial/ethnic minority groups show significantly higher odds compared to Non-Hispanic Whites, consistent with known health disparities in diabetes prevalence.\nSignificance: All p-values &lt; 0.05, so all predictors are statistically significant.\n\nResults from Survey-weighted logistic regression\n\nRegression coefficients for predictors of diabetes diagnosis (diabetes_dx) with the reference group (Male, Non-Hispanic White, average BMI and age) represents the change in log-odds of diabetes associated with a one-unit increase in the predictor (or compared to the reference group), controlling for all other variables.\nBaseline log-odds of diabetes = -2.69\nFor each 1 SD increase in age, the log-odds of diabetes increase by 1.07 ‚Üí odds increase by exp(1.07) = 2.9√ó (‚âà3√ó higher odds).\nFor each 1 SD increase in BMI, odds of diabetes increase by exp(0.55) = 1.73√ó (‚âà73% higher).\nFemales have exp(-0.62) = 0.54√ó the odds of diabetes compared to males ‚Üí about 46% lower odds.\nMexican Americans have exp(0.89) = 2.43√ó higher odds of diabetes vs. Non-Hispanic Whites. Other Hispanics have exp(0.56) = 1.75√ó higher odds Non-Hispanic Blacks have exp(0.68) = 1.97√ó higher odds. Those identifying as ‚ÄúOther/Multi-racial‚Äù have exp(0.75) = 2.12√ó higher odds of diabetes.\n\nInterpretation - Age and BMI are strong positive predictors of diabetes - Females show significantly lower odds compared to males. - Race/Ethnicity: All non-White racial groups have significantly higher odds of diabetes, highlighting persistent disparities in diabetes risk. - All predictors are statistically significant (p &lt; 0.01), suggesting a robust association across demographic and health variables.\n\n\nCode\nsvy_or &lt;- broom::tidy(svy_fit, conf.int = TRUE) %&gt;%\n  dplyr::mutate(OR = exp(estimate), LCL = exp(conf.low), UCL = exp(conf.high)) %&gt;%\n  dplyr::select(term, OR, LCL, UCL, p.value) %&gt;%\n  dplyr::filter(term != \"(Intercept)\")\nknitr::kable(svy_or, caption = \"Survey-weighted odds ratios (per 1 SD)\")\n\n\n\nSurvey-weighted odds ratios (per 1 SD)\n\n\nterm\nOR\nLCL\nUCL\np.value\n\n\n\n\nage_c\n3.0292807\n2.6967690\n3.4027912\n0.0000000\n\n\nbmi_c\n1.8853571\n1.6526296\n2.1508579\n0.0000039\n\n\nsexFemale\n0.5281132\n0.4104905\n0.6794397\n0.0003857\n\n\nraceMexican American\n2.0358434\n1.4850041\n2.7910081\n0.0008262\n\n\nraceOther Hispanic\n1.5915182\n1.1664529\n2.1714810\n0.0087119\n\n\nraceNH Black\n1.6689718\n1.1605895\n2.4000450\n0.0116773\n\n\nraceOther/Multi\n2.3270527\n1.5451752\n3.5045697\n0.0014331\n\n\n\n\n\n\n\nMultivariate Imputation by Chained Equations (Pooled Logistic Regression)\nMICE, as an alternative to the Bayesian Approach Buuren and Groothuis-Oudshoorn (2011), manages missiging data.\n\nFlatness of the density, heavy tails, non-zero peakedness, skewness and multimodality do not hamper performance of multiple imputation for the mean structure (n &gt; 400) even for high percentages (75%) of missing data in one variable Van Buuren and Van Buuren (2012).\nMultiple Imputation (MI) performed using mice R package, imputes missing values of one variable at a time, using regression models based on the other variables in the dataset.\nIn the chain process, each imputed variable become a predictor for the subsequent imputation, and the entire process is repeated multiple times to create several complete datasets, each reflecting different possibilities for the missing data.\n\n\n\nCode\n# ----- Multiple Imputation (predictors only) \nmi_dat &lt;- adult %&gt;%\n  dplyr::select(diabetes_dx, age, bmi, sex, race, WTMEC2YR, SDMVPSU, SDMVSTRA)\n\nmeth &lt;- mice::make.method(mi_dat)\npred &lt;- mice::make.predictorMatrix(mi_dat)\n\n# Do not impute outcome\nmeth[\"diabetes_dx\"] &lt;- \"\"\npred[\"diabetes_dx\", ] &lt;- 0\npred[,\"diabetes_dx\"] &lt;- 1\n\n# Imputation methods\nmeth[\"age\"]  &lt;- \"norm\"\nmeth[\"bmi\"]  &lt;- \"pmm\"\nmeth[\"sex\"]  &lt;- \"polyreg\"\nmeth[\"race\"] &lt;- \"polyreg\"\n\n# Survey design vars as auxiliaries only\nmeth[c(\"WTMEC2YR\",\"SDMVPSU\",\"SDMVSTRA\")] &lt;- \"\"\npred[, c(\"WTMEC2YR\",\"SDMVPSU\",\"SDMVSTRA\")] &lt;- 1\n\nglimpse(mi_dat)\n\n\nRows: 5,769\nColumns: 8\n$ diabetes_dx &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ age         &lt;dbl&gt; 69, 54, 72, 73, 56, 61, 42, 56, 65, 26, 76, 33, 32, 38, 50‚Ä¶\n$ bmi         &lt;dbl&gt; 26.7, 28.6, 28.9, 19.7, 41.7, 35.7, NA, 26.5, 22.0, 20.3, ‚Ä¶\n$ sex         &lt;fct&gt; Male, Male, Male, Female, Male, Female, Male, Female, Male‚Ä¶\n$ race        &lt;fct&gt; NH Black, NH White, NH White, NH White, Mexican American, ‚Ä¶\n$ WTMEC2YR    &lt;dbl&gt; 13481.04, 24471.77, 57193.29, 65541.87, 25344.99, 61758.65‚Ä¶\n$ SDMVPSU     &lt;dbl&gt; 1, 1, 1, 2, 1, 1, 2, 1, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 2, 2‚Ä¶\n$ SDMVSTRA    &lt;dbl&gt; 112, 108, 109, 116, 111, 114, 106, 112, 112, 113, 116, 114‚Ä¶\n\n\nCode\nimp &lt;- mice::mice(mi_dat, m = 5, method = meth, predictorMatrix = pred, seed = 123)\n\n\n\n iter imp variable\n  1   1  bmi\n  1   2  bmi\n  1   3  bmi\n  1   4  bmi\n  1   5  bmi\n  2   1  bmi\n  2   2  bmi\n  2   3  bmi\n  2   4  bmi\n  2   5  bmi\n  3   1  bmi\n  3   2  bmi\n  3   3  bmi\n  3   4  bmi\n  3   5  bmi\n  4   1  bmi\n  4   2  bmi\n  4   3  bmi\n  4   4  bmi\n  4   5  bmi\n  5   1  bmi\n  5   2  bmi\n  5   3  bmi\n  5   4  bmi\n  5   5  bmi\n\n\nResults from MICE:\n\nAfter MICE, the final pooled imputed dataset consisted of 5,769 participants with 8 variables with missing values were addressed\nFive imputations across five iterations each, with BMI imputed conditionally based on other predictors (age, sex, race, and diabetes status).\nThe iterative process showed stable convergence, indicating reliable estimation of missing BMI values for subsequent survey-weighted and Bayesian modeling analyses.\n\n\n\nCode\nfit_mi &lt;- with(imp, {\n  age_c &lt;- as.numeric(scale(age))\n  bmi_c &lt;- as.numeric(scale(bmi))\n  glm(diabetes_dx ~ age_c + bmi_c + sex + race, family = binomial())\n})\npool_mi &lt;- pool(fit_mi)\nsummary(pool_mi)\n\n\n                  term   estimate  std.error  statistic       df       p.value\n1          (Intercept) -2.6895645 0.09941301 -27.054453 5566.204 1.486581e-151\n2                age_c  1.0660265 0.05594733  19.054108 5520.446  1.911564e-78\n3                bmi_c  0.5468538 0.04473386  12.224604 5148.557  6.751227e-34\n4            sexFemale -0.6178297 0.09379129  -6.587282 5551.660  4.892566e-11\n5 raceMexican American  0.8877355 0.13750463   6.456041 5472.583  1.167455e-10\n6   raceOther Hispanic  0.5606621 0.17485537   3.206433 5573.987  1.351505e-03\n7         raceNH Black  0.6809629 0.11981185   5.683602 5576.734  1.385727e-08\n8      raceOther/Multi  0.7476406 0.15300663   4.886328 4749.963  1.061140e-06\n\n\nCode\n## table \n\nmi_or &lt;- summary(pool_mi, conf.int = TRUE, exponentiate = TRUE) %&gt;%\n  dplyr::rename(\n    term = term, OR = estimate, LCL = `2.5 %`, UCL = `97.5 %`, p.value = p.value\n  ) %&gt;%\n  dplyr::filter(term != \"(Intercept)\")\nknitr::kable(mi_or, caption = \"MI pooled odds ratios (per 1 SD)\")\n\n\n\nMI pooled odds ratios (per 1 SD)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nOR\nstd.error\nstatistic\ndf\np.value\nLCL\nUCL\nconf.low\nconf.high\n\n\n\n\n2\nage_c\n2.9038183\n0.0559473\n19.054108\n5520.446\n0.0000000\n2.6021752\n3.2404277\n2.6021752\n3.2404277\n\n\n3\nbmi_c\n1.7278084\n0.0447339\n12.224604\n5148.557\n0.0000000\n1.5827382\n1.8861754\n1.5827382\n1.8861754\n\n\n4\nsexFemale\n0.5391132\n0.0937913\n-6.587282\n5551.660\n0.0000000\n0.4485669\n0.6479368\n0.4485669\n0.6479368\n\n\n5\nraceMexican American\n2.4296216\n0.1375046\n6.456041\n5472.583\n0.0000000\n1.8555327\n3.1813298\n1.8555327\n3.1813298\n\n\n6\nraceOther Hispanic\n1.7518320\n0.1748554\n3.206433\n5573.987\n0.0013515\n1.2434346\n2.4680953\n1.2434346\n2.4680953\n\n\n7\nraceNH Black\n1.9757793\n0.1198118\n5.683602\n5576.734\n0.0000000\n1.5621842\n2.4988753\n1.5621842\n2.4988753\n\n\n8\nraceOther/Multi\n2.1120110\n0.1530066\n4.886328\n4749.963\n0.0000011\n1.5646727\n2.8508138\n1.5646727\n2.8508138\n\n\n\n\n\nDescriptive Statistics of Imputed dataset - age (mean (SD)) = 48.84 (17.57) - females 2923 (52.3%) &gt; males 2669 (47.7%) - majority being NH White count = 2398 (42.9%) - non-diabetics: Diabetics :: 4974 (88.9%):618 (11.1%)\n\n\nCode\nlibrary(gt)\n\n# Bayesian Logistic Regression (formula weights) \nadult_imp1 &lt;- complete(imp, 1) %&gt;%\n  dplyr::mutate(\n    age_c  = as.numeric(scale(age)),\n    bmi_c  = as.numeric(scale(bmi)),\n    wt_norm = WTMEC2YR / mean(WTMEC2YR, na.rm = TRUE),\n    # ensure factor refs match survey/mice:\n    race = forcats::fct_relevel(race, \"NH White\"),\n    sex  = forcats::fct_relevel(sex,  \"Male\")\n  ) %&gt;%\n  dplyr::filter(!is.na(diabetes_dx), !is.na(age_c), !is.na(bmi_c),\n                !is.na(sex), !is.na(race)) %&gt;%\n  droplevels()\n\nstopifnot(all(is.finite(adult_imp1$wt_norm)))\n\nglimpse(adult_imp1)\n\n\nRows: 5,592\nColumns: 11\n$ diabetes_dx &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ age         &lt;dbl&gt; 69, 54, 72, 73, 56, 61, 42, 56, 65, 26, 76, 33, 32, 38, 50‚Ä¶\n$ bmi         &lt;dbl&gt; 26.7, 28.6, 28.9, 19.7, 41.7, 35.7, 23.6, 26.5, 22.0, 20.3‚Ä¶\n$ sex         &lt;fct&gt; Male, Male, Male, Female, Male, Female, Male, Female, Male‚Ä¶\n$ race        &lt;fct&gt; NH Black, NH White, NH White, NH White, Mexican American, ‚Ä¶\n$ WTMEC2YR    &lt;dbl&gt; 13481.04, 24471.77, 57193.29, 65541.87, 25344.99, 61758.65‚Ä¶\n$ SDMVPSU     &lt;dbl&gt; 1, 1, 1, 2, 1, 1, 2, 1, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 2, 2‚Ä¶\n$ SDMVSTRA    &lt;dbl&gt; 112, 108, 109, 116, 111, 114, 106, 112, 112, 113, 116, 114‚Ä¶\n$ age_c       &lt;dbl&gt; 1.13241831, 0.27835981, 1.30323001, 1.36016725, 0.39223428‚Ä¶\n$ bmi_c       &lt;dbl&gt; -0.33319172, -0.06755778, -0.02561558, -1.31184309, 1.7639‚Ä¶\n$ wt_norm     &lt;dbl&gt; 0.3393916, 0.6160884, 1.4398681, 1.6500477, 0.6380722, 1.5‚Ä¶\n\n\nCode\nlibrary(tableone)\n\nvars &lt;- c(\"age\", \"bmi\", \"age_c\", \"bmi_c\", \"wt_norm\", \"sex\", \"race\", \"diabetes_dx\")\n\ntable1 &lt;- CreateTableOne(vars = vars, data = adult_imp1, factorVars = c(\"sex\", \"race\", \"diabetes_dx\"))\nprint(table1, showAllLevels = TRUE)\n\n\n                     \n                      level            Overall      \n  n                                     5592        \n  age (mean (SD))                      48.84 (17.57)\n  bmi (mean (SD))                      29.00 (7.11) \n  age_c (mean (SD))                    -0.02 (1.00) \n  bmi_c (mean (SD))                    -0.01 (0.99) \n  wt_norm (mean (SD))                   1.00 (0.79) \n  sex (%)             Male              2669 (47.7) \n                      Female            2923 (52.3) \n  race (%)            NH White          2398 (42.9) \n                      Mexican American   742 (13.3) \n                      Other Hispanic     489 ( 8.7) \n                      NH Black          1141 (20.4) \n                      Other/Multi        822 (14.7) \n  diabetes_dx (%)     0                 4974 (88.9) \n                      1                  618 (11.1) \n\n\n\n\nCode\n## correlation matrix\nlibrary(ggplot2)\nlibrary(reshape2)\n\ncorrelation_matrix &lt;- cor(adult_imp1[, c(\"diabetes_dx\", \"age\", \"bmi\")], use = \"complete.obs\", method = \"pearson\")\ncorrelation_melted &lt;- melt(correlation_matrix)\n\nggplot(correlation_melted, aes(Var1, Var2, fill = value)) +\n  geom_tile(color = \"white\") +\n  scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", midpoint = 0,\n                       limit = c(-1, 1), space = \"Lab\", name = \"Correlation\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(title = \"Correlation Heatmap\", x = \"Features\", y = \"Features\")\n\n\n\n\n\n\n\n\n\nVisualization of the imputed dataset: (1) Correlation matrix: Pairwise correlations heatmap: show the strength and direction of correlations (Pearson correlation) which measures linear association between diabetes_dx, age, and bmi (2) Diabetes Diagnosis Distribution (3) BMI Distribution by Diabetes Status (4) Predicted Probability of Diabetes vs BMI\n\n\nCode\n# Class distribution\n\nggplot(adult_imp1, aes(x = factor(diabetes_dx))) +\n  geom_bar(fill = \"steelblue\") +\n  labs(\n    title = \"Diabetes Diagnosis Distribution\",\n    x = \"Diabetes Diagnosis (0 = No, 1 = Yes)\",\n    y = \"Count\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\nprop.table(table(adult_imp1$diabetes_dx))\n\n\n\n       0        1 \n0.889485 0.110515 \n\n\nCode\n# Visualization of Diabetes vs BMI (adult_data1)\n\nlibrary(ggplot2)\n\n# Create the plot\nggplot(adult_imp1, aes(x = factor(diabetes_dx), y = bmi, fill = factor(diabetes_dx))) +\n  geom_boxplot(alpha = 0.7) +\n  scale_x_discrete(labels = c(\"0\" = \"No Diabetes\", \"1\" = \"Diabetes\")) +\n  labs(\n    x = \"Diabetes Diagnosis\",\n    y = \"BMI\",\n    title = \"BMI Distribution by Diabetes Status\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nCode\n# logistic regression curve\nggplot(adult_imp1, aes(x = bmi, y = diabetes_dx)) +\n  geom_point(aes(y = diabetes_dx), alpha = 0.2, position = position_jitter(height = 0.02)) +\n  geom_smooth(method = \"glm\", method.args = list(family = \"binomial\"), se = TRUE, color = \"blue\") +\n  labs(\n    x = \"BMI\",\n    y = \"Probability of Diabetes\",\n    title = \"Predicted Probability of Diabetes vs BMI\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nStudy population accross the three datasets\n\nRows: 10175 and Columns: 10 (survey-weighted, merged data)\nRows: 5,769 and Columns: 12 (filtered data, adult)\nRows: 5,592 and Columns: 11 (imputed data, adult_imp1)"
  },
  {
    "objectID": "index.html#multiple-logistic-regression-model-survey-weighted-modeling-and-complete-case-analysis",
    "href": "index.html#multiple-logistic-regression-model-survey-weighted-modeling-and-complete-case-analysis",
    "title": "Bayesian Logistic Regression - Application in Probability Prediction of disease (Diabetes)",
    "section": "Multiple Logistic Regression model (Survey weighted Modeling) and complete-case Analysis",
    "text": "Multiple Logistic Regression model (Survey weighted Modeling) and complete-case Analysis\n\nStrongest predictors: Age and BMI show the largest ORs among continuous variables‚Äîboth strongly linked to diabetes risk.\nProtective factor: Being female reduces the odds of diabetes.\nRace disparities: All racial/ethnic minority groups show significantly higher odds compared to Non-Hispanic Whites, consistent with known health disparities in diabetes prevalence.\nSignificance: All p-values &lt; 0.05, so all predictors are statistically significant.\n\nResults from Survey-weighted logistic regression\n\nRegression coefficients for predictors of diabetes diagnosis (diabetes_dx) with the reference group (Male, Non-Hispanic White, average BMI and age) represents the change in log-odds of diabetes associated with a one-unit increase in the predictor (or compared to the reference group), controlling for all other variables.\nBaseline log-odds of diabetes = -2.69\nFor each 1 SD increase in age, the log-odds of diabetes increase by 1.07 ‚Üí odds increase by exp(1.07) = 2.9√ó (‚âà3√ó higher odds).\nFor each 1 SD increase in BMI, odds of diabetes increase by exp(0.55) = 1.73√ó (‚âà73% higher).\nFemales have exp(-0.62) = 0.54√ó the odds of diabetes compared to males ‚Üí about 46% lower odds.\nMexican Americans have exp(0.89) = 2.43√ó higher odds of diabetes vs. Non-Hispanic Whites. Other Hispanics have exp(0.56) = 1.75√ó higher odds Non-Hispanic Blacks have exp(0.68) = 1.97√ó higher odds. Those identifying as ‚ÄúOther/Multi-racial‚Äù have exp(0.75) = 2.12√ó higher odds of diabetes.\n\nInterpretation - Age and BMI are strong positive predictors of diabetes - Females show significantly lower odds compared to males. - Race/Ethnicity: All non-White racial groups have significantly higher odds of diabetes, highlighting persistent disparities in diabetes risk. - All predictors are statistically significant (p &lt; 0.01), suggesting a robust association across demographic and health variables.\n\n\nCode\nsvy_or &lt;- broom::tidy(svy_fit, conf.int = TRUE) %&gt;%\n  dplyr::mutate(OR = exp(estimate), LCL = exp(conf.low), UCL = exp(conf.high)) %&gt;%\n  dplyr::select(term, OR, LCL, UCL, p.value) %&gt;%\n  dplyr::filter(term != \"(Intercept)\")\nknitr::kable(svy_or, caption = \"Survey-weighted odds ratios (per 1 SD)\")\n\n\n\nSurvey-weighted odds ratios (per 1 SD)\n\n\nterm\nOR\nLCL\nUCL\np.value\n\n\n\n\nage_c\n3.0292807\n2.6967690\n3.4027912\n0.0000000\n\n\nbmi_c\n1.8853571\n1.6526296\n2.1508579\n0.0000039\n\n\nsexFemale\n0.5281132\n0.4104905\n0.6794397\n0.0003857\n\n\nraceMexican American\n2.0358434\n1.4850041\n2.7910081\n0.0008262\n\n\nraceOther Hispanic\n1.5915182\n1.1664529\n2.1714810\n0.0087119\n\n\nraceNH Black\n1.6689718\n1.1605895\n2.4000450\n0.0116773\n\n\nraceOther/Multi\n2.3270527\n1.5451752\n3.5045697\n0.0014331"
  },
  {
    "objectID": "index.html#visualization",
    "href": "index.html#visualization",
    "title": "Bayesian Logistic Regression - Application in Probability Prediction of disease (Diabetes)",
    "section": "Visualization",
    "text": "Visualization\n\nDistributions for Coefficients from adult_imp1 data for age and bmi\n\n\nThe density plots of standardized BMI and Age from the imputed dataset (adult_imp1) show approximately normal distributions centered near zero, consistent with z-score standardization confirming that both predictors were properly centered and scaled prior to Bayesian modeling, ensuring comparability and numerical stability during estimation.\n\n\nPrior Distributions for Coefficients\n\n\nPriors for regression coefficients were drawn from a Normal(0, 2.5) distribution, representing weakly informative assumptions centered at zero with moderate spread. The prior density plots for Age (per 1 SD) and BMI (per 1 SD) demonstrate symmetric bell-shaped distributions, indicating no strong bias toward positive or negative effects before observing data.\n\n\n\nCode\nlibrary(ggplot2)\n\n# adult_imp1 plot \n\n# Convert to long format\nadult_long &lt;- adult_imp1 %&gt;%\n  select(bmi_c, age_c) %&gt;%\n  pivot_longer(cols = everything(), names_to = \"Coefficient\", values_to = \"Value\")\n\n# Plot\nggplot(adult_long, aes(x = Value, fill = Coefficient)) +\n  geom_density(alpha = 0.5) +\n  theme_minimal() +\n  labs(title = \"Distributions for Coefficients from adult_imp1 data\",\n       x = \"Coefficient Value\", y = \"Density\") +\n  scale_fill_manual(values = c(\"bmi_c\" = \"skyblue\", \"age_c\" = \"orange\"))\n\n\n\n\n\n\n\n\n\nCode\n## prior draws \n\nprior_draws &lt;- tibble(\n  term = rep(c(\"Age (per 1 SD)\", \"BMI (per 1 SD)\"), each = 4000),\n  value = c(rnorm(4000, 0, 2.5), rnorm(4000, 0, 2.5))\n)\n\n## Plot (prior) (age and bmi) \nggplot(prior_draws, aes(x = value, fill = term)) +\n  geom_density(alpha = 0.5) +\n  theme_minimal() +\n  labs(title = \"Prior Distributions for Coefficients\",\n       x = \"Coefficient Value\", y = \"Density\") +\n  scale_fill_manual(values = c(\"skyblue\", \"orange\"))\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(brms)\n\nplot(bayes_fit)   # Posterior distributions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictive checking\n\n\nPosterior Distributions (Left Panels)\n\nplots visualizes posterior distributions with high uncertainty, narrow distributions indicating precise estimates.\nAll distributions look smooth and unimodal ‚Üí no multimodality, confirming stable posteriors.\nEach histogram represents the distribution of sampled coefficient values after convergence across all MCMC draws:\nb_raceOtherHispanic: The posterior peaks around 0.4‚Äì0.5, with some spread below 0 and above 1.‚Üí Suggests a modestly positive association with diabetes risk, but some uncertainty (credible interval overlaps 0).\nb_raceNHBlack: Centered around 0.5‚Äì0.6, with a narrower, symmetric shape. ‚Üí Indicates a consistent positive effect‚ÄîNH Black participants have higher odds of diabetes, and uncertainty is low.\nb_raceOtherDMulti: Centered around 0.8‚Äì0.9, with slightly wider spread but entirely above 0. ‚Üí Stronger evidence for increased odds of diabetes among Other/Multi-racial individuals.\n\n\n\nMCMC Trace Plots (Right Panels)\n\nEach shows 4 MCMC chains (different colors) across 1000 iterations:\nThe chains mix well and overlap substantially, without visible trends or drifts ‚Üí indicates good convergence.\nThe parameter values oscillate around stable means with no systematic pattern ‚Üí confirms stationarity.\nCombined with Rhat ‚âà 1 and high ESS from your summary, the trace plots visually validate posterior convergence and independence.\n\nMCMC Convergence: MCMC Trace plots show chains for each parameter over iterations are well-mixed chains without trends indicate convergence and stable posterior estimates.\n\n\nResults from Posterior\n\nThe Bayesian logistic regression model identified significant associations between demographic and anthropometric factors and diabetes diagnosis.\nBelow is the tabulated format for Bayesian posterior odds ratios (95% CrI) ‚Äî reference: NH White (race), Male (sex)\nAge a strong predictor: each standardized unit increase in age was associated with nearly threefold higher odds of diabetes (OR = 2.99; 95% CrI = 2.64‚Äì3.37).\nBMI showed a strong positive association (OR = 1.87; 95% CrI = 1.71‚Äì2.05), higher body mass substantially increased diabetes risk.\nFemale sex had lower odds of diabetes compared to males (OR = 0.52; 95% CrI = 0.42‚Äì0.63).\nCompared with Non-Hispanic Whites (reference group), several racial/ethnic groups had higher odds:\nMexican Americans (OR = 2.00; 95% CrI = 1.41‚Äì2.84)\nNon-Hispanic Blacks (OR = 1.71; 95% CrI = 1.28‚Äì2.27)\nOther/Multi-racial individuals (OR = 2.27; 95% CrI = 1.56‚Äì3.28)\nOther Hispanics showed a positive but non-significant association (OR = 1.54; 95% CrI = 0.93‚Äì2.43)."
  },
  {
    "objectID": "index.html#posterior-predictive-checks-ppc",
    "href": "index.html#posterior-predictive-checks-ppc",
    "title": "Bayesian Logistic Regression - Application in Probability Prediction of disease (Diabetes)",
    "section": "Posterior Predictive Checks (PPC)",
    "text": "Posterior Predictive Checks (PPC)\n\nTo assesses how the model reproduces observed data and validate model fit, visualizations below compares the predicted and observed data to show density overlays for both mean and SD. There was no large discrepancies indicating potential misfit and good alignment suggesting reliable predictions.\nA total of 50 posterior predictive sample draws generated 5,592 observations, producing a simulated distribution of predicted outcomes consistent with the sample size and assesses model fit and evaluated how well the Bayesian model reproduced the observed data pattern.\nppc_bars() plot compared the observed diabetes outcomes with 50 replicated datasets drawn from the posterior distribution.\nThe replicated distributions closely matched the observed proportions, indicating that the Bayesian model adequately captured the outcome variability and overall data structure.\n\nModel‚Äôs predictions align with reality where mean(y_rep) = average predicted probability of diabetes for each individual, across all posterior draws of the parameters and y = the actual observed diabetes status (0 = non-diabetic, 1 = diabetic).\n\n\n\nCode\nppc_bars(y = adult_imp1$diabetes_dx, yrep = pp_samples[1:50, ])\n\n\n\n\n\n\n\n\n\n\n\nCode\n#PP check for proportions (useful for binary) mean comparison to check if the simulated means match the observed mean\n\n## mean\nppc_stat(y = adult_imp1$diabetes_dx, yrep = pp_samples[1:100, ], stat = \"mean\") +\n  labs(title = \"Posterior Predictive Check: Mean of Replicates\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nPosterior Predictive stats: Mean\n-A posterior predictive check was performed on the mean diabetes outcome using 100 replicated datasets from the posterior distribution. - The distribution of the simulated means closely aligned with the observed mean, suggesting that the Bayesian model accurately captures the central tendency of the outcome.\n\n\nCode\n#PP check for proportions (useful for binary) mean and sd comparison to check if the simulated means match the observed mean\n\n## sd\nppc_stat(y = adult_imp1$diabetes_dx, yrep = pp_samples[1:100, ], stat = \"sd\") +\n  labs(title = \"PPC: Standard Deviation of Replicates\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nPosterior Predictive stats: Standard Deviation\n\nA posterior predictive check was conducted on the standard deviation of diabetes outcomes using 100 replicated datasets from the posterior distribution.\nThe simulated standard deviations closely matched the observed value, indicating that the Bayesian model adequately captures the variability in the outcome data.\n\n\n\nCode\nlibrary(brms)\nlibrary(dplyr)\nlibrary(posterior)\nlibrary(bayesplot)\nlibrary(ggplot2)\n\n\npost_wide &lt;- as_draws_df(bayes_fit)  # or posterior_samples(bayes_fit)\nnames(post_wide)  # should show: b_Intercept, b_bmi_c, b_age_c, etc.\n\n\n [1] \"b_Intercept\"           \"b_age_c\"               \"b_bmi_c\"              \n [4] \"b_sexFemale\"           \"b_raceMexicanAmerican\" \"b_raceOtherHispanic\"  \n [7] \"b_raceNHBlack\"         \"b_raceOtherDMulti\"     \"Intercept\"            \n[10] \"lprior\"                \"lp__\"                  \".chain\"               \n[13] \".iteration\"            \".draw\"                \n\n\nCode\nmcmc_areas(\n  post_wide,\n  pars = c(\"b_age_c\",\"b_bmi_c\",\"b_sexFemale\",\n           \"b_raceMexicanAmerican\",\"b_raceOtherHispanic\",\n           \"b_raceNHBlack\",\"b_raceOtherDMulti\")\n)\n\n\n\n\n\n\n\n\n\n\n\nAssumptions for Bayesian Logistic Regression\n\nData Binary outcome, independent observations\nRelationship Linear in logit, no perfect collinearity\nPriors Properly chosen and informative enough\nPosterior Proper and convergent\nFit No complete separation, good predictive checks\n\nPosterior Distributions of Regression Coefficients for Predictors of Diabetes\nThis figure presents the posterior distributions of regression coefficients estimated from the Bayesian logistic regression model. Each density curve represents the uncertainty around the parameter‚Äôs posterior mean for a given predictor.\n\nAge and BMI show strong positive associations with diabetes status, as their posterior distributions are concentrated above zero, suggesting higher values increase the likelihood of diabetes.\n\nSex (Female) has a distribution centered slightly below zero, indicating a lower probability of diabetes compared to males.\nRace categories (Mexican American, Other Hispanic, Non-Hispanic Black, and Other/Multi) show broader distributions with varying levels of uncertainty relative to the reference group (Non-Hispanic White).\nThe shaded regions indicate the 80% credible intervals, representing the range within which the true parameter values are most likely to lie based on the posterior samples.\n\n\n\n\nCode\nbayes_R2(bayes_fit)      # Model fit\n\n\n    Estimate  Est.Error      Q2.5    Q97.5\nR2 0.1313342 0.01265055 0.1064607 0.156078\n\n\n\n\nBayesian ùëÖ^2 (model fit statics):\nModel Fit to quantify predictive performance. - Explains about 13% of the variability in diabetes status, with credible uncertainty bounds suggesting reasonable but modest explanatory power. - but other factors (e.g., genetics, lifestyle, environment) also contribute to outcome variability.\n\n\nCode\npredicted &lt;- fitted(bayes_fit, summary = TRUE)\nobserved &lt;- adult_imp1[, c(\"bmi\", \"age\")]\n\n# Plot for **bmi** (obs vs pred)\n\nggplot(data = NULL, aes(x = observed$bmi, y = predicted[, \"Estimate\"])) +\n  geom_point() +\n  geom_errorbar(aes(ymin = predicted[, \"Q2.5\"], ymax = predicted[, \"Q97.5\"])) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"red\") +\n  xlab(\"Observed bmi\") + ylab(\"Predicted bmi\")\n\n\n\n\n\n\n\n\n\n\n\nObserved vs.¬†Predicted BMI\n\nA comparison of observed BMI values with their posterior predicted estimates was performed using the Bayesian model.\nEach point represents an individual‚Äôs observed BMI versus the model‚Äôs predicted mean.\nError bars indicate the 95% credible intervals of the predictions.\nThe dashed red line represents perfect prediction (observed = predicted). - The plot demonstrates that the model‚Äôs predictions generally align with the observed data, with most points closely following the diagonal, indicating good predictive performance for BMI.\n\n\n\nCode\nlibrary(posterior)\nlibrary(dplyr)\nlibrary(tidyr)\n\n# Extract posterior draws as a matrix, then convert to tibble\npost &lt;- as_draws_matrix(bayes_fit) %&gt;%   # safer than as_draws_df for manipulation\n  as.data.frame() %&gt;%\n  select(b_bmi_c, b_age_c) %&gt;%\n  pivot_longer(\n    everything(),\n    names_to = \"term\",\n    values_to = \"estimate\"\n  ) %&gt;%\n  mutate(\n    term = case_when(\n      term == \"b_bmi_c\" ~ \"BMI (per 1 SD)\",\n      term == \"b_age_c\" ~ \"Age (per 1 SD)\"\n    ),\n    type = \"Posterior\"\n  )\nprior_draws &lt;- tibble(\n  term = rep(c(\"BMI (per 1 SD)\", \"Age (per 1 SD)\"), each = 4000),\n  estimate = c(rnorm(4000, 0, 1), rnorm(4000, 0, 1)),\n  type = \"Prior\"\n)\ncombined_draws &lt;- bind_rows(prior_draws, post)\n\n\n\n\nCode\nlibrary(ggplot2)\n\nggplot(combined_draws, aes(x = estimate, fill = type)) +\n  geom_density(alpha = 0.4) +\n  facet_wrap(~ term, scales = \"free\", ncol = 2) +\n  theme_minimal(base_size = 13) +\n  labs(\n    title = \"Prior vs Posterior Distributions\",\n    x = \"Coefficient estimate\",\n    y = \"Density\",\n    fill = \"\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nPrior vs Posterior Distributions Visualization:\nDensity plots were generated to overlay the prior and posterior distributions for BMI and age. Posterior distributions were narrower and shifted away from zero relative to the priors, indicating that the data provided substantial evidence for positive associations of BMI and age with diabetes risk. Faceting by predictor enabled clear visual comparison of how each coefficient‚Äôs distribution evolved after model updating.\nInterpretation: The shift from diffuse priors to concentrated posteriors demonstrates the model‚Äôs ability to incorporate empirical evidence and refine prior beliefs. Narrower posterior distributions reflect reduced uncertainty in parameter estimates. The posterior predictive proportions of diabetes closely matched the observed prevalence in the analytic dataset, supporting the model‚Äôs reliability for inference and prediction.\n\n\nModel Comparison\n\nModel Comparison Across all analytic methods: survey-weighted maximum likelihood estimation (MLE), multiple imputation with pooled estimates (MICE), and Bayesian regression shows the associations between BMI, age, and diabetes diagnosis were consistent in direction and magnitude.\nFor BMI, the odds ratios ranged from 1.73 (95% CI: 1.58‚Äì1.89) in the MICE-pooled model to 1.89 (95% CI: 1.65‚Äì2.15) in the survey-weighted MLE model, and 1.87 (95% CrI: 1.71‚Äì2.05) in the Bayesian model.\nFor age, the estimated odds ratios were 2.90 (95% CI: 2.60‚Äì3.24) using MICE, 3.03 (95% CI: 2.70‚Äì3.40) from the survey-weighted MLE model, and 2.99 (95% CrI: 2.64‚Äì3.37) in the Bayesian analysis.\n\n\n\nCode\n# Results\n\n #Build compact results table (BMI & Age only) \nlibrary(dplyr); \nlibrary(tidyr); \nlibrary(knitr); \nlibrary(stringr)\n\n# pretty \"OR (LCL‚ÄìUCL)\" string\n\n  fmt_or &lt;- function(or, lcl, ucl, digits = 2) {\n  paste0(\n    formatC(or,  format = \"f\", digits = digits), \" (\",\n    formatC(lcl, format = \"f\", digits = digits), \"‚Äì\",\n    formatC(ucl, format = \"f\", digits = digits), \")\"\n  )\n}\n\n# guardrails: require these to exist from Modeling\nstopifnot(exists(\"svy_or\"), exists(\"mi_or\"), exists(\"bayes_or\"))\nfor (nm in c(\"svy_or\",\"mi_or\",\"bayes_or\")) {\n  if (!all(c(\"term\",\"OR\",\"LCL\",\"UCL\") %in% names(get(nm)))) {\n    stop(nm, \" must have columns: term, OR, LCL, UCL\")\n  }\n}\n\nsvy_tbl   &lt;- svy_or   %&gt;% mutate(Model = \"Survey-weighted MLE\")\nmi_tbl    &lt;- mi_or    %&gt;% mutate(Model = \"mice pooled\")\nbayes_tbl &lt;- bayes_or %&gt;% mutate(Model = \"Bayesian\")\n\nall_tbl &lt;- bind_rows(svy_tbl, mi_tbl, bayes_tbl) %&gt;%\n  mutate(term = case_when(\n    str_detect(term, \"bmi_c|\\\\bBMI\\\\b\") ~ \"BMI (per 1 SD)\",\n    str_detect(term, \"age_c|\\\\bAge\\\\b\") ~ \"Age (per 1 SD)\",\n    TRUE ~ term\n  )) %&gt;%\n  filter(term %in% c(\"BMI (per 1 SD)\", \"Age (per 1 SD)\")) %&gt;%\n  mutate(OR_CI = fmt_or(OR, LCL, UCL, digits = 2)) %&gt;%\n  select(Model, term, OR_CI) %&gt;%\n  arrange(\n    factor(Model, levels = c(\"Survey-weighted MLE\",\"mice pooled\",\"Bayesian\")),\n    factor(term,  levels = c(\"BMI (per 1 SD)\",\"Age (per 1 SD)\"))\n  )\n\nres_wide &lt;- all_tbl %&gt;%\n  pivot_wider(names_from = term, values_from = OR_CI) %&gt;%\n  rename(\n    `BMI (per 1 SD) OR (95% CI)` = `BMI (per 1 SD)`,\n    `Age (per 1 SD) OR (95% CI)` = `Age (per 1 SD)`\n  )\n\nkable(\n  res_wide,\n  align = c(\"l\",\"c\",\"c\"),\n  caption = \"Odds ratios (per 1 SD) with 95% CIs across models\"\n)\n\n\n\nOdds ratios (per 1 SD) with 95% CIs across models\n\n\n\n\n\n\n\nModel\nBMI (per 1 SD) OR (95% CI)\nAge (per 1 SD) OR (95% CI)\n\n\n\n\nSurvey-weighted MLE\n1.89 (1.65‚Äì2.15)\n3.03 (2.70‚Äì3.40)\n\n\nmice pooled\n1.73 (1.58‚Äì1.89)\n2.90 (2.60‚Äì3.24)\n\n\nBayesian\n1.87 (1.71‚Äì2.05)\n2.99 (2.64‚Äì3.37)\n\n\n\n\n\n\n\nCode\n# Compute proportion of diabetes=1 for each draw\npp_proportion &lt;- rowMeans(pp_samples)  # proportion of 1's in each posterior draw\n\n# Optional: visualize the posterior probability distribution\npp_proportion_df &lt;- tibble(proportion = pp_proportion)\n\nggplot(pp_proportion_df, aes(x = proportion)) +\n  geom_histogram(binwidth = 0.01, fill = \"skyblue\", color = \"black\") +\n  labs(\n    title = \"Posterior Distribution of Proportion of Diabetes = 1\",\n    x = \"Proportion of Diabetes = 1\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nPosterior Distribution of Diabetes Prevalence\n\nA histogram of these values shows the distribution of predicted prevalence calculated for each draw of the Bayesian model, reflecting uncertainty in the model estimates, central tendency and variability of diabetes prevalence\nMost posterior predictions cluster around 10‚Äì11%, indicating good alignment with the observed/imputed data and demonstrating that the model captures the underlying population pattern.\n\n\n\nCode\nlibrary(dplyr)\nlibrary(knitr)\nsvy_mean &lt;- svymean(~diabetes_dx, nhanes_design_adult, na.rm = TRUE)\n\n# Create summary table\nsummary_table &lt;- tibble(\n  Method = c(\"Survey-weighted mean (NHANES)\", \n             \"Imputed dataset mean\", \n             \"Posterior predictive mean\"),\n  diabetes_mean = c(\n    coef(svy_mean),           # survey-weighted mean\n    mean(adult_imp1$diabetes_dx, na.rm = TRUE),  # imputed dataset\n    mean(pp_proportion)       # posterior predictive mean\n  ),\n  SE = c(\n    SE(svy_mean),             # survey-weighted SE\n    NA,                       # not available for raw mean\n    NA                        # not available for posterior predictive\n  )\n)\n\n# Render table\nkable(summary_table, digits = 4, caption = \"Comparison of Diabetes Prevalence Across Methods\")\n\n\n\nComparison of Diabetes Prevalence Across Methods\n\n\nMethod\ndiabetes_mean\nSE\n\n\n\n\nSurvey-weighted mean (NHANES)\n0.0890\n0.0048\n\n\nImputed dataset mean\n0.1105\nNA\n\n\nPosterior predictive mean\n0.1089\nNA\n\n\n\n\n\n\n\nComparison of Diabetes Prevalence Across Methods\n\nThe estimated prevalence of diabetes was consistent across different analytical approaches:\nSurvey-weighted mean (NHANES): 8.9% (SE = 0.0048)\nImputed dataset mean (MICE): 11.1%\nPosterior predictive mean (Bayesian model): 10.95%\nThese results indicate that multiple imputation and Bayesian posterior predictions yield slightly higher prevalence estimates than the raw survey-weighted mean, but all methods are broadly consistent. The posterior predictive distribution closely matches the observed prevalence, suggesting that the Bayesian model is well-calibrated.\nBayesian model predicts that about 10‚Äì11% of this population has diabetes, with a relatively narrow range across posterior draws, reflects uncertainty in the estimate\nWhile most predictions cluster around 10‚Äì11%, the model allows for values as low as 8.5% and as high as 12.8%.\nOn comparing this with the raw imputed data proportion show that the the model predictions align with the observed/imputed data.\nThe predicted proportion incorporates uncertainty from both the Bayesian model and the imputed data, providing a more robust estimate of diabetes prevalence.\nThese results suggest that approximately 1 in 10 adults in this population may have diabetes, which can help policymakers and clinicians plan and prioritize targeted interventions effectively.\n\n\n\nCode\nlibrary(tidyverse)\n\n# Posterior predicted proportion vector\n# pp_proportion &lt;- rowMeans(pp_samples)  # if not already done\n\nknown_prev &lt;- 0.089   # NHANES prevalence\n\n# Posterior summary\nposterior_mean &lt;- mean(pp_proportion)\nposterior_ci &lt;- quantile(pp_proportion, c(0.025, 0.975))  # 95% credible interval\n\n# Create a data frame for plotting\npp_df &lt;- tibble(proportion = pp_proportion)\n\n# Plot\nggplot(pp_df, aes(x = proportion)) +\n  geom_histogram(binwidth = 0.005, fill = \"skyblue\", color = \"black\") +\n  geom_vline(xintercept = known_prev, color = \"red\", linetype = \"dashed\", size = 1) +\n  geom_vline(xintercept = posterior_mean, color = \"blue\", linetype = \"solid\", size = 1) +\n  geom_rect(aes(xmin = posterior_ci[1], xmax = posterior_ci[2], ymin = 0, ymax = Inf),\n            fill = \"blue\", alpha = 0.1, inherit.aes = FALSE) +\n  labs(\n    title = \"Posterior Predicted Diabetes Proportion vs NHANES Prevalence\",\n    subtitle = paste0(\"Red dashed = NHANES prevalence (\", known_prev, \n                      \"), Blue solid = Posterior mean (\", round(posterior_mean,3), \")\"),\n    x = \"Proportion of Diabetes = 1\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nPosterior Predicted Diabetes Proportion vs.¬†NHANES Prevalence\n\nWe compared the posterior predicted proportion of diabetes from the Bayesian model with the observed NHANES prevalence (8.9%).\nThe blue solid line represents the posterior mean, while the shaded blue area indicates the 95% credible interval of predicted proportions.\nThe red dashed line shows the NHANES survey-weighted prevalence for reference.\nMost posterior predictions cluster around 10‚Äì11%, slightly higher than the NHANES mean, but the credible interval overlaps the observed prevalence, indicating good model calibration.\nThis visualization highlights the model‚Äôs ability to capture uncertainty in predictions while remaining consistent with the observed data.\n\n\n\nCode\nlibrary(dplyr)\n\n# Posterior predicted proportion\n\nposterior_mean &lt;- mean(pp_proportion)\nposterior_ci &lt;- quantile(pp_proportion, c(0.025, 0.975))  # 95% credible interval\n\n# NHANES prevalence with SE from survey::svymean\n# Suppose you already have:\n# svymean(~diabetes_dx, nhanes_design_adult, na.rm = TRUE)\nknown_prev &lt;- 0.089        # Mean prevalence\nknown_se   &lt;- 0.0048       # Standard error from survey\n\n# Calculate 95% confidence interval\nknown_ci &lt;- c(\n  known_prev - 1.96 * known_se,\n  known_prev + 1.96 * known_se\n)\n\n# Print results\ndata.frame(\n  Type = c(\"Posterior Prediction\", \"NHANES Prevalence\"),\n  Mean = c(posterior_mean, known_prev),\n  Lower_95 = c(posterior_ci[1], known_ci[1]),\n  Upper_95 = c(posterior_ci[2], known_ci[2])\n)\n\n\n                     Type      Mean   Lower_95  Upper_95\n2.5% Posterior Prediction 0.1089181 0.09629381 0.1216962\n        NHANES Prevalence 0.0890000 0.07959200 0.0984080\n\n\nCode\n# Create a data frame for plotting\nci_df &lt;- data.frame(\n  Type = c(\"Posterior Prediction\", \"NHANES Prevalence\"),\n  Mean = c(0.1096674, 0.089),\n  Lower_95 = c(0.09772443, 0.079592),\n  Upper_95 = c(0.1210658, 0.098408)\n)\n\n\n# --- 1. Survey-weighted (Population) prevalence ---\npop_est &lt;- svymean(~diabetes_dx, nhanes_design_adult, na.rm = TRUE)\npop_prev &lt;- as.numeric(pop_est)\npop_se &lt;- as.numeric(SE(pop_est))\npop_ci &lt;- c(pop_prev - 1.96 * pop_se, pop_prev + 1.96 * pop_se)\n\n# --- 2. Bayesian posterior prevalence ---\n# bayes_pred = matrix of posterior draws (iterations √ó individuals)\npp_proportion &lt;- rowMeans(pp_samples)             # prevalence per posterior draw\npost_prev &lt;- mean(pp_proportion)                  # posterior mean prevalence\npost_ci &lt;- quantile(pp_proportion, c(0.025, 0.975))  # 95% credible interval\n\n# --- 3. Combine into one data frame ---\nbar_df &lt;- tibble(\n  Source     = c(\"Survey-weighted (Population)\", \"Bayesian Posterior\"),\n  Prevalence = c(pop_prev, post_prev),\n  CI_low     = c(pop_ci[1], post_ci[1]),\n  CI_high    = c(pop_ci[2], post_ci[2])\n)\n\n# --- 4. Plot ---\nggplot(bar_df, aes(x = Source, y = Prevalence, fill = Source)) +\n  geom_col(alpha = 0.85, width = 0.6) +\n  geom_errorbar(\n    aes(ymin = CI_low, ymax = CI_high),\n    width = 0.15,\n    color = \"black\",\n    linewidth = 0.8\n  ) +\n  guides(fill = \"none\") +\n  labs(\n    title = \"Population vs Posterior Diabetes Prevalence\",\n    subtitle = \"Survey-weighted estimate (design-based) vs Bayesian (model-based)\",\n    y = \"Prevalence (Proportion with Diabetes)\",\n    x = NULL\n  ) +\n  theme_minimal(base_size = 13)\n\n\n\n\n\n\n\n\n\n\n\nComparison of Diabetes Prevalence Across Methods\n\nThe bar plot compares survey-weighted, imputed, and Bayesian posterior estimates of diabetes prevalence:\nSurvey-weighted prevalence: 8.9% (95% CI: 0.080‚Äì0.098), reflecting the NHANES population after accounting for complex sampling.\nImputed (unweighted) prevalence: 11.1%, slightly higher due to unadjusted overrepresentation of subgroups with higher diabetes rates.\nBayesian posterior mean: 10.9% (95% CrI: 0.098‚Äì0.121), closely replicating the imputed data while slightly shrinking toward the population-level mean, consistent with Bayesian regularization.\nThe posterior credible interval overlaps the survey 95% CI, indicating that the Bayesian model reproduces population-level prevalence accurately. This demonstrates good model calibration and predictive validity, while visualizing the uncertainty of both survey-based and model-based estimates.\n\nPractical Implications - Health departments can estimate diabetes burden at the state or county level using Bayesian small-area estimation. - Clinicians and public health researchers can plan targeted screening where predicted prevalence is higher than observed. - Epidemiologists can validate disease models before applying them to regions without survey data.\n\n\nCode\nlibrary(tidyr)\nlibrary(bayesplot)\nlibrary(posterior)\n\n# Convert fitted model to draws array\npost_array &lt;- as_draws_array(bayes_fit)  # draws x chains x parameters\n\n# Plot autocorrelation for age and bmi\nmcmc_acf(post_array, pars = c(\"b_age_c\", \"b_bmi_c\"))\n\n\n\n\n\n\n\n\n\n\n\nMCMC Autocorrelation for Key Parameters\nPairwise plots (mcmc_pairs, posterior) explore correlations between parameters. - Autocorrelation plots for the posterior draws of age and BMI coefficients assess chain mixing and convergence showing correlation of each draw with its lagged values across iterations. - Rapid decay of autocorrelation toward zero indicates that the Markov chains are mixing well and successive draws are relatively independent. - Both age and BMI coefficients exhibited low autocorrelation after a few lags, supporting the reliability of posterior estimates. - This diagnostic confirms that the Bayesian model sampling was adequate and stable, ensuring valid inference from the posterior distributions."
  },
  {
    "objectID": "index.html#comparative-visualizations",
    "href": "index.html#comparative-visualizations",
    "title": "Bayesian Logistic Regression - Application in Probability Prediction of disease (Diabetes)",
    "section": "Comparative Visualizations",
    "text": "Comparative Visualizations\n\nPosterior predictive check\nA total of 50 posterior predictive sample draws generated 5,592 observations, producing a simulated distribution of predicted outcomes consistent with the sample size and assesses model fit and evaluated how well the Bayesian model reproduced the observed data pattern.\nppc_bars() plot compared the observed diabetes outcomes with 50 replicated datasets drawn from the posterior distribution. - The replicated distributions closely matched the observed proportions, indicating that the Bayesian model adequately captured the outcome variability and overall data structure. - Model‚Äôs predictions align with reality where mean(y_rep) = average predicted probability of diabetes for each individual, across all posterior draws of the parameters and y = the actual observed diabetes status (0 = non-diabetic, 1 = diabetic).\n\n\nCode\nppc_bars(y = adult_imp1$diabetes_dx, yrep = pp_samples[1:50, ])\n\n\n\n\n\n\n\n\n\n\n\nCode\n#PP check for proportions (useful for binary) mean comparison to check if the simulated means match the observed mean\n\n## mean\nppc_stat(y = adult_imp1$diabetes_dx, yrep = pp_samples[1:100, ], stat = \"mean\") +\n  labs(title = \"Posterior Predictive Check: Mean of Replicates\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nPosterior Predictive stats: Mean\n-A posterior predictive check was performed on the mean diabetes outcome using 100 replicated datasets from the posterior distribution. - The distribution of the simulated means closely aligned with the observed mean, suggesting that the Bayesian model accurately captures the central tendency of the outcome.\n\n\nCode\n#PP check for proportions (useful for binary) mean and sd comparison to check if the simulated means match the observed mean\n\n## sd\nppc_stat(y = adult_imp1$diabetes_dx, yrep = pp_samples[1:100, ], stat = \"sd\") +\n  labs(title = \"PPC: Standard Deviation of Replicates\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nPosterior Predictive stats: Standard Deviation\n\nA posterior predictive check was conducted on the standard deviation of diabetes outcomes using 100 replicated datasets from the posterior distribution.\nThe simulated standard deviations closely matched the observed value, indicating that the Bayesian model adequately captures the variability in the outcome data.\n\n\n\nCode\nlibrary(brms)\nlibrary(dplyr)\nlibrary(posterior)\nlibrary(bayesplot)\nlibrary(ggplot2)\n\n\npost_wide &lt;- as_draws_df(bayes_fit)  # or posterior_samples(bayes_fit)\nnames(post_wide)  # should show: b_Intercept, b_bmi_c, b_age_c, etc.\n\n\n [1] \"b_Intercept\"           \"b_age_c\"               \"b_bmi_c\"              \n [4] \"b_sexFemale\"           \"b_raceMexicanAmerican\" \"b_raceOtherHispanic\"  \n [7] \"b_raceNHBlack\"         \"b_raceOtherDMulti\"     \"Intercept\"            \n[10] \"lprior\"                \"lp__\"                  \".chain\"               \n[13] \".iteration\"            \".draw\"                \n\n\nCode\nmcmc_areas(\n  post_wide,\n  pars = c(\"b_age_c\",\"b_bmi_c\",\"b_sexFemale\",\n           \"b_raceMexicanAmerican\",\"b_raceOtherHispanic\",\n           \"b_raceNHBlack\",\"b_raceOtherDMulti\")\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\nbayes_R2(bayes_fit)      # Model fit\n\n\n    Estimate  Est.Error      Q2.5    Q97.5\nR2 0.1313342 0.01265055 0.1064607 0.156078\n\n\n\n\nBayesian ùëÖ^2 (model fit statics):\nModel Fit to quantify predictive performance. - Explains about 13% of the variability in diabetes status, with credible uncertainty bounds suggesting reasonable but modest explanatory power. - but other factors (e.g., genetics, lifestyle, environment) also contribute to outcome variability.\n\n\nCode\npredicted &lt;- fitted(bayes_fit, summary = TRUE)\nobserved &lt;- adult_imp1[, c(\"bmi\", \"age\")]\n\n# Plot for **bmi** (obs vs pred)\n\nggplot(data = NULL, aes(x = observed$bmi, y = predicted[, \"Estimate\"])) +\n  geom_point() +\n  geom_errorbar(aes(ymin = predicted[, \"Q2.5\"], ymax = predicted[, \"Q97.5\"])) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"red\") +\n  xlab(\"Observed bmi\") + ylab(\"Predicted bmi\")\n\n\n\n\n\n\n\n\n\n\n\nObserved vs.¬†Predicted BMI\n\nA comparison of observed BMI values with their posterior predicted estimates was performed using the Bayesian model.\nEach point represents an individual‚Äôs observed BMI versus the model‚Äôs predicted mean.\nError bars indicate the 95% credible intervals of the predictions.\nThe dashed red line represents perfect prediction (observed = predicted). - The plot demonstrates that the model‚Äôs predictions generally align with the observed data, with most points closely following the diagonal, indicating good predictive performance for BMI.\n\n\n\nCode\nlibrary(posterior)\nlibrary(dplyr)\nlibrary(tidyr)\n\n# Extract posterior draws as a matrix, then convert to tibble\npost &lt;- as_draws_matrix(bayes_fit) %&gt;%   # safer than as_draws_df for manipulation\n  as.data.frame() %&gt;%\n  select(b_bmi_c, b_age_c) %&gt;%\n  pivot_longer(\n    everything(),\n    names_to = \"term\",\n    values_to = \"estimate\"\n  ) %&gt;%\n  mutate(\n    term = case_when(\n      term == \"b_bmi_c\" ~ \"BMI (per 1 SD)\",\n      term == \"b_age_c\" ~ \"Age (per 1 SD)\"\n    ),\n    type = \"Posterior\"\n  )\nprior_draws &lt;- tibble(\n  term = rep(c(\"BMI (per 1 SD)\", \"Age (per 1 SD)\"), each = 4000),\n  estimate = c(rnorm(4000, 0, 1), rnorm(4000, 0, 1)),\n  type = \"Prior\"\n)\ncombined_draws &lt;- bind_rows(prior_draws, post)\n\n\n\n\nPrior vs Posterior Distributions\nTo assess how the Bayesian model updates beliefs from prior information to posterior estimates, we compared prior vs posterior coefficient distributions for key predictors: BMI and age. 1. Prior Draws - Simulated from a standard normal distribution (mean = 0, SD = 1) for both BMI and age coefficients. Represent initial beliefs about coefficient values before seeing the data. 2. Posterior Draws - Extracted from the fitted model (bayes_fit) for b_bmi_c and b_age_c.¬†- Pivoted to long format and labeled as ‚ÄúPosterior‚Äù. 3. Visualization Combined prior and posterior draws - Plotted density overlays with facets for BMI and age. - Posterior distributions are narrower and often shifted from prior, reflecting information gained from the data. - Differences between prior and posterior highlight the model‚Äôs learning about effect sizes. - Posterior Predictive Proportions of Diabetes - Computed the proportion of diabetes cases (diabetes = 1) for each posterior draw (pp_samples).\nInterpretaion: - Prior vs posterior plots demonstrate that the Bayesian model updates prior beliefs in a data-informed way. - Posterior predictive proportions closely match observed prevalence, supporting model reliability for inference and prediction.\n\n\nCode\nlibrary(ggplot2)\n\nggplot(combined_draws, aes(x = estimate, fill = type)) +\n  geom_density(alpha = 0.4) +\n  facet_wrap(~ term, scales = \"free\", ncol = 2) +\n  theme_minimal(base_size = 13) +\n  labs(\n    title = \"Prior vs Posterior Distributions\",\n    x = \"Coefficient estimate\",\n    y = \"Density\",\n    fill = \"\"\n  )\n\n\n\n\n\n\n\n\n\nPrior vs.¬†Posterior Distributions - We visualized the prior and posterior distributions of the BMI and age coefficients. - Priors were centered at 0, reflecting weak prior beliefs about the direction and magnitude of the effects. - Posteriors were shifted away from 0, indicating that the data provided strong evidence for associations with the outcome. - The density plots highlight the uncertainty and magnitude of the estimated effects, with posterior distributions narrower than the priors, demonstrating that the data meaningfully updated our beliefs. - Faceting by term allows comparison of each predictor‚Äôs prior and posterior distributions.\n\n\nModel Comparison\n\nAcross all analytic methods‚Äîsurvey-weighted maximum likelihood estimation (MLE), multiple imputation with pooled estimates (MICE), and Bayesian regression‚Äîthe associations between BMI, age, and diabetes diagnosis were consistent in direction and magnitude.\nFor BMI, the odds ratios ranged from 1.73 (95% CI: 1.58‚Äì1.89) in the MICE-pooled model to 1.89 (95% CI: 1.65‚Äì2.15) in the survey-weighted MLE model, and 1.87 (95% CrI: 1.71‚Äì2.05) in the Bayesian model.\nFor age, the estimated odds ratios were 2.90 (95% CI: 2.60‚Äì3.24) using MICE, 3.03 (95% CI: 2.70‚Äì3.40) from the survey-weighted MLE model, and 2.99 (95% CrI: 2.64‚Äì3.37) in the Bayesian analysis.\n\n\n\nCode\n# Results\n\n #Build compact results table (BMI & Age only) \nlibrary(dplyr); \nlibrary(tidyr); \nlibrary(knitr); \nlibrary(stringr)\n\n# pretty \"OR (LCL‚ÄìUCL)\" string\n\n  fmt_or &lt;- function(or, lcl, ucl, digits = 2) {\n  paste0(\n    formatC(or,  format = \"f\", digits = digits), \" (\",\n    formatC(lcl, format = \"f\", digits = digits), \"‚Äì\",\n    formatC(ucl, format = \"f\", digits = digits), \")\"\n  )\n}\n\n# guardrails: require these to exist from Modeling\nstopifnot(exists(\"svy_or\"), exists(\"mi_or\"), exists(\"bayes_or\"))\nfor (nm in c(\"svy_or\",\"mi_or\",\"bayes_or\")) {\n  if (!all(c(\"term\",\"OR\",\"LCL\",\"UCL\") %in% names(get(nm)))) {\n    stop(nm, \" must have columns: term, OR, LCL, UCL\")\n  }\n}\n\nsvy_tbl   &lt;- svy_or   %&gt;% mutate(Model = \"Survey-weighted MLE\")\nmi_tbl    &lt;- mi_or    %&gt;% mutate(Model = \"mice pooled\")\nbayes_tbl &lt;- bayes_or %&gt;% mutate(Model = \"Bayesian\")\n\nall_tbl &lt;- bind_rows(svy_tbl, mi_tbl, bayes_tbl) %&gt;%\n  mutate(term = case_when(\n    str_detect(term, \"bmi_c|\\\\bBMI\\\\b\") ~ \"BMI (per 1 SD)\",\n    str_detect(term, \"age_c|\\\\bAge\\\\b\") ~ \"Age (per 1 SD)\",\n    TRUE ~ term\n  )) %&gt;%\n  filter(term %in% c(\"BMI (per 1 SD)\", \"Age (per 1 SD)\")) %&gt;%\n  mutate(OR_CI = fmt_or(OR, LCL, UCL, digits = 2)) %&gt;%\n  select(Model, term, OR_CI) %&gt;%\n  arrange(\n    factor(Model, levels = c(\"Survey-weighted MLE\",\"mice pooled\",\"Bayesian\")),\n    factor(term,  levels = c(\"BMI (per 1 SD)\",\"Age (per 1 SD)\"))\n  )\n\nres_wide &lt;- all_tbl %&gt;%\n  pivot_wider(names_from = term, values_from = OR_CI) %&gt;%\n  rename(\n    `BMI (per 1 SD) OR (95% CI)` = `BMI (per 1 SD)`,\n    `Age (per 1 SD) OR (95% CI)` = `Age (per 1 SD)`\n  )\n\nkable(\n  res_wide,\n  align = c(\"l\",\"c\",\"c\"),\n  caption = \"Odds ratios (per 1 SD) with 95% CIs across models\"\n)\n\n\n\nOdds ratios (per 1 SD) with 95% CIs across models\n\n\n\n\n\n\n\nModel\nBMI (per 1 SD) OR (95% CI)\nAge (per 1 SD) OR (95% CI)\n\n\n\n\nSurvey-weighted MLE\n1.89 (1.65‚Äì2.15)\n3.03 (2.70‚Äì3.40)\n\n\nmice pooled\n1.73 (1.58‚Äì1.89)\n2.90 (2.60‚Äì3.24)\n\n\nBayesian\n1.87 (1.71‚Äì2.05)\n2.99 (2.64‚Äì3.37)\n\n\n\n\n\n\n\nCode\n# Compute proportion of diabetes=1 for each draw\npp_proportion &lt;- rowMeans(pp_samples)  # proportion of 1's in each posterior draw\n\n# Optional: visualize the posterior probability distribution\npp_proportion_df &lt;- tibble(proportion = pp_proportion)\n\nggplot(pp_proportion_df, aes(x = proportion)) +\n  geom_histogram(binwidth = 0.01, fill = \"skyblue\", color = \"black\") +\n  labs(\n    title = \"Posterior Distribution of Proportion of Diabetes = 1\",\n    x = \"Proportion of Diabetes = 1\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nPosterior Distribution of Diabetes Prevalence\n\nA histogram of these values shows the distribution of predicted prevalence calculated for each draw of the Bayesian model (pp_proportion), reflecting uncertainty in the model estimates, central tendency and variability of diabetes prevalence\nMost posterior predictions cluster around 10‚Äì11%, indicating good alignment with the observed/imputed data and demonstrating that the model captures the underlying population pattern.\n\n\n\nCode\nlibrary(dplyr)\nlibrary(knitr)\nsvy_mean &lt;- svymean(~diabetes_dx, nhanes_design_adult, na.rm = TRUE)\n\n# Create summary table\nsummary_table &lt;- tibble(\n  Method = c(\"Survey-weighted mean (NHANES)\", \n             \"Imputed dataset mean\", \n             \"Posterior predictive mean\"),\n  diabetes_mean = c(\n    coef(svy_mean),           # survey-weighted mean\n    mean(adult_imp1$diabetes_dx, na.rm = TRUE),  # imputed dataset\n    mean(pp_proportion)       # posterior predictive mean\n  ),\n  SE = c(\n    SE(svy_mean),             # survey-weighted SE\n    NA,                       # not available for raw mean\n    NA                        # not available for posterior predictive\n  )\n)\n\n# Render table\nkable(summary_table, digits = 4, caption = \"Comparison of Diabetes Prevalence Across Methods\")\n\n\n\nComparison of Diabetes Prevalence Across Methods\n\n\nMethod\ndiabetes_mean\nSE\n\n\n\n\nSurvey-weighted mean (NHANES)\n0.0890\n0.0048\n\n\nImputed dataset mean\n0.1105\nNA\n\n\nPosterior predictive mean\n0.1089\nNA\n\n\n\n\n\n\n\nComparison of Diabetes Prevalence Across Methods\n\nThe estimated prevalence of diabetes was consistent across different analytical approaches:\nSurvey-weighted mean (NHANES): 8.9% (SE = 0.0048)\nImputed dataset mean (MICE): 11.1%\nPosterior predictive mean (Bayesian model): 10.95%\n\nThese results indicate that multiple imputation and Bayesian posterior predictions yield slightly higher prevalence estimates than the raw survey-weighted mean, but all methods are broadly consistent. The posterior predictive distribution closely matches the observed prevalence, suggesting that the Bayesian model is well-calibrated.\n\nBayesian model predicts that about 10‚Äì11% of this population has diabetes, with a relatively narrow range across posterior draws, reflects uncertainty in the estimate\nWhile most predictions cluster around 10‚Äì11%, the model allows for values as low as 8.5% and as high as 12.8%.\nOn comparing this with the raw imputed data proportion show that the the model predictions align with the observed/imputed data.\n\nThe predicted proportion incorporates uncertainty from both the Bayesian model and the imputed data, providing a more robust estimate of diabetes prevalence.\nThese results suggest that approximately 1 in 10 adults in this population may have diabetes, which can help policymakers and clinicians plan and prioritize targeted interventions effectively.\n\n\nCode\nlibrary(tidyverse)\n\n# Posterior predicted proportion vector\n# pp_proportion &lt;- rowMeans(pp_samples)  # if not already done\n\nknown_prev &lt;- 0.089   # NHANES prevalence\n\n# Posterior summary\nposterior_mean &lt;- mean(pp_proportion)\nposterior_ci &lt;- quantile(pp_proportion, c(0.025, 0.975))  # 95% credible interval\n\n# Create a data frame for plotting\npp_df &lt;- tibble(proportion = pp_proportion)\n\n# Plot\nggplot(pp_df, aes(x = proportion)) +\n  geom_histogram(binwidth = 0.005, fill = \"skyblue\", color = \"black\") +\n  geom_vline(xintercept = known_prev, color = \"red\", linetype = \"dashed\", size = 1) +\n  geom_vline(xintercept = posterior_mean, color = \"blue\", linetype = \"solid\", size = 1) +\n  geom_rect(aes(xmin = posterior_ci[1], xmax = posterior_ci[2], ymin = 0, ymax = Inf),\n            fill = \"blue\", alpha = 0.1, inherit.aes = FALSE) +\n  labs(\n    title = \"Posterior Predicted Diabetes Proportion vs NHANES Prevalence\",\n    subtitle = paste0(\"Red dashed = NHANES prevalence (\", known_prev, \n                      \"), Blue solid = Posterior mean (\", round(posterior_mean,3), \")\"),\n    x = \"Proportion of Diabetes = 1\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nPosterior Predicted Diabetes Proportion vs.¬†NHANES Prevalence\n\nWe compared the posterior predicted proportion of diabetes from the Bayesian model with the observed NHANES prevalence (8.9%).\nThe blue solid line represents the posterior mean, while the shaded blue area indicates the 95% credible interval of predicted proportions.\nThe red dashed line shows the NHANES survey-weighted prevalence for reference.\nMost posterior predictions cluster around 10‚Äì11%, slightly higher than the NHANES mean, but the credible interval overlaps the observed prevalence, indicating good model calibration.\nThis visualization highlights the model‚Äôs ability to capture uncertainty in predictions while remaining consistent with the observed data.\n\n\n\nCode\nlibrary(dplyr)\n\n# Posterior predicted proportion\n\nposterior_mean &lt;- mean(pp_proportion)\nposterior_ci &lt;- quantile(pp_proportion, c(0.025, 0.975))  # 95% credible interval\n\n# NHANES prevalence with SE from survey::svymean\n# Suppose you already have:\n# svymean(~diabetes_dx, nhanes_design_adult, na.rm = TRUE)\nknown_prev &lt;- 0.089        # Mean prevalence\nknown_se   &lt;- 0.0048       # Standard error from survey\n\n# Calculate 95% confidence interval\nknown_ci &lt;- c(\n  known_prev - 1.96 * known_se,\n  known_prev + 1.96 * known_se\n)\n\n# Print results\ndata.frame(\n  Type = c(\"Posterior Prediction\", \"NHANES Prevalence\"),\n  Mean = c(posterior_mean, known_prev),\n  Lower_95 = c(posterior_ci[1], known_ci[1]),\n  Upper_95 = c(posterior_ci[2], known_ci[2])\n)\n\n\n                     Type      Mean   Lower_95  Upper_95\n2.5% Posterior Prediction 0.1089181 0.09629381 0.1216962\n        NHANES Prevalence 0.0890000 0.07959200 0.0984080\n\n\nCode\n# Create a data frame for plotting\nci_df &lt;- data.frame(\n  Type = c(\"Posterior Prediction\", \"NHANES Prevalence\"),\n  Mean = c(0.1096674, 0.089),\n  Lower_95 = c(0.09772443, 0.079592),\n  Upper_95 = c(0.1210658, 0.098408)\n)\n\n\n# --- 1. Survey-weighted (Population) prevalence ---\npop_est &lt;- svymean(~diabetes_dx, nhanes_design_adult, na.rm = TRUE)\npop_prev &lt;- as.numeric(pop_est)\npop_se &lt;- as.numeric(SE(pop_est))\npop_ci &lt;- c(pop_prev - 1.96 * pop_se, pop_prev + 1.96 * pop_se)\n\n# --- 2. Bayesian posterior prevalence ---\n# bayes_pred = matrix of posterior draws (iterations √ó individuals)\npp_proportion &lt;- rowMeans(pp_samples)             # prevalence per posterior draw\npost_prev &lt;- mean(pp_proportion)                  # posterior mean prevalence\npost_ci &lt;- quantile(pp_proportion, c(0.025, 0.975))  # 95% credible interval\n\n# --- 3. Combine into one data frame ---\nbar_df &lt;- tibble(\n  Source     = c(\"Survey-weighted (Population)\", \"Bayesian Posterior\"),\n  Prevalence = c(pop_prev, post_prev),\n  CI_low     = c(pop_ci[1], post_ci[1]),\n  CI_high    = c(pop_ci[2], post_ci[2])\n)\n\n# --- 4. Plot ---\nggplot(bar_df, aes(x = Source, y = Prevalence, fill = Source)) +\n  geom_col(alpha = 0.85, width = 0.6) +\n  geom_errorbar(\n    aes(ymin = CI_low, ymax = CI_high),\n    width = 0.15,\n    color = \"black\",\n    linewidth = 0.8\n  ) +\n  guides(fill = \"none\") +\n  labs(\n    title = \"Population vs Posterior Diabetes Prevalence\",\n    subtitle = \"Survey-weighted estimate (design-based) vs Bayesian (model-based)\",\n    y = \"Prevalence (Proportion with Diabetes)\",\n    x = NULL\n  ) +\n  theme_minimal(base_size = 13)\n\n\n\n\n\n\n\n\n\n\n\nComparison of Diabetes Prevalence Across Methods\n\nThe bar plot compares survey-weighted, imputed, and Bayesian posterior estimates of diabetes prevalence:\nSurvey-weighted prevalence: 8.9% (95% CI: 0.080‚Äì0.098), reflecting the NHANES population after accounting for complex sampling.\nImputed (unweighted) prevalence: 11.1%, slightly higher due to unadjusted overrepresentation of subgroups with higher diabetes rates.\nBayesian posterior mean: 10.9% (95% CrI: 0.098‚Äì0.121), closely replicating the imputed data while slightly shrinking toward the population-level mean, consistent with Bayesian regularization.\nThe posterior credible interval overlaps the survey 95% CI, indicating that the Bayesian model reproduces population-level prevalence accurately. This demonstrates good model calibration and predictive validity, while visualizing the uncertainty of both survey-based and model-based estimates.\n\nPractical Implications - Health departments can estimate diabetes burden at the state or county level using Bayesian small-area estimation. - Clinicians and public health researchers can plan targeted screening where predicted prevalence is higher than observed. - Epidemiologists can validate disease models before applying them to regions without survey data.\n\n\nCode\nlibrary(tidyr)\nlibrary(bayesplot)\nlibrary(posterior)\n\n# Convert fitted model to draws array\npost_array &lt;- as_draws_array(bayes_fit)  # draws x chains x parameters\n\n# Plot autocorrelation for age and bmi\nmcmc_acf(post_array, pars = c(\"b_age_c\", \"b_bmi_c\"))\n\n\n\n\n\n\n\n\n\n\n\nMCMC Autocorrelation for Key Parameters\nPairwise plots (mcmc_pairs, posterior) explore correlations between parameters. - Autocorrelation plots for the posterior draws of age and BMI coefficients assess chain mixing and convergence showing correlation of each draw with its lagged values across iterations. - Rapid decay of autocorrelation toward zero indicates that the Markov chains are mixing well and successive draws are relatively independent. - Both age and BMI coefficients exhibited low autocorrelation after a few lags, supporting the reliability of posterior estimates. - This diagnostic confirms that the Bayesian model sampling was adequate and stable, ensuring valid inference from the posterior distributions."
  },
  {
    "objectID": "index.html#bayesian-logistic-regression-and-analysis",
    "href": "index.html#bayesian-logistic-regression-and-analysis",
    "title": "Bayesian Logistic Regression - Application in Probability Prediction of disease (Diabetes)",
    "section": "Bayesian Logistic Regression and Analysis",
    "text": "Bayesian Logistic Regression and Analysis\n\nA Bayesian logistic regression model fitted on the first imputed dataset (adult_imp1) - with survey weights-Normalized MEC exam weights (wt_norm) with mean 1.00 (SD 0.79) with no missing values and assessed predictors of diabetes diagnosis - with continuous variables standardized, categorical variables correctly re-leveled for reference categories\nPrior Specification for the study model\n\nIntercept prior: student_t(3, 0, 10) ‚Äî allowing heavy tails for flexibility in the intercept estimate. R. V. D. Schoot et al. (2013)\nRegression coefficients prior: normal(0, 2.5) ‚Äî providing weakly informative regularization provide gentle regularization, constraining extreme values without overpowering the data R. van de Schoot et al. (2021)\n\nModel Estimation\n\nFour Markov Chain Monte Carlo (MCMC) chains, each with 2000 iterations (50% warm-up), and an adaptive delta of 0.95 ensured good chain convergence and reduced divergent transitions.\nPosterior summaries represent the central tendency and uncertainty around the model parameters through credible intervals (CrI).\n\nModel Formula:\n\ndiabetes_dx | weights(wt_norm) ~ age_c + bmi_c + sex + race\nFamily: bernoulli Links: mu = logit Data: adult_imp1 (Number of observations: 5592) Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; total post-warmup draws = 4000 Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS and Tail_ESS are effective sample size measures, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCode\nlibrary(gt)\n\npriors &lt;- c(\n  set_prior(\"normal(0, 2.5)\", class = \"b\"),\n  set_prior(\"student_t(3, 0, 10)\", class = \"Intercept\") \n)\n\nbayes_fit &lt;- brm(\n  formula = diabetes_dx | weights(wt_norm) ~ age_c + bmi_c + sex + race,\n  data    = adult_imp1,\n  family  = bernoulli(link = \"logit\"),\n  prior   = priors,\n  chains  = 4, iter = 2000, seed = 123,\n  control = list(adapt_delta = 0.95),\n  refresh = 0   # quiet Stan output\n)\n\n\nRunning /opt/R/4.4.2/lib/R/bin/R CMD SHLIB foo.c\nusing C compiler: ‚Äògcc (GCC) 11.5.0 20240719 (Red Hat 11.5.0-2)‚Äô\ngcc -I\"/opt/R/4.4.2/lib/R/include\" -DNDEBUG   -I\"/opt/R/4.4.2/lib/R/library/Rcpp/include/\"  -I\"/opt/R/4.4.2/lib/R/library/RcppEigen/include/\"  -I\"/opt/R/4.4.2/lib/R/library/RcppEigen/include/unsupported\"  -I\"/opt/R/4.4.2/lib/R/library/BH/include\" -I\"/opt/R/4.4.2/lib/R/library/StanHeaders/include/src/\"  -I\"/opt/R/4.4.2/lib/R/library/StanHeaders/include/\"  -I\"/opt/R/4.4.2/lib/R/library/RcppParallel/include/\"  -I\"/opt/R/4.4.2/lib/R/library/rstan/include\" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DUSE_STANC3 -DSTRICT_R_HEADERS  -DBOOST_PHOENIX_NO_VARIADIC_EXPRESSION  -D_HAS_AUTO_PTR_ETC=0  -include '/opt/R/4.4.2/lib/R/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp'  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/usr/local/include    -fpic  -g -O2  -c foo.c -o foo.o\nIn file included from /opt/R/4.4.2/lib/R/library/RcppEigen/include/Eigen/Core:19,\n                 from /opt/R/4.4.2/lib/R/library/RcppEigen/include/Eigen/Dense:1,\n                 from /opt/R/4.4.2/lib/R/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22,\n                 from &lt;command-line&gt;:\n/opt/R/4.4.2/lib/R/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:679:10: fatal error: cmath: No such file or directory\n  679 | #include &lt;cmath&gt;\n      |          ^~~~~~~\ncompilation terminated.\nmake: *** [/opt/R/4.4.2/lib/R/etc/Makeconf:195: foo.o] Error 1\n\n\nCode\nprior_summary(bayes_fit)\n\n\n               prior     class                coef group resp dpar nlpar lb ub\n      normal(0, 2.5)         b                                                \n      normal(0, 2.5)         b               age_c                            \n      normal(0, 2.5)         b               bmi_c                            \n      normal(0, 2.5)         b raceMexicanAmerican                            \n      normal(0, 2.5)         b         raceNHBlack                            \n      normal(0, 2.5)         b     raceOtherDMulti                            \n      normal(0, 2.5)         b   raceOtherHispanic                            \n      normal(0, 2.5)         b           sexFemale                            \n student_t(3, 0, 10) Intercept                                                \n tag       source\n             user\n     (vectorized)\n     (vectorized)\n     (vectorized)\n     (vectorized)\n     (vectorized)\n     (vectorized)\n     (vectorized)\n             user\n\n\nCode\nsummary(bayes_fit)            # Bayesian model summary\n\n\n Family: bernoulli \n  Links: mu = logit \nFormula: diabetes_dx | weights(wt_norm) ~ age_c + bmi_c + sex + race \n   Data: adult_imp1 (Number of observations: 5592) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept              -2.66      0.09    -2.84    -2.50 1.00     4187     3510\nage_c                   1.09      0.06     0.97     1.22 1.00     3012     3098\nbmi_c                   0.63      0.05     0.53     0.72 1.00     3472     3315\nsexFemale              -0.66      0.10    -0.86    -0.46 1.00     4003     3052\nraceMexicanAmerican     0.69      0.18     0.35     1.04 1.00     3526     2843\nraceOtherHispanic       0.43      0.24    -0.07     0.89 1.00     4058     3114\nraceNHBlack             0.54      0.15     0.24     0.82 1.00     3597     3177\nraceOtherDMulti         0.82      0.19     0.45     1.19 1.00     3763     3257\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1)."
  },
  {
    "objectID": "slides.html#introduction",
    "href": "slides.html#introduction",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013‚Äì2014)",
    "section": "Introduction",
    "text": "Introduction\n\nStudy Aim: Predict diabetes status using Bayesian logistic regression on imputed data. Early identification of risk factors is key to Diabetes diagnosis and prevention\nChallenges Frequentists - maximum likelihood estimation (MLE) with unstable estimates in the presence of missing data, quasi-separation, or small samples\nBayesian analysis - flexible, regularize estimates, quantify uncertainty under missingness or imputation ( Baldwin and Larson 2017; Kruschke and Liddell 2017) and incorporate prior, provide credible intervals.\nMarkov Chain Monte Carlo (MCMC) predicts patient health status across diseases\nSupport model checking, variable selection, and uncertainty quantification"
  },
  {
    "objectID": "slides.html#missing-data-assessment",
    "href": "slides.html#missing-data-assessment",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013‚Äì2014)",
    "section": "Missing Data Assessment",
    "text": "Missing Data Assessment\n\nOverall missingness: ~4%, No variable completely missing, Missingness is not uniform\nMissingness pattern: likely MAR (Missing At Random).\nClustered mainly in bmi and diabetes_dx.\nDecision: Apply Multiple Imputation by Chained Equations (MICE)."
  },
  {
    "objectID": "slides.html#multivariate-imputation-by-chained-equations-mice",
    "href": "slides.html#multivariate-imputation-by-chained-equations-mice",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013‚Äì2014)",
    "section": "Multivariate Imputation by Chained Equations (MICE)",
    "text": "Multivariate Imputation by Chained Equations (MICE)\n\nMethod: Predictive mean matching (PMM) for continuous vars; logistic regression for binary.\nIterations: 5 imputations, 10 iterations each, combined imputed datasets using Rubin‚Äôs rules.\nDistribution plots confirmed consistency with the original data."
  },
  {
    "objectID": "slides.html#mice-pooled-logistic-regression",
    "href": "slides.html#mice-pooled-logistic-regression",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013‚Äì2014)",
    "section": "MICE (Pooled Logistic Regression)",
    "text": "MICE (Pooled Logistic Regression)\n  glm(diabetes_dx ~ age_c + bmi_c + sex + race, family = binomial()) }) pool_mi &lt;- pool(fit_mi) \n\nn=5,769 participants\nModel significance: All predictors are statistically significant (p &lt; 0.01), suggesting a robust association across demographic and health variables."
  },
  {
    "objectID": "slides.html#bayesian-logistic-regression",
    "href": "slides.html#bayesian-logistic-regression",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013‚Äì2014)",
    "section": "Bayesian Logistic Regression",
    "text": "Bayesian Logistic Regression\nOutcome: diabetes_dx (0 = non-diabetic, 1 = diabetic)\nPredictors: age_c, bmi_c, sex, race.\n\nIntercept prior: student_t(3, 0, 10) ‚Äî allows heavy tails for flexibility in the intercept estimate. R. V. D. Schoot et al. (2013)\nRegression coefficients prior: normal(0, 2.5) ‚Äî providing weakly informative regularization, constraining extreme values without overpowering the data. R. van de Schoot et al. (2021)\nImplemented in brms (Stan backend), Posterior draws = 4000 (4 chains √ó 1000 iterations).\n\nLogistic link function"
  },
  {
    "objectID": "slides.html#bayesian-model-equation",
    "href": "slides.html#bayesian-model-equation",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013‚Äì2014)",
    "section": "Bayesian Model Equation",
    "text": "Bayesian Model Equation\n#| eval: false\n#| include: false\nlibrary(gt)\npriors &lt;- c(\n  set_prior(\"normal(0, 2.5)\", class = \"b\"),\n  set_prior(\"student_t(3, 0, 10)\", class = \"Intercept\") \n)\nbayes_fit &lt;- brm(\n  formula = diabetes_dx | weights(wt_norm) ~ age_c + bmi_c + sex + race,\n  data    = adult_imp1,\n  family  = bernoulli(link = \"logit\"),\n  prior   = priors,\n  chains  = 4, iter = 2000, seed = 123,\n  control = list(adapt_delta = 0.95),\n  refresh = 0   # quiet Stan output\n)\n\\[\n\\text{logit}(P(Y=1)) = \\beta_0 + \\beta_1 \\text{age}_c + \\beta_2 \\text{bmi}_c + \\beta_3 \\text{sex} + \\beta_4 \\text{race}\n\\]\n\nP(Y=1) : Probability of being diabetic\nCoefficients estimated with posterior means and 95% credible intervals."
  },
  {
    "objectID": "slides.html#bayesian-model-diagnostics-mcmc",
    "href": "slides.html#bayesian-model-diagnostics-mcmc",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013‚Äì2014)",
    "section": "Bayesian Model Diagnostics (MCMC)",
    "text": "Bayesian Model Diagnostics (MCMC)\n\nRhat ‚âà 1.00 ‚Üí convergence achieved.\nBulk ESS &gt; 3000 for all parameters ‚Üí good mixing.\nTrace plots showed stable sampling across chains.\nPosterior distributions were unimodal and well-centered."
  },
  {
    "objectID": "slides.html#posterior-estimates",
    "href": "slides.html#posterior-estimates",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013‚Äì2014)",
    "section": "Posterior Estimates",
    "text": "Posterior Estimates\n\n\n\n\n\n\n\n\n\nPredictor\nEstimate\n95% CI\nInterpretation\n\n\n\n\nIntercept\n-2.66\n[-2.84, -2.50]\nBaseline log-odds\n\n\nAge_c\n1.09\n[0.97, 1.22]\n‚Üë age increases diabetes risk\n\n\nBMI_c\n0.88\n[0.76, 1.01]\nHigher BMI linked with higher risk\n\n\nHTN\n0.65\n[0.50, 0.81]\nHypertension predicts diabetes"
  },
  {
    "objectID": "slides.html#posterior-predictive-distribution",
    "href": "slides.html#posterior-predictive-distribution",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013‚Äì2014)",
    "section": "Posterior Predictive Distribution",
    "text": "Posterior Predictive Distribution\n#| echo: true\nlibrary(ggplot2)\n\nggplot(combined_draws, aes(x = estimate, fill = type)) +\n  geom_density(alpha = 0.4) +\n  facet_wrap(~ term, scales = \"free\", ncol = 2) +\n  theme_minimal(base_size = 13) +\n  labs(\n    title = \"Prior vs Posterior Distributions\",\n    x = \"Coefficient estimate\",\n    y = \"Density\",\n    fill = \"\"\n  )"
  },
  {
    "objectID": "slides.html#model-interpretation",
    "href": "slides.html#model-interpretation",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013‚Äì2014)",
    "section": "Model Interpretation",
    "text": "Model Interpretation\n\nStrong positive relationship between age, BMI, and diabetes probability.\nPosterior predictive checks: model captures data patterns well.\nImputation reduced bias and improved model robustness."
  },
  {
    "objectID": "slides.html#limitations",
    "href": "slides.html#limitations",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013‚Äì2014)",
    "section": "Limitations",
    "text": "Limitations\n\nNHANES data are cross-sectional ‚Üí no causal inference.\nPotential unmeasured confounding (diet, physical activity).\nLimited predictors ‚Üí simplified model structure.\nImputation assumes MAR; violations may introduce bias."
  },
  {
    "objectID": "slides.html#conclusion",
    "href": "slides.html#conclusion",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013‚Äì2014)",
    "section": "Conclusion",
    "text": "Conclusion\n\nBayesian logistic regression effectively models uncertainty.\nMICE improved data completeness and reliability.\nPosterior predictions provide interpretable probabilities for diabetes risk.\nFramework adaptable to other health outcomes (e.g., hypertension, obesity)."
  },
  {
    "objectID": "slides.html#acknowledgements",
    "href": "slides.html#acknowledgements",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013‚Äì2014)",
    "section": "Acknowledgements",
    "text": "Acknowledgements\n\nFaculty: Dr.¬†Ashraf Cohen, PhD, MS\nUniversity of West Florida, Department: Mathematics and Statistics\n        Thanks for the guidance"
  },
  {
    "objectID": "slides.html#references-1",
    "href": "slides.html#references-1",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013‚Äì2014)",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\nSchoot, Rens van de, Sarah Depaoli, Ruth King, Bianca Kramer, Kaspar M√§rtens, Mahlet G. Tadesse, Marina Vannucci, et al. 2021. ‚ÄúBayesian statistics and modelling.‚Äù Nature Reviews Methods Primers 1 (1): 1. https://doi.org/10.1038/s43586-020-00001-2.\n\n\nSchoot, Rens Van De, David Kaplan, Jaap Denissen, Jens B Asendorpf, and Marcel A G Van Aken. 2013. ‚ÄúA Gentle Introduction to Bayesian Analysis: Applications to Developmental Research.‚Äù https://doi.org/10.1111/cdev.12169."
  },
  {
    "objectID": "slides.html#data",
    "href": "slides.html#data",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013‚Äì2014)",
    "section": "Data",
    "text": "Data\nSurvey weighted: National Health and Nutrition Examination Survey dataset (2013-2014) (NHANES).\n\nStudy population: Adults (&gt;20 years).\nPredictors: age, sex, race, BMI\nResponse variable: diabetes_dx (diagnosis: 0 = No, 1 = Yes)\n\n  head(adult)"
  },
  {
    "objectID": "slides.html#early-identification-of-risk-factors-is-key-to-diabetes-diagnosis-and-prevention",
    "href": "slides.html#early-identification-of-risk-factors-is-key-to-diabetes-diagnosis-and-prevention",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013‚Äì2014)",
    "section": "Early identification of risk factors is key to Diabetes diagnosis and prevention",
    "text": "Early identification of risk factors is key to Diabetes diagnosis and prevention\nPredict diabetes using Bayesian logistic regression\nChallenges Frequentists - maximum likelihood estimation is unstable in missing data, quasi-separation, or small samples\nBayesian Method - flexible, regularize estimates, quantify uncertainty under missingness or imputation ( Baldwin and Larson 2017; Kruschke and Liddell 2017) - incorporate prior, provide credible intervals via Markov Chain Monte Carlo (MCMC) to predict patient health status across diseases - Support model checking, variable selection, and uncertainty quantification"
  },
  {
    "objectID": "slides.html#data-survey-weighted-national-health-and-nutrition-examination-survey-dataset-2013-2014-nhanes",
    "href": "slides.html#data-survey-weighted-national-health-and-nutrition-examination-survey-dataset-2013-2014-nhanes",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013‚Äì2014)",
    "section": "Data: Survey weighted: National Health and Nutrition Examination Survey dataset (2013-2014) (NHANES)",
    "text": "Data: Survey weighted: National Health and Nutrition Examination Survey dataset (2013-2014) (NHANES)\n- Study population: Adults (&gt;20 years)(n=5769): - Age: evenly distributed across adult age groups - BMI: concentrated in the overweight and obese ranges - Female¬†&gt; Male¬†participants - most participants are non-diabetic, a minority are diabetic: a relatively lower prevalence of diabetes in the sample - Predictors: age, sex, race, BMI - Response variable: diabetes_dx (diagnosis: 0 = No, 1 = Yes) r head(adult)"
  },
  {
    "objectID": "slides.html#missing-assessment-overall-missingness-4",
    "href": "slides.html#missing-assessment-overall-missingness-4",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013‚Äì2014)",
    "section": "Missing Assessment (Overall missingness: ~4%)",
    "text": "Missing Assessment (Overall missingness: ~4%)\n\n\n\n\nNo variable is completely missing. Missingness is likely MAR (Missing At Random) and is clustered mainly in bmi (4.3%) and diabetes_dx (3.1%).\n\nApply Multiple Imputation by Chained Equations (MICE)."
  },
  {
    "objectID": "slides.html#section",
    "href": "slides.html#section",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013‚Äì2014)",
    "section": "",
    "text": "No variable is completely missing. Missingness is likely MAR (Missing At Random) and is clustered mainly in bmi and diabetes_dx.\nDecision: Apply Multiple Imputation by Chained Equations (MICE)."
  },
  {
    "objectID": "slides.html#multiple-imputation-by-chained-equations-mice",
    "href": "slides.html#multiple-imputation-by-chained-equations-mice",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013‚Äì2014)",
    "section": "Multiple Imputation by Chained Equations (MICE)",
    "text": "Multiple Imputation by Chained Equations (MICE)\n\nHandles missing data (van Buuren & Groothuis-Oudshoorn, 2011; van Buuren, 2012)\nIt iteratively imputes incomplete variables using regression models\n\nPMM for continuous variables; logistic regression for binary variables\n\n5 imputations √ó 10 iterations for stability and convergence\n\nEstimates pooled using Rubin‚Äôs rules to account for uncertainty\n\nTrace plots confirm convergence and consistency with original data\n\n  glm(diabetes_dx ~ age_c + bmi_c + sex + race, family = binomial()) }) pool_mi &lt;- pool(fit_mi) \n\nModel significance: All predictors are statistically significant (p &lt; 0.01), suggesting a robust association across demographic and health variables."
  },
  {
    "objectID": "slides.html#aim-early-identification-of-risk-factors-is-key-to-diabetes-diagnosis-and-prevention",
    "href": "slides.html#aim-early-identification-of-risk-factors-is-key-to-diabetes-diagnosis-and-prevention",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013‚Äì2014)",
    "section": "Aim: Early identification of risk factors is key to Diabetes diagnosis and prevention",
    "text": "Aim: Early identification of risk factors is key to Diabetes diagnosis and prevention\nPredict diabetes using Bayesian logistic regression\nChallenges Frequentists - maximum likelihood estimation is unstable in missing data, quasi-separation, or small samples\nBayesian Method - flexible, regularize estimates, quantify uncertainty under missingness or imputation ( Baldwin and Larson 2017; Kruschke and Liddell 2017) - incorporate prior, provide credible intervals via Markov Chain Monte Carlo (MCMC) to predict patient health status across diseases - Support model checking, variable selection, and uncertainty quantification"
  },
  {
    "objectID": "slides.html#model-equation",
    "href": "slides.html#model-equation",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013‚Äì2014)",
    "section": "Model Equation",
    "text": "Model Equation\nlibrary(gt)\npriors &lt;- c(\n  set_prior(\"normal(0, 2.5)\", class = \"b\"),\n  set_prior(\"student_t(3, 0, 10)\", class = \"Intercept\") \n)\nbayes_fit &lt;- brm(\n  formula = diabetes_dx | weights(wt_norm) ~ age_c + bmi_c + sex + race,\n  data    = adult_imp1,\n  family  = bernoulli(link = \"logit\"),\n  prior   = priors,\n  chains  = 4, iter = 2000, seed = 123,\n  control = list(adapt_delta = 0.95),\n  refresh = 0   # quiet Stan output\n)\n\\[\n\\text{logit}(P(Y=1)) = \\beta_0 + \\beta_1 \\text{age}_c + \\beta_2 \\text{bmi}_c + \\beta_3 \\text{sex} + \\beta_4 \\text{race}\n\\]\n\nP(Y=1) : Probability of being diabetic\nCoefficients estimated with posterior means and 95% credible intervals."
  },
  {
    "objectID": "slides.html#data-source-and-data-exploration",
    "href": "slides.html#data-source-and-data-exploration",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013‚Äì2014)",
    "section": "Data Source and Data Exploration:",
    "text": "Data Source and Data Exploration:\nNational Health and Nutrition Examination Survey (Survey weighted dataset) (2013-2014) (NHANES)\n\nStudy population: Adults (&gt;20 years)(n=5769):\n\nAge: evenly distributed across adult age groups\nBMI: concentrated in the overweight and obese ranges\nFemale¬†&gt; Male¬†participants\nmost participants are non-diabetic, a minority are diabetic: a relatively lower prevalence of diabetes in the sample\n\nPredictors: age, sex, race, BMI\nResponse variable: diabetes_dx (diagnosis: 0 = No, 1 = Yes)\n\n  head(adult)"
  },
  {
    "objectID": "slides.html#multiple-imputation-by-chained-equations-mice-handles-missing-data",
    "href": "slides.html#multiple-imputation-by-chained-equations-mice-handles-missing-data",
    "title": "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013‚Äì2014)",
    "section": "Multiple Imputation by Chained Equations (MICE): Handles missing data",
    "text": "Multiple Imputation by Chained Equations (MICE): Handles missing data\n(van Buuren & Groothuis-Oudshoorn, 2011; van Buuren, 2012)\n  glm(diabetes_dx ~ age_c + bmi_c + sex + race, family = binomial()) }) pool_mi &lt;- pool(fit_mi) \n\nIt iteratively imputes incomplete variables using regression models\n\nPMM for continuous variables; logistic regression for binary variables\n\n5 imputations √ó 10 iterations for stability and convergence\n\nEstimates pooled using Rubin‚Äôs rules to account for uncertainty\n\nTrace plots confirm convergence and consistency with original data\nModel significance: All predictors are statistically significant (p &lt; 0.01), suggesting a robust association across demographic and health variables."
  }
]