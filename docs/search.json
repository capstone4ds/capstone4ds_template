[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayesian Logistic Regression - Application in Probability Prediction of disease (Diabetes)",
    "section": "",
    "text": "Slides: slides.html ( Go to slides.qmd to edit)"
  },
  {
    "objectID": "index.html#aims",
    "href": "index.html#aims",
    "title": "Bayesian Logistic Regression - Application in Probability Prediction of disease (Diabetes)",
    "section": "Aims",
    "text": "Aims\nThis study aims to apply Bayesian logistic regression to predict diabetes status and examine its association with body mass index (BMI), age (‚â•20 years), gender, and race using data from the 2013‚Äì2014 NHANES survey. The NHANES dataset employs a complex sampling design involving stratification, clustering, and oversampling of specific population subgroups. By adopting a Bayesian analytical framework, this study seeks to overcome challenges commonly encountered in traditional logistic regression‚Äîsuch as missing data, separation, and biases introduced by complete case analysis‚Äîthereby improving the efficiency, robustness, and interpretability of model estimates in predicting population-level health outcomes."
  },
  {
    "objectID": "index.html#bayesian-logistic-regression",
    "href": "index.html#bayesian-logistic-regression",
    "title": "Bayesian Logistic Regression - Application in Probability Prediction of disease (Diabetes)",
    "section": "Bayesian Logistic Regression",
    "text": "Bayesian Logistic Regression\nTo estimate the associations between predictors and outcome probabilities. Bayesian framework integrates prior information with observed data to generate posterior distributions, allowing direct probabilistic interpretation of parameters.\nThis approach provides flexibility in model specification, accounts for parameter uncertainty, and produces credible intervals that fully reflect uncertainty in the estimates.\nUnlike traditional frequentist methods, the Bayesian method enables inference through posterior probabilities, supporting more nuanced decision-making and interpretation.\n\nModel Structure\n\nBayesian logistic regression is a probabilistic modeling framework used to estimate the relationship between one or more predictors (continuous or categorical) and a binary outcome (e.g., presence/absence of disease).\n\nIt extends classical logistic regression by combining it with Bayesian inference, treating model parameters as random variables with probability distributions rather than fixed point estimates Leeuw and Klugkist (2012) and Klauenberg et al. (2015)\nBayesian logistic regression models the log-odds of a binary outcome as a linear combination of predictors:\n\n\n\nIn the Bayesian framework, model parameters ( ) are treated as random variables and assigned prior distributions that reflect existing knowledge or plausible ranges before observing the data. After incorporating the observed evidence, the priors are updated through Bayes‚Äô theorem (Leeuw and Klugkist 2012; Klauenberg et al. 2015):\nIn the Bayesian framework, the coefficients () are assigned prior distributions, which are updated in light of the observed data to yield posterior distributions.\nThe Bayesian approach naturally incorporates uncertainty in all model parameters.\n\nIt combines prior beliefs with observed data to produce posterior distributions according to Bayes‚Äô theorem: [ ]\n\nLikelihood: is the probability of the observed data given the model parameters (as in classical logistic regression).\n\nPrior: Encodes prior knowledge or beliefs about parameter values before observing the data.\n\nPosterior: is the updated beliefs about parameters after observing the data.\nPosterior ‚ãâ Likelihood * Prior\n\n\nPrior Specification - Regression coefficients prior: normal(0, 2.5) ‚Äî providing weakly informative regularization provide gentle regularization, constraining extreme values without overpowering the data R. van de Schoot et al.¬†(2021) - Intercept prior: Student‚Äôs t-distribution prior, student_t(3, 0, 10) (van de Schoot et al., 2013).\nThis weakly informative prior:\n- Has 3 degrees of freedom** (( = 3 )), producing heavy tails that allow for occasional large effects.\n- Is centered at 0 (( = 0 )), reflecting no initial bias toward positive or negative associations.\n- Has a scale parameter of 10 (( = 10 )), allowing broad variation in possible coefficient values.\nSuch priors improve stability in models with small sample sizes, high collinearity, or potential outliers.\n\n\nAdvantages of Bayesian Logistic Regression\n\nUncertainty quantification: Produces full posterior distributions instead of single-point estimates.\nCredible intervals: Provide the range within which a parameter lies with a specified probability (e.g., 95%).\nFlexible priors: Allow incorporation of expert knowledge or results from previous studies.\nProbabilistic predictions: Posterior predictive distributions yield direct probabilities for future observations.\nComprehensive model checking: Posterior predictive checks (PPCs) evaluate how well simulated outcomes reproduce observed data.\n\n\n\nPosterior Predictions\n\nPosterior distributions of the coefficients are used to estimate the probability of the outcome for given predictor values. This enables statements such as:\n¬†‚ÄúGiven the predictors, the probability of the outcome lies between X% and Y%.‚Äù\nPosterior predictions incorporate two sources of uncertainty:\n\nParameter uncertainty: Variability in estimated model coefficients.\nPredictive uncertainty: Variability in future outcomes given those parameters.\n\nIn Bayesian analysis, every unknown parameter ‚Äî such as a regression coefficient, mean, or variance ‚Äî is treated as a random variable with a probability distribution that expresses uncertainty given the observed data.\n\n\n\nModel Evaluation and Diagnostics\n\nModel quality and convergence are assessed using standard Bayesian diagnostics:\nPosterior inference performed using Markov Chain Monte Carlo (MCMC) sampling via the No-U-Turn Sampler (NUTS), a variant of Hamiltonian Monte Carlo (HMC). Four chains run with sufficient warm-up iterations to ensure convergence. Austin et al. (2021)\nConvergence diagnostics: Markov Chain Monte Carlo (MCMC) performance was evaluated using ( ) (R-hat) and effective sample size (ESS).\nAutocorrelation checks: Ensure independence between successive MCMC draws.\nPosterior predictive checks (PPC): Compare simulated data from posterior distributions to observed outcomes.\nBayesian R¬≤: Quantified the proportion of variance explained by the predictors, incorporating uncertainty."
  },
  {
    "objectID": "index.html#posterior-predictive-checks-ppc",
    "href": "index.html#posterior-predictive-checks-ppc",
    "title": "Bayesian Logistic Regression - Application in Probability Prediction of disease (Diabetes)",
    "section": "Posterior Predictive Checks (PPC)",
    "text": "Posterior Predictive Checks (PPC)\n\nTo assesses how the model reproduces observed data and validate model fit, visualizations below compares the predicted and observed data to show density overlays for both mean and SD. There was no large discrepancies indicating potential misfit and good alignment suggesting reliable predictions.\nA total of 50 posterior predictive sample draws generated 5,592 observations, producing a simulated distribution of predicted outcomes consistent with the sample size and assesses model fit and evaluated how well the Bayesian model reproduced the observed data pattern.\nppc_bars() plot compared the observed diabetes outcomes with 50 replicated datasets drawn from the posterior distribution.\nThe replicated distributions closely matched the observed proportions, indicating that the Bayesian model adequately captured the outcome variability and overall data structure.\n\nModel‚Äôs predictions align with reality where mean(y_rep) = average predicted probability of diabetes for each individual, across all posterior draws of the parameters and y = the actual observed diabetes status (0 = non-diabetic, 1 = diabetic).\n\n\n\nCode\nppc_bars(y = adult_imp1$diabetes_dx, yrep = pp_samples[1:50, ])\n\n\n\n\n\n\n\n\n\n\n\nCode\n#PP check for proportions (useful for binary) mean comparison to check if the simulated means match the observed mean\n\n## mean\nppc_stat(y = adult_imp1$diabetes_dx, yrep = pp_samples[1:100, ], stat = \"mean\") +\n  labs(title = \"Posterior Predictive Check: Mean of Replicates\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nPosterior Predictive stats: Mean\n-A posterior predictive check was performed on the mean diabetes outcome using 100 replicated datasets from the posterior distribution. - The distribution of the simulated means closely aligned with the observed mean, suggesting that the Bayesian model accurately captures the central tendency of the outcome.\n\n\nCode\n#PP check for proportions (useful for binary) mean and sd comparison to check if the simulated means match the observed mean\n\n## sd\nppc_stat(y = adult_imp1$diabetes_dx, yrep = pp_samples[1:100, ], stat = \"sd\") +\n  labs(title = \"PPC: Standard Deviation of Replicates\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nPosterior Predictive stats: Standard Deviation\n\nA posterior predictive check was conducted on the standard deviation of diabetes outcomes using 100 replicated datasets from the posterior distribution.\nThe simulated standard deviations closely matched the observed value, indicating that the Bayesian model adequately captures the variability in the outcome data.\n\n\n\nCode\nlibrary(brms)\nlibrary(dplyr)\nlibrary(posterior)\nlibrary(bayesplot)\nlibrary(ggplot2)\n\n\npost_wide &lt;- as_draws_df(bayes_fit)  # or posterior_samples(bayes_fit)\nnames(post_wide)  # should show: b_Intercept, b_bmi_c, b_age_c, etc.\n\n\n [1] \"b_Intercept\"           \"b_age_c\"               \"b_bmi_c\"              \n [4] \"b_sexFemale\"           \"b_raceMexicanAmerican\" \"b_raceOtherHispanic\"  \n [7] \"b_raceNHBlack\"         \"b_raceOtherDMulti\"     \"Intercept\"            \n[10] \"lprior\"                \"lp__\"                  \".chain\"               \n[13] \".iteration\"            \".draw\"                \n\n\nCode\nmcmc_areas(\n  post_wide,\n  pars = c(\"b_age_c\",\"b_bmi_c\",\"b_sexFemale\",\n           \"b_raceMexicanAmerican\",\"b_raceOtherHispanic\",\n           \"b_raceNHBlack\",\"b_raceOtherDMulti\")\n)\n\n\n\n\n\n\n\n\n\n\n\nAssumptions for Bayesian Logistic Regression\n\nData Binary outcome, independent observations\nRelationship Linear in logit, no perfect collinearity\nPriors Properly chosen and informative enough\nPosterior Proper and convergent\nFit No complete separation, good predictive checks\n\nPosterior Distributions of Regression Coefficients for Predictors of Diabetes\nThis figure presents the posterior distributions of regression coefficients estimated from the Bayesian logistic regression model. Each density curve represents the uncertainty around the parameter‚Äôs posterior mean for a given predictor.\n\nAge and BMI show strong positive associations with diabetes status, as their posterior distributions are concentrated above zero, suggesting higher values increase the likelihood of diabetes.\n\nSex (Female) has a distribution centered slightly below zero, indicating a lower probability of diabetes compared to males.\nRace categories (Mexican American, Other Hispanic, Non-Hispanic Black, and Other/Multi) show broader distributions with varying levels of uncertainty relative to the reference group (Non-Hispanic White).\nThe shaded regions indicate the 80% credible intervals, representing the range within which the true parameter values are most likely to lie based on the posterior samples.\n\n\n\n\nCode\nbayes_R2(bayes_fit)      # Model fit\n\n\n    Estimate  Est.Error      Q2.5    Q97.5\nR2 0.1313342 0.01265055 0.1064607 0.156078\n\n\n\n\nBayesian ùëÖ^2 (model fit statics):\nModel Fit to quantify predictive performance. - Explains about 13% of the variability in diabetes status, with credible uncertainty bounds suggesting reasonable but modest explanatory power. - but other factors (e.g., genetics, lifestyle, environment) also contribute to outcome variability.\n\n\nCode\npredicted &lt;- fitted(bayes_fit, summary = TRUE)\nobserved &lt;- adult_imp1[, c(\"bmi\", \"age\")]\n\n# Plot for **bmi** (obs vs pred)\n\nggplot(data = NULL, aes(x = observed$bmi, y = predicted[, \"Estimate\"])) +\n  geom_point() +\n  geom_errorbar(aes(ymin = predicted[, \"Q2.5\"], ymax = predicted[, \"Q97.5\"])) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"red\") +\n  xlab(\"Observed bmi\") + ylab(\"Predicted bmi\")\n\n\n\n\n\n\n\n\n\n\n\nObserved vs.¬†Predicted BMI\n\nA comparison of observed BMI values with their posterior predicted estimates was performed using the Bayesian model.\nEach point represents an individual‚Äôs observed BMI versus the model‚Äôs predicted mean.\nError bars indicate the 95% credible intervals of the predictions.\nThe dashed red line represents perfect prediction (observed = predicted). - The plot demonstrates that the model‚Äôs predictions generally align with the observed data, with most points closely following the diagonal, indicating good predictive performance for BMI.\n\n\n\nCode\nlibrary(posterior)\nlibrary(dplyr)\nlibrary(tidyr)\n\n# Extract posterior draws as a matrix, then convert to tibble\npost &lt;- as_draws_matrix(bayes_fit) %&gt;%   # safer than as_draws_df for manipulation\n  as.data.frame() %&gt;%\n  select(b_bmi_c, b_age_c) %&gt;%\n  pivot_longer(\n    everything(),\n    names_to = \"term\",\n    values_to = \"estimate\"\n  ) %&gt;%\n  mutate(\n    term = case_when(\n      term == \"b_bmi_c\" ~ \"BMI (per 1 SD)\",\n      term == \"b_age_c\" ~ \"Age (per 1 SD)\"\n    ),\n    type = \"Posterior\"\n  )\nprior_draws &lt;- tibble(\n  term = rep(c(\"BMI (per 1 SD)\", \"Age (per 1 SD)\"), each = 4000),\n  estimate = c(rnorm(4000, 0, 1), rnorm(4000, 0, 1)),\n  type = \"Prior\"\n)\ncombined_draws &lt;- bind_rows(prior_draws, post)\n\n\n\n\nCode\nlibrary(ggplot2)\n\nggplot(combined_draws, aes(x = estimate, fill = type)) +\n  geom_density(alpha = 0.4) +\n  facet_wrap(~ term, scales = \"free\", ncol = 2) +\n  theme_minimal(base_size = 13) +\n  labs(\n    title = \"Prior vs Posterior Distributions\",\n    x = \"Coefficient estimate\",\n    y = \"Density\",\n    fill = \"\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nPrior vs Posterior Distributions Visualization:\nDensity plots were generated to overlay the prior and posterior distributions for BMI and age. Posterior distributions were narrower and shifted away from zero relative to the priors, indicating that the data provided substantial evidence for positive associations of BMI and age with diabetes risk. Faceting by predictor enabled clear visual comparison of how each coefficient‚Äôs distribution evolved after model updating.\nInterpretation: The shift from diffuse priors to concentrated posteriors demonstrates the model‚Äôs ability to incorporate empirical evidence and refine prior beliefs. Narrower posterior distributions reflect reduced uncertainty in parameter estimates. The posterior predictive proportions of diabetes closely matched the observed prevalence in the analytic dataset, supporting the model‚Äôs reliability for inference and prediction.\n\n\nModel Comparison\n\nModel Comparison Across all analytic methods: survey-weighted maximum likelihood estimation (MLE), multiple imputation with pooled estimates (MICE), and Bayesian regression shows the associations between BMI, age, and diabetes diagnosis were consistent in direction and magnitude.\nFor BMI, the odds ratios ranged from 1.73 (95% CI: 1.58‚Äì1.89) in the MICE-pooled model to 1.89 (95% CI: 1.65‚Äì2.15) in the survey-weighted MLE model, and 1.87 (95% CrI: 1.71‚Äì2.05) in the Bayesian model.\nFor age, the estimated odds ratios were 2.90 (95% CI: 2.60‚Äì3.24) using MICE, 3.03 (95% CI: 2.70‚Äì3.40) from the survey-weighted MLE model, and 2.99 (95% CrI: 2.64‚Äì3.37) in the Bayesian analysis.\n\n\n\nCode\n# Results\n\n #Build compact results table (BMI & Age only) \nlibrary(dplyr); \nlibrary(tidyr); \nlibrary(knitr); \nlibrary(stringr)\n\n# pretty \"OR (LCL‚ÄìUCL)\" string\n\n  fmt_or &lt;- function(or, lcl, ucl, digits = 2) {\n  paste0(\n    formatC(or,  format = \"f\", digits = digits), \" (\",\n    formatC(lcl, format = \"f\", digits = digits), \"‚Äì\",\n    formatC(ucl, format = \"f\", digits = digits), \")\"\n  )\n}\n\n# guardrails: require these to exist from Modeling\nstopifnot(exists(\"svy_or\"), exists(\"mi_or\"), exists(\"bayes_or\"))\nfor (nm in c(\"svy_or\",\"mi_or\",\"bayes_or\")) {\n  if (!all(c(\"term\",\"OR\",\"LCL\",\"UCL\") %in% names(get(nm)))) {\n    stop(nm, \" must have columns: term, OR, LCL, UCL\")\n  }\n}\n\nsvy_tbl   &lt;- svy_or   %&gt;% mutate(Model = \"Survey-weighted MLE\")\nmi_tbl    &lt;- mi_or    %&gt;% mutate(Model = \"mice pooled\")\nbayes_tbl &lt;- bayes_or %&gt;% mutate(Model = \"Bayesian\")\n\nall_tbl &lt;- bind_rows(svy_tbl, mi_tbl, bayes_tbl) %&gt;%\n  mutate(term = case_when(\n    str_detect(term, \"bmi_c|\\\\bBMI\\\\b\") ~ \"BMI (per 1 SD)\",\n    str_detect(term, \"age_c|\\\\bAge\\\\b\") ~ \"Age (per 1 SD)\",\n    TRUE ~ term\n  )) %&gt;%\n  filter(term %in% c(\"BMI (per 1 SD)\", \"Age (per 1 SD)\")) %&gt;%\n  mutate(OR_CI = fmt_or(OR, LCL, UCL, digits = 2)) %&gt;%\n  select(Model, term, OR_CI) %&gt;%\n  arrange(\n    factor(Model, levels = c(\"Survey-weighted MLE\",\"mice pooled\",\"Bayesian\")),\n    factor(term,  levels = c(\"BMI (per 1 SD)\",\"Age (per 1 SD)\"))\n  )\n\nres_wide &lt;- all_tbl %&gt;%\n  pivot_wider(names_from = term, values_from = OR_CI) %&gt;%\n  rename(\n    `BMI (per 1 SD) OR (95% CI)` = `BMI (per 1 SD)`,\n    `Age (per 1 SD) OR (95% CI)` = `Age (per 1 SD)`\n  )\n\nkable(\n  res_wide,\n  align = c(\"l\",\"c\",\"c\"),\n  caption = \"Odds ratios (per 1 SD) with 95% CIs across models\"\n)\n\n\n\nOdds ratios (per 1 SD) with 95% CIs across models\n\n\n\n\n\n\n\nModel\nBMI (per 1 SD) OR (95% CI)\nAge (per 1 SD) OR (95% CI)\n\n\n\n\nSurvey-weighted MLE\n1.89 (1.65‚Äì2.15)\n3.03 (2.70‚Äì3.40)\n\n\nmice pooled\n1.73 (1.58‚Äì1.89)\n2.90 (2.60‚Äì3.24)\n\n\nBayesian\n1.87 (1.71‚Äì2.05)\n2.99 (2.64‚Äì3.37)\n\n\n\n\n\n\n\nCode\n# Compute proportion of diabetes=1 for each draw\npp_proportion &lt;- rowMeans(pp_samples)  # proportion of 1's in each posterior draw\n\n# Optional: visualize the posterior probability distribution\npp_proportion_df &lt;- tibble(proportion = pp_proportion)\n\nggplot(pp_proportion_df, aes(x = proportion)) +\n  geom_histogram(binwidth = 0.01, fill = \"skyblue\", color = \"black\") +\n  labs(\n    title = \"Posterior Distribution of Proportion of Diabetes = 1\",\n    x = \"Proportion of Diabetes = 1\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nPosterior Distribution of Diabetes Prevalence\n\nA histogram of these values shows the distribution of predicted prevalence calculated for each draw of the Bayesian model, reflecting uncertainty in the model estimates, central tendency and variability of diabetes prevalence\nMost posterior predictions cluster around 10‚Äì11%, indicating good alignment with the observed/imputed data and demonstrating that the model captures the underlying population pattern.\n\n\n\nCode\nlibrary(dplyr)\nlibrary(knitr)\nsvy_mean &lt;- svymean(~diabetes_dx, nhanes_design_adult, na.rm = TRUE)\n\n# Create summary table\nsummary_table &lt;- tibble(\n  Method = c(\"Survey-weighted mean (NHANES)\", \n             \"Imputed dataset mean\", \n             \"Posterior predictive mean\"),\n  diabetes_mean = c(\n    coef(svy_mean),           # survey-weighted mean\n    mean(adult_imp1$diabetes_dx, na.rm = TRUE),  # imputed dataset\n    mean(pp_proportion)       # posterior predictive mean\n  ),\n  SE = c(\n    SE(svy_mean),             # survey-weighted SE\n    NA,                       # not available for raw mean\n    NA                        # not available for posterior predictive\n  )\n)\n\n# Render table\nkable(summary_table, digits = 4, caption = \"Comparison of Diabetes Prevalence Across Methods\")\n\n\n\nComparison of Diabetes Prevalence Across Methods\n\n\nMethod\ndiabetes_mean\nSE\n\n\n\n\nSurvey-weighted mean (NHANES)\n0.0890\n0.0048\n\n\nImputed dataset mean\n0.1105\nNA\n\n\nPosterior predictive mean\n0.1089\nNA\n\n\n\n\n\n\n\nComparison of Diabetes Prevalence Across Methods\n\nThe estimated prevalence of diabetes was consistent across different analytical approaches:\nSurvey-weighted mean (NHANES): 8.9% (SE = 0.0048)\nImputed dataset mean (MICE): 11.1%\nPosterior predictive mean (Bayesian model): 10.95%\nThese results indicate that multiple imputation and Bayesian posterior predictions yield slightly higher prevalence estimates than the raw survey-weighted mean, but all methods are broadly consistent. The posterior predictive distribution closely matches the observed prevalence, suggesting that the Bayesian model is well-calibrated.\nBayesian model predicts that about 10‚Äì11% of this population has diabetes, with a relatively narrow range across posterior draws, reflects uncertainty in the estimate\nWhile most predictions cluster around 10‚Äì11%, the model allows for values as low as 8.5% and as high as 12.8%.\nOn comparing this with the raw imputed data proportion show that the the model predictions align with the observed/imputed data.\nThe predicted proportion incorporates uncertainty from both the Bayesian model and the imputed data, providing a more robust estimate of diabetes prevalence.\nThese results suggest that approximately 1 in 10 adults in this population may have diabetes, which can help policymakers and clinicians plan and prioritize targeted interventions effectively.\n\n\n\nCode\nlibrary(tidyverse)\n\n# Posterior predicted proportion vector\n# pp_proportion &lt;- rowMeans(pp_samples)  # if not already done\n\nknown_prev &lt;- 0.089   # NHANES prevalence\n\n# Posterior summary\nposterior_mean &lt;- mean(pp_proportion)\nposterior_ci &lt;- quantile(pp_proportion, c(0.025, 0.975))  # 95% credible interval\n\n# Create a data frame for plotting\npp_df &lt;- tibble(proportion = pp_proportion)\n\n# Plot\nggplot(pp_df, aes(x = proportion)) +\n  geom_histogram(binwidth = 0.005, fill = \"skyblue\", color = \"black\") +\n  geom_vline(xintercept = known_prev, color = \"red\", linetype = \"dashed\", size = 1) +\n  geom_vline(xintercept = posterior_mean, color = \"blue\", linetype = \"solid\", size = 1) +\n  geom_rect(aes(xmin = posterior_ci[1], xmax = posterior_ci[2], ymin = 0, ymax = Inf),\n            fill = \"blue\", alpha = 0.1, inherit.aes = FALSE) +\n  labs(\n    title = \"Posterior Predicted Diabetes Proportion vs NHANES Prevalence\",\n    subtitle = paste0(\"Red dashed = NHANES prevalence (\", known_prev, \n                      \"), Blue solid = Posterior mean (\", round(posterior_mean,3), \")\"),\n    x = \"Proportion of Diabetes = 1\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nPosterior Predicted Diabetes Proportion vs.¬†NHANES Prevalence\n\nWe compared the posterior predicted proportion of diabetes from the Bayesian model with the observed NHANES prevalence (8.9%).\nThe blue solid line represents the posterior mean, while the shaded blue area indicates the 95% credible interval of predicted proportions.\nThe red dashed line shows the NHANES survey-weighted prevalence for reference.\nMost posterior predictions cluster around 10‚Äì11%, slightly higher than the NHANES mean, but the credible interval overlaps the observed prevalence, indicating good model calibration.\nThis visualization highlights the model‚Äôs ability to capture uncertainty in predictions while remaining consistent with the observed data.\n\n\n\nCode\nlibrary(dplyr)\n\n# Posterior predicted proportion\n\nposterior_mean &lt;- mean(pp_proportion)\nposterior_ci &lt;- quantile(pp_proportion, c(0.025, 0.975))  # 95% credible interval\n\n# NHANES prevalence with SE from survey::svymean\n# Suppose you already have:\n# svymean(~diabetes_dx, nhanes_design_adult, na.rm = TRUE)\nknown_prev &lt;- 0.089        # Mean prevalence\nknown_se   &lt;- 0.0048       # Standard error from survey\n\n# Calculate 95% confidence interval\nknown_ci &lt;- c(\n  known_prev - 1.96 * known_se,\n  known_prev + 1.96 * known_se\n)\n\n# Print results\ndata.frame(\n  Type = c(\"Posterior Prediction\", \"NHANES Prevalence\"),\n  Mean = c(posterior_mean, known_prev),\n  Lower_95 = c(posterior_ci[1], known_ci[1]),\n  Upper_95 = c(posterior_ci[2], known_ci[2])\n)\n\n\n                     Type      Mean   Lower_95  Upper_95\n2.5% Posterior Prediction 0.1089181 0.09629381 0.1216962\n        NHANES Prevalence 0.0890000 0.07959200 0.0984080\n\n\nCode\n# Create a data frame for plotting\nci_df &lt;- data.frame(\n  Type = c(\"Posterior Prediction\", \"NHANES Prevalence\"),\n  Mean = c(0.1096674, 0.089),\n  Lower_95 = c(0.09772443, 0.079592),\n  Upper_95 = c(0.1210658, 0.098408)\n)\n\n\n# --- 1. Survey-weighted (Population) prevalence ---\npop_est &lt;- svymean(~diabetes_dx, nhanes_design_adult, na.rm = TRUE)\npop_prev &lt;- as.numeric(pop_est)\npop_se &lt;- as.numeric(SE(pop_est))\npop_ci &lt;- c(pop_prev - 1.96 * pop_se, pop_prev + 1.96 * pop_se)\n\n# --- 2. Bayesian posterior prevalence ---\n# bayes_pred = matrix of posterior draws (iterations √ó individuals)\npp_proportion &lt;- rowMeans(pp_samples)             # prevalence per posterior draw\npost_prev &lt;- mean(pp_proportion)                  # posterior mean prevalence\npost_ci &lt;- quantile(pp_proportion, c(0.025, 0.975))  # 95% credible interval\n\n# --- 3. Combine into one data frame ---\nbar_df &lt;- tibble(\n  Source     = c(\"Survey-weighted (Population)\", \"Bayesian Posterior\"),\n  Prevalence = c(pop_prev, post_prev),\n  CI_low     = c(pop_ci[1], post_ci[1]),\n  CI_high    = c(pop_ci[2], post_ci[2])\n)\n\n# --- 4. Plot ---\nggplot(bar_df, aes(x = Source, y = Prevalence, fill = Source)) +\n  geom_col(alpha = 0.85, width = 0.6) +\n  geom_errorbar(\n    aes(ymin = CI_low, ymax = CI_high),\n    width = 0.15,\n    color = \"black\",\n    linewidth = 0.8\n  ) +\n  guides(fill = \"none\") +\n  labs(\n    title = \"Population vs Posterior Diabetes Prevalence\",\n    subtitle = \"Survey-weighted estimate (design-based) vs Bayesian (model-based)\",\n    y = \"Prevalence (Proportion with Diabetes)\",\n    x = NULL\n  ) +\n  theme_minimal(base_size = 13)\n\n\n\n\n\n\n\n\n\n\n\nComparison of Diabetes Prevalence Across Methods\n\nThe bar plot compares survey-weighted, imputed, and Bayesian posterior estimates of diabetes prevalence:\nSurvey-weighted prevalence: 8.9% (95% CI: 0.080‚Äì0.098), reflecting the NHANES population after accounting for complex sampling.\nImputed (unweighted) prevalence: 11.1%, slightly higher due to unadjusted overrepresentation of subgroups with higher diabetes rates.\nBayesian posterior mean: 10.9% (95% CrI: 0.098‚Äì0.121), closely replicating the imputed data while slightly shrinking toward the population-level mean, consistent with Bayesian regularization.\nThe posterior credible interval overlaps the survey 95% CI, indicating that the Bayesian model reproduces population-level prevalence accurately. This demonstrates good model calibration and predictive validity, while visualizing the uncertainty of both survey-based and model-based estimates.\n\nPractical Implications - Health departments can estimate diabetes burden at the state or county level using Bayesian small-area estimation. - Clinicians and public health researchers can plan targeted screening where predicted prevalence is higher than observed. - Epidemiologists can validate disease models before applying them to regions without survey data.\n\n\nCode\nlibrary(tidyr)\nlibrary(bayesplot)\nlibrary(posterior)\n\n# Convert fitted model to draws array\npost_array &lt;- as_draws_array(bayes_fit)  # draws x chains x parameters\n\n# Plot autocorrelation for age and bmi\nmcmc_acf(post_array, pars = c(\"b_age_c\", \"b_bmi_c\"))\n\n\n\n\n\n\n\n\n\n\n\nMCMC Autocorrelation for Key Parameters\nPairwise plots (mcmc_pairs, posterior) explore correlations between parameters. - Autocorrelation plots for the posterior draws of age and BMI coefficients assess chain mixing and convergence showing correlation of each draw with its lagged values across iterations. - Rapid decay of autocorrelation toward zero indicates that the Markov chains are mixing well and successive draws are relatively independent. - Both age and BMI coefficients exhibited low autocorrelation after a few lags, supporting the reliability of posterior estimates. - This diagnostic confirms that the Bayesian model sampling was adequate and stable, ensuring valid inference from the posterior distributions.\n\nAutocorrelation within MCMC samples for a single parameter across iterations ‚Äî i.e., how strongly each sample depends on its previous ones.\nIt reflects MCMC chain mixing and efficiency, not relationships between parameters\n\n\n\nCode\nlibrary(bayesplot)\n\n# Extract posterior draws\npost &lt;- as_draws_df(bayes_fit)\n\n# Select numeric parameters of interest\npost_subset &lt;- post %&gt;% \n  dplyr::select(b_age_c, b_bmi_c, b_sexFemale, \n                b_raceMexicanAmerican, b_raceNHBlack)\n\n# Compute correlation matrix\ncor_matrix &lt;- cor(post_subset)\n\n# Visualize\nmcmc_pairs(as_draws_array(bayes_fit), \n           pars = c(\"b_age_c\", \"b_bmi_c\", \"b_sexFemale\"),\n           off_diag_args = list(size = 1.5, alpha = 0.4))\n\n\n\n\n\n\n\n\n\nInterpretation of Posterior correlation among parameters (e.g., between b_age_c, b_bmi_c, b_sexFemale)\nEach diagonal panel shows the posterior density (distribution) of one parameter, while the off-diagonal scatterplots show pairwise relationships (correlations) between parameters across posterior draws.\n\n1. Marginal Distributions (diagonals)\n\nThe histograms show that all parameters (b_age_c, b_bmi_c, b_sexFemale) have approximately symmetric and unimodal posterior distributions.\nThis indicates stable and well-identified estimates with no sampling irregularities or multimodality.\nThe narrow spread (small variance) reflects high precision of parameter estimates.\n\n\n\n2. Joint Distributions (off-diagonal scatterplots)\n\nThe scatterplots show elliptical clouds centered around the mean, with no strong linear patterns.\nThis means low posterior correlation among parameters ‚Äî i.e., b_age_c, b_bmi_c, and b_sexFemale are largely independent in their posterior estimates.\nThe absence of diagonal streaks or skewed clusters suggests that collinearity is minimal and that the MCMC chains successfully explored the parameter space.\n\n\n\n3. Subtle Correlation Notes\n\nA mild negative tilt between b_age_c and b_sexFemale indicates a slight negative correlation, meaning as the posterior estimate for age increases, the effect of being female slightly decreases.\nHowever, this pattern is weak ‚Äî confirming that both predictors contribute distinct information to the diabetes outcome.\nThe absence of strong linear relationships among parameters suggests that age, BMI, and sex independently contributed to the prediction of diabetes status. The smooth, unimodal histograms along the diagonals confirmed stable model convergence and well-behaved posterior samples."
  },
  {
    "objectID": "About.html",
    "href": "About.html",
    "title": "About",
    "section": "",
    "text": "Contributors\n\nNamita Mishra ‚Äì analytic coding, content draft, developed project plan, collaborated via GitHub.\nAutumn Wilcox ‚Äì analytic coding, content draft, structured project workflow, collaborated via GitHub.\n\n\nDr.¬†Namita Mishra is a physician, a Head and Neck surgeon and a public health researcher with a strong foundation in medicine, epidemiology, and data science. She is a graduate student in Data Science (Health Analytics).\nHer work focuses on early detection and prevention of non-communicable diseases (cancer, obesity) and on health disparities at community level. She has researched salivary gland tumors, cardiac implants and community based research on healthy food access. Leveraging skills from Data Science, she integrates statistical modeling and Bayesian methods into her analyses. Her Bioinformatics expertise utilizes geodata visualization tools (3D Maps and GIS) for presentations.Passionate about bridging clinical insight with data-driven approaches, dedicated to advancing sustainable, evidence-based solutions in epidemiology and community health.\nOutside work she explores - gardening, cooking, singing, and sewing.\nüìß Contact: nmishra@uwf.edu\n\nAutumn S. Wilcox is a U.S. Navy veteran and Data Science graduate student at the University of West Florida, specializing in Analytics and Modeling. She has over nine years of experience in Network Operations and Technical Writing, including her current role at Navy Federal Credit Union, where she supports enterprise technology and process documentation initiatives. Autumn also holds certification in Clinical Research Quality Management (CRQM) and has contributed to quality oversight and compliance efforts in clinical research settings.\nHer background bridges technology, analytics, and healthcare, with a focus on applying data-driven approaches to improve communication and systems reliability. Outside of work, Autumn enjoys traveling, photography, and finding creative inspiration through music.\nüìß Contact: awr12@students.uwf.edu"
  }
]