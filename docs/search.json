[
  {
    "objectID": "About.html",
    "href": "About.html",
    "title": "About",
    "section": "",
    "text": "Contributors\n\nNamita Mishra ‚Äì analytic coding, content draft, developed project plan, collaborated via GitHub.\nAutumn Wilcox ‚Äì analytic coding, content draft, structured project workflow, collaborated via GitHub.\n\n\nDr.¬†Namita Mishra is a physician, a Head and Neck surgeon and a public health researcher with a strong foundation in medicine, epidemiology, and data science. She is a graduate student in Data Science (Health Analytics).\nHer work focuses on early detection and prevention of non-communicable diseases (cancer, obesity) and on health disparities at community level. She has researched salivary gland tumors, cardiac implants and community based research on healthy food access. Leveraging skills from Data Science, she integrates statistical modeling and Bayesian methods into her analyses. Her Bioinformatics expertise utilizes geodata visualization tools (3D Maps and GIS) for presentations.Passionate about bridging clinical insight with data-driven approaches, dedicated to advancing sustainable, evidence-based solutions in epidemiology and community health.\nOutside work she explores - gardening, cooking, singing, and sewing.\nüìß Contact: nmishra@uwf.edu\n\nAutumn S. Wilcox is a U.S. Navy veteran and Data Science graduate student at the University of West Florida, specializing in Analytics and Modeling. She has over nine years of experience in Network Operations and Technical Writing, including her current role at Navy Federal Credit Union, where she supports enterprise technology and process documentation initiatives. Autumn also holds certification in Clinical Research Quality Management (CRQM) and has contributed to quality oversight and compliance efforts in clinical research settings.\nHer background bridges technology, analytics, and healthcare, with a focus on applying data-driven approaches to improve communication and systems reliability. Outside of work, Autumn enjoys traveling, photography, and finding creative inspiration through music.\nüìß Contact: awr12@students.uwf.edu"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayesian Logistic Regression - Application in Probability Prediction of disease (Diabetes)",
    "section": "",
    "text": "Slides: slides.html ( Go to slides.qmd to edit)"
  },
  {
    "objectID": "index.html#aims",
    "href": "index.html#aims",
    "title": "Bayesian Logistic Regression - Application in Probability Prediction of disease (Diabetes)",
    "section": "Aims",
    "text": "Aims\nThis study aims to apply Bayesian logistic regression to predict diabetes status and examine its association with body mass index (BMI), age (‚â•20 years), gender, and race using data from the 2013‚Äì2014 NHANES survey. The NHANES dataset employs a complex sampling design involving stratification, clustering, and oversampling of specific population subgroups. By adopting a Bayesian analytical framework, this study seeks to overcome challenges commonly encountered in traditional logistic regression‚Äîsuch as missing data, separation, and biases introduced by complete case analysis‚Äîthereby improving the efficiency, robustness, and interpretability of model estimates in predicting population-level health outcomes."
  },
  {
    "objectID": "index.html#bayesian-logistic-regression",
    "href": "index.html#bayesian-logistic-regression",
    "title": "Bayesian Logistic Regression - Application in Probability Prediction of disease (Diabetes)",
    "section": "Bayesian Logistic Regression",
    "text": "Bayesian Logistic Regression\nTo estimate the associations between predictors and outcome probabilities. Bayesian framework integrates prior information with observed data to generate posterior distributions, allowing direct probabilistic interpretation of parameters.\nThis approach provides flexibility in model specification, accounts for parameter uncertainty, and produces credible intervals that fully reflect uncertainty in the estimates.\nUnlike traditional frequentist methods, the Bayesian method enables inference through posterior probabilities, supporting more nuanced decision-making and interpretation.\n\nModel Structure\n\nBayesian logistic regression is a probabilistic modeling framework used to estimate the relationship between one or more predictors (continuous or categorical) and a binary outcome (e.g., presence/absence of disease).\n\nIt extends classical logistic regression by combining it with Bayesian inference, treating model parameters as random variables with probability distributions rather than fixed point estimates Leeuw and Klugkist (2012) and Klauenberg et al. (2015)\nBayesian logistic regression models the log-odds of a binary outcome as a linear combination of predictors:\n\n\n\nIn the Bayesian framework, model parameters ( ) are treated as random variables and assigned prior distributions that reflect existing knowledge or plausible ranges before observing the data. After incorporating the observed evidence, the priors are updated through Bayes‚Äô theorem (Leeuw and Klugkist 2012; Klauenberg et al. 2015):\nIn the Bayesian framework, the coefficients () are assigned prior distributions, which are updated in light of the observed data to yield posterior distributions.\nThe Bayesian approach naturally incorporates uncertainty in all model parameters.\n\nIt combines prior beliefs with observed data to produce posterior distributions according to Bayes‚Äô theorem: [ ]\n\nLikelihood: is the probability of the observed data given the model parameters (as in classical logistic regression).\n\nPrior: Encodes prior knowledge or beliefs about parameter values before observing the data.\n\nPosterior: is the updated beliefs about parameters after observing the data.\nPosterior ‚ãâ Likelihood * Prior\n\n\nPrior Specification - Regression coefficients prior: normal(0, 2.5) ‚Äî providing weakly informative regularization provide gentle regularization, constraining extreme values without overpowering the data R. van de Schoot et al.¬†(2021) - Intercept prior: Student‚Äôs t-distribution prior, student_t(3, 0, 10) (van de Schoot et al., 2013).\nThis weakly informative prior:\n- Has 3 degrees of freedom** (( = 3 )), producing heavy tails that allow for occasional large effects.\n- Is centered at 0 (( = 0 )), reflecting no initial bias toward positive or negative associations.\n- Has a scale parameter of 10 (( = 10 )), allowing broad variation in possible coefficient values.\nSuch priors improve stability in models with small sample sizes, high collinearity, or potential outliers.\n\n\nAdvantages of Bayesian Logistic Regression\n\nUncertainty quantification: Produces full posterior distributions instead of single-point estimates.\nCredible intervals: Provide the range within which a parameter lies with a specified probability (e.g., 95%).\nFlexible priors: Allow incorporation of expert knowledge or results from previous studies.\nProbabilistic predictions: Posterior predictive distributions yield direct probabilities for future observations.\nComprehensive model checking: Posterior predictive checks (PPCs) evaluate how well simulated outcomes reproduce observed data.\n\n\n\nPosterior Predictions\n\nPosterior distributions of the coefficients are used to estimate the probability of the outcome for given predictor values. This enables statements such as:\n¬†‚ÄúGiven the predictors, the probability of the outcome lies between X% and Y%.‚Äù\nPosterior predictions incorporate two sources of uncertainty:\n\nParameter uncertainty: Variability in estimated model coefficients.\nPredictive uncertainty: Variability in future outcomes given those parameters.\n\nIn Bayesian analysis, every unknown parameter ‚Äî such as a regression coefficient, mean, or variance ‚Äî is treated as a random variable with a probability distribution that expresses uncertainty given the observed data.\n\n\n\nModel Evaluation and Diagnostics\n\nModel quality and convergence are assessed using standard Bayesian diagnostics:\nPosterior inference performed using Markov Chain Monte Carlo (MCMC) sampling via the No-U-Turn Sampler (NUTS), a variant of Hamiltonian Monte Carlo (HMC). Four chains run with sufficient warm-up iterations to ensure convergence. Austin et al. (2021), Dong, An, and Kim (2019)\nConvergence diagnostics: Markov Chain Monte Carlo (MCMC) performance was evaluated using ( ) (R-hat) and effective sample size (ESS).\nAutocorrelation checks: Ensure independence between successive MCMC draws.\nPosterior predictive checks (PPC): Compare simulated data from posterior distributions to observed outcomes.\nBayesian R¬≤: Quantified the proportion of variance explained by the predictors, incorporating uncertainty."
  },
  {
    "objectID": "index.html#survey-weighted-logistic-regression-complete-case-analysis",
    "href": "index.html#survey-weighted-logistic-regression-complete-case-analysis",
    "title": "Bayesian Logistic Regression - Application in Probability Prediction of disease (Diabetes)",
    "section": "Survey-Weighted Logistic Regression (Complete-Case Analysis)",
    "text": "Survey-Weighted Logistic Regression (Complete-Case Analysis)\n\nWe conducted a complete-case survey-weighted logistic regression to estimate the association between age, BMI, sex, and race with diabetes status.\nAll analyses accounted for the NHANES complex sampling design, including sampling weights, primary sampling units (PSUs), and strata with the sufficiency of outcome data and categorical predictors (sex and race) retained at least two observed levels among participants with non-missing diabetes outcomes.\nFor a complete-case indicator to retain only participants with non-missing values on the outcome and all model predictors (age, BMI, sex, and race).\nComplete-case dataset to fit a survey-weighted logistic regression model:\n\ndiabetes_dx‚àºage_c + bmi_c + sex + race\n\nThe model was estimated using svyglm() with a quasibinomial family to accommodate the binary outcome under the NHANES design.\nThis generated design-adjusted coefficient estimates, standard errors, and odds ratios reflecting population-representative associations for U.S. adults.\n\n\n\nCode\n# Modeling\n\nlibrary(broom)\nlibrary(mice)\nlibrary(brms)\nlibrary(posterior)\nlibrary(bayesplot)\nlibrary(knitr)\n\n# --- Guardrails for modeling ---\nn_outcome &lt;- sum(!is.na(adult$diabetes_dx))\nif (n_outcome == 0) stop(\"Too few non-missing outcomes for modeling. n = 0\")\n\n# Ensure factors and &gt;=2 observed levels among complete outcomes\nadult &lt;- adult %&gt;%\n  dplyr::mutate(\n    sex  = if (!is.factor(sex))  factor(sex)  else sex,\n    race = if (!is.factor(race)) factor(race) else race\n  )\n\nif (nlevels(droplevels(adult$sex[!is.na(adult$diabetes_dx)]))  &lt; 2)\n  stop(\"sex has &lt;2 observed levels after filtering; check data availability.\")\nif (nlevels(droplevels(adult$race[!is.na(adult$diabetes_dx)])) &lt; 2)\n  stop(\"race has &lt;2 observed levels after filtering; check Data Prep.\")\n\n   #  Survey-weighted complete-case \n\n# Build a logical filter on the original adult data (same length as design$data)\nkeep_cc &lt;- with(\n  adult,\n  !is.na(diabetes_dx) & !is.na(age_c) & !is.na(bmi_c) &\n  !is.na(sex) & !is.na(race)\n)\n\n# Subset the survey design using the logical vector (same length as original)\ndes_cc &lt;- subset(nhanes_design_adult, keep_cc)\n\n# Corresponding complete-case data (optional)\ncc &lt;- adult[keep_cc, ] |&gt; droplevels()\ncat(\"\\nComplete-case N for survey-weighted model:\", nrow(cc), \"\\n\")\n\n\n\nComplete-case N for survey-weighted model: 5349 \n\n\nCode\nform_cc &lt;- diabetes_dx ~ age_c + bmi_c + sex + race\nsvy_fit &lt;- survey::svyglm(formula = form_cc, design = des_cc, family = quasibinomial())\nsummary(svy_fit)\n\n\n\nCall:\nsvyglm(formula = form_cc, design = des_cc, family = quasibinomial())\n\nSurvey design:\nsubset(nhanes_design_adult, keep_cc)\n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          -2.67143    0.11935 -22.383 1.68e-08 ***\nage_c                 1.10833    0.05042  21.981 1.94e-08 ***\nbmi_c                 0.63412    0.05713  11.099 3.88e-06 ***\nsexFemale            -0.63844    0.10926  -5.843 0.000386 ***\nraceMexican American  0.71091    0.13681   5.196 0.000826 ***\nraceOther Hispanic    0.46469    0.13474   3.449 0.008712 ** \nraceNH Black          0.51221    0.15754   3.251 0.011677 *  \nraceOther/Multi       0.84460    0.17756   4.757 0.001433 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasibinomial family taken to be 0.8455444)\n\nNumber of Fisher Scoring iterations: 6\n\n\n\n\nCode\nsvy_or &lt;- broom::tidy(svy_fit, conf.int = TRUE) %&gt;%\n  dplyr::mutate(OR = exp(estimate), LCL = exp(conf.low), UCL = exp(conf.high)) %&gt;%\n  dplyr::select(term, OR, LCL, UCL, p.value) %&gt;%\n  dplyr::filter(term != \"(Intercept)\")\nknitr::kable(svy_or, caption = \"Survey-weighted odds ratios (per 1 SD)\")\n\n\n\nSurvey-weighted odds ratios (per 1 SD)\n\n\nterm\nOR\nLCL\nUCL\np.value\n\n\n\n\nage_c\n3.0292807\n2.6967690\n3.4027912\n0.0000000\n\n\nbmi_c\n1.8853571\n1.6526296\n2.1508579\n0.0000039\n\n\nsexFemale\n0.5281132\n0.4104905\n0.6794397\n0.0003857\n\n\nraceMexican American\n2.0358434\n1.4850041\n2.7910081\n0.0008262\n\n\nraceOther Hispanic\n1.5915182\n1.1664529\n2.1714810\n0.0087119\n\n\nraceNH Black\n1.6689718\n1.1605895\n2.4000450\n0.0116773\n\n\nraceOther/Multi\n2.3270527\n1.5451752\n3.5045697\n0.0014331\n\n\n\n\n\nInterpretation\n\nAge (age_c) Estimate: 1.10833 (p &lt; 0.0001) For each 1 SD increase in age (if centered & scaled):The log-odds of diabetes increase by 1.108.The odds increase by exp(1.108) ‚âà 3.03.\nOlder adults have ~3 times higher odds of diabetes per unit increase in age_c.¬†BMI (bmi_c): Estimate: 0.63412 (p &lt; 0.0001) For each 1-unit increase in centered BMI:Odds of diabetes increase by exp(0.634) ‚âà 1.88.\nSex (reference = Male): sexFemale\nEstimate: ‚Äì0.63844 (p &lt; 0.001) Females have:Lower log-odds of diabetes than males.Odds ratio = exp(‚Äì0.638) ‚âà 0.53. Females have about 47% lower odds of diabetes compared to males, after adjusting for age, BMI, and race.\nRace (reference = Non-Hispanic White) Mexican American Estimate: 0.71091 OR = exp(0.711) ‚âà 2.04 About 2√ó higher odds of diabetes compared to NH Whites.\nOther Hispanic Estimate: 0.46469 OR = exp(0.465) ‚âà 1.59 ~59% higher odds compared to NH Whites.\nNon-Hispanic Black Estimate: 0.51221 OR = exp(0.512) ‚âà 1.67 ~67% higher odds compared to NH Whites.\nOther/Multi-racial Estimate: 0.84460 OR = exp(0.845) ‚âà 2.33 More than twice the odds compared to NH Whites.\n\nOverall Conclusion\n\nAge and BMI are very strong predictors of diabetes. Females are significantly less likely to have diabetes than males. Several racial/ethnic groups (Mexican American, NH Black, Other/Multi) have significantly higher odds than Non-Hispanic Whites.\nAll associations remain after adjusting for other variables."
  },
  {
    "objectID": "index.html#bayesian-logistic-regression-1",
    "href": "index.html#bayesian-logistic-regression-1",
    "title": "Bayesian Logistic Regression - Application in Probability Prediction of disease (Diabetes)",
    "section": "Bayesian Logistic Regression",
    "text": "Bayesian Logistic Regression\nBayesian models assume complete data. Multivariate Imputation by Chained Equations (MICE) provides multiple realistic versions of missing data. Pooling across posterior distributions yields the correct full posterior:\nPosterior (parameters + missing data uncertainty)\n\nMultivariate Imputation by Chained Equations (Pooled Logistic Regression)\nMultiple Imputation (MICE) was applied to impute missing predictors using robust chained‚Äêequation models. Five multiply imputed datasets were created, each representing a plausible version of the complete data.\n\nMICE Buuren and Groothuis-Oudshoorn (2011), manages missiging data\nFlatness of the density, heavy tails, non-zero peakedness, skewness and multimodality do not hamper performance of multiple imputation for the mean structure (n &gt; 400) even for high percentages (75%) of missing data in one variable Van Buuren and Van Buuren (2012).\nMultiple Imputation (MI) in R package, imputes missing values of one variable at a time, using regression models based on the other variables in the dataset.\nIn the chain process, each imputed variable become a predictor for the subsequent imputation, and the entire process is repeated multiple times to create several complete datasets, each reflecting different possibilities for the missing data.\n\n\n\nCode\n# ----- Multiple Imputation (predictors only) \nmi_dat &lt;- adult %&gt;%\n  dplyr::select(diabetes_dx, age, bmi, sex, race, WTMEC2YR, SDMVPSU, SDMVSTRA)\n\nmeth &lt;- mice::make.method(mi_dat)\npred &lt;- mice::make.predictorMatrix(mi_dat)\n\n# Do not impute outcome\nmeth[\"diabetes_dx\"] &lt;- \"\"\npred[\"diabetes_dx\", ] &lt;- 0\npred[,\"diabetes_dx\"] &lt;- 1\n\n# Imputation methods\nmeth[\"age\"]  &lt;- \"norm\"\nmeth[\"bmi\"]  &lt;- \"pmm\"\nmeth[\"sex\"]  &lt;- \"polyreg\"\nmeth[\"race\"] &lt;- \"polyreg\"\n\n# Survey design vars as auxiliaries only\nmeth[c(\"WTMEC2YR\",\"SDMVPSU\",\"SDMVSTRA\")] &lt;- \"\"\npred[, c(\"WTMEC2YR\",\"SDMVPSU\",\"SDMVSTRA\")] &lt;- 1\n\nimp &lt;- mice::mice(mi_dat, m = 5, method = meth, predictorMatrix = pred, seed = 123)\n\n\n\n iter imp variable\n  1   1  bmi\n  1   2  bmi\n  1   3  bmi\n  1   4  bmi\n  1   5  bmi\n  2   1  bmi\n  2   2  bmi\n  2   3  bmi\n  2   4  bmi\n  2   5  bmi\n  3   1  bmi\n  3   2  bmi\n  3   3  bmi\n  3   4  bmi\n  3   5  bmi\n  4   1  bmi\n  4   2  bmi\n  4   3  bmi\n  4   4  bmi\n  4   5  bmi\n  5   1  bmi\n  5   2  bmi\n  5   3  bmi\n  5   4  bmi\n  5   5  bmi\n\n\nImputation was performed on the survey weighted adult data (obs = 5769)\n\nBMI was imputed using PMM\nm = 5 multiply imputed datasets\nOutcome variable is not impute the outcome\nAfter imputation, we run the model in each dataset and get the pool of all the estimates.\nFinal pooled imputed dataset consisted of 5592 obs\nThe iterative process showed stable convergence, indicating reliable estimation of missing BMI values for subsequent Bayesian modeling analyses\n\n\n\nCode\nglimpse(adult_imp1)\n\n\nRows: 5,592\nColumns: 11\n$ diabetes_dx &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ age         &lt;dbl&gt; 69, 54, 72, 73, 56, 61, 42, 56, 65, 26, 76, 33, 32, 38, 50‚Ä¶\n$ bmi         &lt;dbl&gt; 26.7, 28.6, 28.9, 19.7, 41.7, 35.7, 23.6, 26.5, 22.0, 20.3‚Ä¶\n$ sex         &lt;fct&gt; Male, Male, Male, Female, Male, Female, Male, Female, Male‚Ä¶\n$ race        &lt;fct&gt; NH Black, NH White, NH White, NH White, Mexican American, ‚Ä¶\n$ WTMEC2YR    &lt;dbl&gt; 13481.04, 24471.77, 57193.29, 65541.87, 25344.99, 61758.65‚Ä¶\n$ SDMVPSU     &lt;dbl&gt; 1, 1, 1, 2, 1, 1, 2, 1, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 2, 2‚Ä¶\n$ SDMVSTRA    &lt;dbl&gt; 112, 108, 109, 116, 111, 114, 106, 112, 112, 113, 116, 114‚Ä¶\n$ age_c       &lt;dbl&gt; 1.13241831, 0.27835981, 1.30323001, 1.36016725, 0.39223428‚Ä¶\n$ bmi_c       &lt;dbl&gt; -0.33319172, -0.06755778, -0.02561558, -1.31184309, 1.7639‚Ä¶\n$ wt_norm     &lt;dbl&gt; 0.3393916, 0.6160884, 1.4398681, 1.6500477, 0.6380722, 1.5‚Ä¶\n\n\nStudy population across the three datasets\n\nRows: 10175 and Columns: 10 (survey-weighted, merged data)\nRows: 5,769 and Columns: 12 (filtered Adult data)\nRows: 5,592 and Columns: 11 (imputed data, adult_imp1)\n\n\n\nBayesian Logistic Regression:\nA Bayesian logistic regression model was then fitted to each imputed dataset\n\nwith survey weights-Normalized MEC exam weights (wt_norm) and mean 1.00 (SD 0.79)\nwith no missing values\nwith continuous variables standardized\ncategorical variables re-leveled for reference categoriesand posterior samples were combined across imputations.\n\nPosterior (parameters + missing data uncertainty)\nThis procedure appropriately propagates uncertainty from both sampling variability and missing data, producing valid Bayesian posterior estimates.\nPrior Specification\n\nIntercept prior: student_t(3, 0, 10) ‚Äî allowing heavy tails for flexibility in the intercept estimate R. V. D. Schoot et al. (2013)\nRegression coefficients prior: normal(0, 2.5)\nWeakly informative regularization provide gentle regularization, constraining extreme values without overpowering the data R. van de Schoot et al. (2021)\n\nModel Formula\ndiabetes_dx | weights(wt_norm) ~ age_c + bmi_c + sex + race\nFamily: bernoulli Links:mu = logit Data: adult_imp1 (Number of observations: 5592) Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; total post-warmup draws = 4000 Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS and Tail_ESS are effective sample size measures, and Rhat is the potential scale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nCode\nlibrary(gt)\n\npriors &lt;- c(\n  set_prior(\"normal(0, 2.5)\", class = \"b\"),\n  set_prior(\"student_t(3, 0, 10)\", class = \"Intercept\") \n)\n\nbayes_fit &lt;- brm(\n  formula = diabetes_dx | weights(wt_norm) ~ age_c + bmi_c + sex + race,\n  data    = adult_imp1,\n  family  = bernoulli(link = \"logit\"),\n  prior   = priors,\n  chains  = 4, iter = 2000, seed = 123,\n  control = list(adapt_delta = 0.95),\n  refresh = 0   # quiet Stan output\n)\n\n\nRunning /opt/R/4.4.2/lib/R/bin/R CMD SHLIB foo.c\nusing C compiler: ‚Äògcc (GCC) 11.5.0 20240719 (Red Hat 11.5.0-2)‚Äô\ngcc -I\"/opt/R/4.4.2/lib/R/include\" -DNDEBUG   -I\"/opt/R/4.4.2/lib/R/library/Rcpp/include/\"  -I\"/opt/R/4.4.2/lib/R/library/RcppEigen/include/\"  -I\"/opt/R/4.4.2/lib/R/library/RcppEigen/include/unsupported\"  -I\"/opt/R/4.4.2/lib/R/library/BH/include\" -I\"/opt/R/4.4.2/lib/R/library/StanHeaders/include/src/\"  -I\"/opt/R/4.4.2/lib/R/library/StanHeaders/include/\"  -I\"/opt/R/4.4.2/lib/R/library/RcppParallel/include/\"  -I\"/opt/R/4.4.2/lib/R/library/rstan/include\" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DUSE_STANC3 -DSTRICT_R_HEADERS  -DBOOST_PHOENIX_NO_VARIADIC_EXPRESSION  -D_HAS_AUTO_PTR_ETC=0  -include '/opt/R/4.4.2/lib/R/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp'  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/usr/local/include    -fpic  -g -O2  -c foo.c -o foo.o\nIn file included from /opt/R/4.4.2/lib/R/library/RcppEigen/include/Eigen/Core:19,\n                 from /opt/R/4.4.2/lib/R/library/RcppEigen/include/Eigen/Dense:1,\n                 from /opt/R/4.4.2/lib/R/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22,\n                 from &lt;command-line&gt;:\n/opt/R/4.4.2/lib/R/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:679:10: fatal error: cmath: No such file or directory\n  679 | #include &lt;cmath&gt;\n      |          ^~~~~~~\ncompilation terminated.\nmake: *** [/opt/R/4.4.2/lib/R/etc/Makeconf:195: foo.o] Error 1\n\n\nCode\nprior_summary(bayes_fit)\n\n\n               prior     class                coef group resp dpar nlpar lb ub\n      normal(0, 2.5)         b                                                \n      normal(0, 2.5)         b               age_c                            \n      normal(0, 2.5)         b               bmi_c                            \n      normal(0, 2.5)         b raceMexicanAmerican                            \n      normal(0, 2.5)         b         raceNHBlack                            \n      normal(0, 2.5)         b     raceOtherDMulti                            \n      normal(0, 2.5)         b   raceOtherHispanic                            \n      normal(0, 2.5)         b           sexFemale                            \n student_t(3, 0, 10) Intercept                                                \n tag       source\n             user\n     (vectorized)\n     (vectorized)\n     (vectorized)\n     (vectorized)\n     (vectorized)\n     (vectorized)\n     (vectorized)\n             user\n\n\nCode\nsummary(bayes_fit)            # Bayesian model summary\n\n\n Family: bernoulli \n  Links: mu = logit \nFormula: diabetes_dx | weights(wt_norm) ~ age_c + bmi_c + sex + race \n   Data: adult_imp1 (Number of observations: 5592) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept              -2.66      0.09    -2.84    -2.50 1.00     4187     3510\nage_c                   1.09      0.06     0.97     1.22 1.00     3012     3098\nbmi_c                   0.63      0.05     0.53     0.72 1.00     3472     3315\nsexFemale              -0.66      0.10    -0.86    -0.46 1.00     4003     3052\nraceMexicanAmerican     0.69      0.18     0.35     1.04 1.00     3526     2843\nraceOtherHispanic       0.43      0.24    -0.07     0.89 1.00     4058     3114\nraceNHBlack             0.54      0.15     0.24     0.82 1.00     3597     3177\nraceOtherDMulti         0.82      0.19     0.45     1.19 1.00     3763     3257\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1)."
  },
  {
    "objectID": "index.html#mcmc-diagnostics",
    "href": "index.html#mcmc-diagnostics",
    "title": "Bayesian Logistic Regression - Application in Probability Prediction of disease (Diabetes)",
    "section": "MCMC Diagnostics",
    "text": "MCMC Diagnostics\n\nMCMC Trace Plots (Right Panels)\n\nFour Markov Chain Monte Carlo (MCMC) chains, each with 2000 iterations (50% warm-up), and an adaptive delta of 0.95 ensured good chain convergence and reduced divergent transitions.\nEach Trace Plots shows 4 MCMC chains (different colors) across 2000 iterations\nThe chains mix well and overlap substantially, without visible trends or drifts ‚Üí indicates good convergence.\nThe parameter values oscillate around stable means with no systematic pattern ‚Üí confirms stationarity.\nCombined with Rhat ‚âà 1 and high ESS from your summary, the trace plots visually validate posterior convergence and independence.\nMCMC Convergence: well-mixed chains without trends indicate convergence and stable posterior estimates.\nPlots of posterior distributions with high uncertainty, narrow distributions indicating precise estimates.\nAll distributions look smooth and unimodal ‚Üí no multimodality, confirming stable posteriors.\nEach histogram represents the distribution of sampled coefficient values after convergence across all MCMC draws\n\nb_raceOtherHispanic: The posterior peaks around 0.4‚Äì0.5, with some spread below 0 and above 1.‚Üí Suggests a modestly positive association with diabetes risk, but some uncertainty (credible interval overlaps 0).\nb_raceNHBlack: Centered around 0.5‚Äì0.6, with a narrower, symmetric shape. ‚Üí Indicates a consistent positive effect‚ÄîNH Black participants have higher odds of diabetes, and uncertainty is low.\nb_raceOtherDMulti: Centered around 0.8‚Äì0.9, with slightly wider spread but entirely above 0. ‚Üí Stronger evidence for increased odds of diabetes among Other/Multi-racial individuals.\n\n\n\n\nSummary stats of the selected parameters from posterior draws\n\n\nCode\n# Posterior ORs (drop intercept, clean labels)\n\nbayes_or &lt;- posterior_summary(bayes_fit, pars = \"^b_\") %&gt;%\n  as.data.frame() %&gt;%\n  tibble::rownames_to_column(\"raw\") %&gt;%\n  dplyr::mutate(\n    term = gsub(\"^b_\", \"\", raw),\n    term = gsub(\"race\", \"race:\", term),\n    term = gsub(\"sex\",  \"sex:\",  term),\n    term = gsub(\"OtherDMulti\", \"Other/Multi\", term),\n    term = gsub(\"OtherHispanic\", \"Other Hispanic\", term),\n    OR   = exp(Estimate),\n    LCL  = exp(Q2.5),\n    UCL  = exp(Q97.5)\n  ) %&gt;%\n  dplyr::select(term, OR, LCL, UCL) %&gt;%\n  dplyr::filter(term != \"Intercept\")\n\nknitr::kable(\n  bayes_or %&gt;%\n    dplyr::mutate(dplyr::across(c(OR,LCL,UCL), ~round(.x, 2))),\n  digits = 2,\n  caption = \"Bayesian posterior odds ratios (95% CrI) ‚Äî reference: NH White (race), Male (sex)\"\n)\n\n\n\nBayesian posterior odds ratios (95% CrI) ‚Äî reference: NH White (race), Male (sex)\n\n\nterm\nOR\nLCL\nUCL\n\n\n\n\nage_c\n2.99\n2.64\n3.37\n\n\nbmi_c\n1.87\n1.71\n2.05\n\n\nsex:Female\n0.52\n0.42\n0.63\n\n\nrace:MexicanAmerican\n2.00\n1.41\n2.84\n\n\nrace:Other Hispanic\n1.54\n0.93\n2.43\n\n\nrace:NHBlack\n1.71\n1.28\n2.27\n\n\nrace:Other/Multi\n2.27\n1.56\n3.28\n\n\n\n\n\nPosterior Draws\n\nUsing 4,000 posterior draws from the Bayesian model (4 chains √ó 1,000 post-warmup draws per chain), the mcmc_areas visualized the posterior distributions of key predictors: age, BMI, sex, and race/ethnicity.\nThe posterior densities show the range and uncertainty of each coefficient and the 95% credible intervals depicted by the shaded areas, highlighting which predictors have strong evidence of association with diabetes.\nReference: Male , Non-Hispanic White. Age and BMI are standardized (per 1 SD).* - represent the central tendency and uncertainty around the model parameters through credible intervals (CrI).\nThe Bayesian logistic regression model identified significant associations between demographic and anthropometric factors and diabetes diagnosis.\n\nBayesian posterior odds ratios (95% CrI) with reference group being: NH White, Male, corresponds to very low probability of diabetes (~7%).\nAge a strong predictor: For each 1 SD increase in age, the odds of diabetes are about 3 times higher, with high certainty (credible interval well above 0) and higher odds of diabetes (OR = 2.99; 95% CrI = 2.64‚Äì3.37).\nBMI showed a strong positive association (OR = 1.87; 95% CrI = 1.71‚Äì2.05), higher body mass substantially increased diabetes risk.\nFemale sex had lower odds of diabetes compared to males (OR = 0.52; 95% CrI = 0.42‚Äì0.63).\nCompared with Non-Hispanic Whites (reference group), several racial/ethnic groups had higher odds:\nMexican Americans (OR = 2.00; 95% CrI = 1.41‚Äì2.84)\nNon-Hispanic Blacks (OR = 1.71; 95% CrI = 1.28‚Äì2.27)\nOther/Multi-racial individuals (OR = 2.27; 95% CrI = 1.56‚Äì3.28)\nOther Hispanics showed a positive but non-significant association (OR = 1.54; 95% CrI = 0.93‚Äì2.43).\n\n\nBelow is the Summary of all parameters\n\n\nCode\nlibrary(brms)\nlibrary(dplyr)\nlibrary(tibble)\n\n# Extract posterior draws\npost &lt;- posterior_summary(bayes_fit, robust = FALSE) %&gt;% \n  as_tibble(rownames = \"term\")\n\n# Extract convergence diagnostics\ndiag &lt;- rstan::monitor(as.array(bayes_fit$fit), print = FALSE) %&gt;%\n  as_tibble(rownames = \"term\") %&gt;%\n  select(term, Rhat, n_eff = Bulk_ESS)\n\n# Combine summaries + diagnostics\nbayes_results &lt;- post %&gt;%\n  left_join(diag, by = \"term\") %&gt;%\n  mutate(\n    OR       = exp(Estimate),\n    OR_LCL   = exp(Q2.5),\n    OR_UCL   = exp(Q97.5)\n  ) %&gt;%\n  select(\n    term,\n    Estimate,\n    Est.Error,\n    Q2.5,\n    Q97.5,\n    Rhat,\n    n_eff,\n    OR,\n    OR_LCL,\n    OR_UCL\n  )\n\nbayes_results\n\n\n# A tibble: 11 √ó 10\n   term         Estimate Est.Error     Q2.5    Q97.5  Rhat n_eff      OR  OR_LCL\n   &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 b_Intercept  -2.66e+0    0.0884 -2.84e+0 -2.50e+0 1.00   2141 6.96e-2 5.84e-2\n 2 b_age_c       1.09e+0    0.0622  9.72e-1  1.22e+0 1.00   1575 2.99e+0 2.64e+0\n 3 b_bmi_c       6.27e-1    0.0476  5.34e-1  7.20e-1 1.000  1870 1.87e+0 1.71e+0\n 4 b_sexFemale  -6.59e-1    0.102  -8.59e-1 -4.58e-1 1.01   2143 5.18e-1 4.23e-1\n 5 b_raceMexic‚Ä¶  6.92e-1    0.177   3.47e-1  1.04e+0 1.00   1741 2.00e+0 1.41e+0\n 6 b_raceOther‚Ä¶  4.31e-1    0.244  -7.16e-2  8.87e-1 1.01   2147 1.54e+0 9.31e-1\n 7 b_raceNHBla‚Ä¶  5.38e-1    0.147   2.43e-1  8.18e-1 1.00   1685 1.71e+0 1.28e+0\n 8 b_raceOther‚Ä¶  8.19e-1    0.189   4.45e-1  1.19e+0 1.00   1915 2.27e+0 1.56e+0\n 9 Intercept    -2.67e+0    0.0678 -2.81e+0 -2.55e+0 1.00   1525 6.90e-2 6.03e-2\n10 lprior       -1.65e+1    0.0531 -1.66e+1 -1.64e+1 1.00   1304 6.81e-8 6.07e-8\n11 lp__         -1.43e+3    2.04   -1.44e+3 -1.43e+3 1.00    865 0       0      \n# ‚Ñπ 1 more variable: OR_UCL &lt;dbl&gt;\n\n\nCode\nsummary(bayes_fit)\n\n\n Family: bernoulli \n  Links: mu = logit \nFormula: diabetes_dx | weights(wt_norm) ~ age_c + bmi_c + sex + race \n   Data: adult_imp1 (Number of observations: 5592) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept              -2.66      0.09    -2.84    -2.50 1.00     4187     3510\nage_c                   1.09      0.06     0.97     1.22 1.00     3012     3098\nbmi_c                   0.63      0.05     0.53     0.72 1.00     3472     3315\nsexFemale              -0.66      0.10    -0.86    -0.46 1.00     4003     3052\nraceMexicanAmerican     0.69      0.18     0.35     1.04 1.00     3526     2843\nraceOtherHispanic       0.43      0.24    -0.07     0.89 1.00     4058     3114\nraceNHBlack             0.54      0.15     0.24     0.82 1.00     3597     3177\nraceOtherDMulti         0.82      0.19     0.45     1.19 1.00     3763     3257\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\nCode\nlibrary(brms)\n\nplot(bayes_fit)   # Posterior distributions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPosterior Coefficients (log-odds scale)\nIntercept: b_Intercept = ‚Äì2.66 (SD = 0.088, LCL ‚âà ‚Äì2.84)\n\nBaseline log-odds of diabetes for: male, NH White, average age, average BMI with ~ 6.5% baseline diabetes probability.\n\nAge (per 1 SD): b_age_c = 1.094 (SD = 0.062, LCL ‚âà 0.97)\n\nVery strong positive effect.\nInterpreting as odds ratio: OR ‚âà exp(1.094) ‚âà 2.99 A 1-SD increase in age almost triples the odds of diabetes.\n95% credible interval excludes 0 ‚Üí strong evidence of association.\n\nBMI (per 1 SD): b_bmi_c = 0.627 (SD = 0.048, LCL ‚âà 0.53)\n\nAlso strong positive effect.\nOR ‚âà exp(0.627) ‚âà 1.87\nA 1-SD BMI increase raises the odds of diabetes by ~87%.\nVery tight credible interval ‚Üí high certainty.\n\nSex (Female) b_sexFemale = ‚Äì0.659 (SD = 0.102, LCL ‚âà ‚Äì0.859)\n\nWomen have lower odds of diabetes than men (Reference = Male)\nOR ‚âà exp(‚Äì0.659) ‚âà 0.52\nFemales have ~48% lower odds compared with males, holding age, BMI, race constant.\n\nRace\nMexican American: b_raceMexicanAmerican = 0.692 (SD = 0.177, LCL ‚âà 0.347)\n\nIncreased odds of diabetes.\nOR ‚âà exp(0.692) ‚âà 1.99\nNearly double the odds compared to NH Whites.\n\nOther Hispanic: b_raceOtherHispanic = 0.431 (SD = 0.244, LCL ‚âà ‚Äì0.072)\n\nPositive mean, but lower bound crosses 0.\nOR ‚âà exp(0.431) ‚âà 1.54\nEvidence is weaker that ‚ÄúOther Hispanic‚Äù have higher odds than NH Whites.\n\nNH Black: b_raceNHBlack = 0.538 (SD = 0.147, LCL ‚âà 0.243)\n\nStrong positive association.\nOR ‚âà exp(0.538) ‚âà 1.71\nNH Black individuals have ~70% higher odds vs NH Whites.\n\nOther / Multiracial: b_raceOtherDMulti = 0.819 (SD = 0.189, LCL ‚âà 0.445)\n\nStrong evidence of increased risk.\nOR ‚âà exp(0.819) ‚âà 2.27\nOver twice the odds compared to NH Whites\n\nInterpretation\n\nAge and BMI showed positive associations\nBMI showed a negative association in most draws, with posterior estimates ranging roughly from 0.61 to 0.70, indicating that higher BMI is associated with lower odds of the outcome in this analysis\nAge showed a positive association, with posterior estimates ranging roughly from 1.00 to 1.14, suggesting that higher age increases the odds of the outcome.\nFemale sex showed a negative association\nRacial/ethnic groups had elevated odds relative to the reference group"
  },
  {
    "objectID": "index.html#prior-vs-posterior-examination",
    "href": "index.html#prior-vs-posterior-examination",
    "title": "Bayesian Logistic Regression - Application in Probability Prediction of disease (Diabetes)",
    "section": "Prior vs Posterior Examination",
    "text": "Prior vs Posterior Examination\n\n\nCode\nlibrary(brms)\nlibrary(dplyr)\nlibrary(posterior)\nlibrary(bayesplot)\nlibrary(ggplot2)\n\n\npost_wide &lt;- as_draws_df(bayes_fit)  # or posterior_samples(bayes_fit)\nnames(post_wide)  # should show: b_Intercept, b_bmi_c, b_age_c, etc.\n\n\n [1] \"b_Intercept\"           \"b_age_c\"               \"b_bmi_c\"              \n [4] \"b_sexFemale\"           \"b_raceMexicanAmerican\" \"b_raceOtherHispanic\"  \n [7] \"b_raceNHBlack\"         \"b_raceOtherDMulti\"     \"Intercept\"            \n[10] \"lprior\"                \"lp__\"                  \".chain\"               \n[13] \".iteration\"            \".draw\"                \n\n\nCode\nmcmc_areas(\n  post_wide,\n  pars = c(\"b_age_c\",\"b_bmi_c\",\"b_sexFemale\",\n           \"b_raceMexicanAmerican\",\"b_raceOtherHispanic\",\n           \"b_raceNHBlack\",\"b_raceOtherDMulti\")\n)\n\n\n\n\n\n\n\n\n\n\nThe posterior distributions represents density curve with the uncertainty around the parameter‚Äôs posterior mean for age and BMI\nAge and BM: strong positive associations with diabetes status, as their posterior distributions are concentrated above zero, suggesting higher values increase the likelihood of diabetes.\nSex (Female) has a distribution centered slightly below zero, indicating a lower probability of diabetes compared to males.\nRace categories (Mexican American, Other Hispanic, Non-Hispanic Black, and Other/Multi) show broader distributions with varying levels of uncertainty relative to the reference group (Non-Hispanic White).\nThe shaded regions indicate the 80% credible intervalswithin which the true parameter values are most likely to lie based on the posterior samples."
  },
  {
    "objectID": "index.html#posterior-predictive-checking-ppc",
    "href": "index.html#posterior-predictive-checking-ppc",
    "title": "Bayesian Logistic Regression - Application in Probability Prediction of disease (Diabetes)",
    "section": "Posterior Predictive Checking (PPC)",
    "text": "Posterior Predictive Checking (PPC)\n\n\nCode\nlibrary(posterior)\nlibrary(dplyr)\nlibrary(tidyr)\n\n# Extract posterior draws as a matrix, then convert to tibble\npost &lt;- as_draws_matrix(bayes_fit) %&gt;%   # safer than as_draws_df for manipulation\n  as.data.frame() %&gt;%\n  select(b_bmi_c, b_age_c) %&gt;%\n  pivot_longer(\n    everything(),\n    names_to = \"term\",\n    values_to = \"estimate\"\n  ) %&gt;%\n  mutate(\n    term = case_when(\n      term == \"b_bmi_c\" ~ \"BMI (per 1 SD)\",\n      term == \"b_age_c\" ~ \"Age (per 1 SD)\"\n    ),\n    type = \"Posterior\"\n  )\nprior_draws &lt;- tibble(\n  term = rep(c(\"BMI (per 1 SD)\", \"Age (per 1 SD)\"), each = 4000),\n  estimate = c(rnorm(4000, 0, 1), rnorm(4000, 0, 1)),\n  type = \"Prior\"\n)\ncombined_draws &lt;- bind_rows(prior_draws, post)\n\n\n\n\nCode\nlibrary(ggplot2)\n\nggplot(combined_draws, aes(x = estimate, fill = type)) +\n  geom_density(alpha = 0.4) +\n  facet_wrap(~ term, scales = \"free\", ncol = 2) +\n  theme_minimal(base_size = 13) +\n  labs(\n    title = \"Prior vs Posterior Distributions\",\n    x = \"Coefficient estimate\",\n    y = \"Density\",\n    fill = \"\"\n  )\n\n\n\n\n\n\n\n\n\n\nPrior vs Posterior Distributions :\n\nDensity plots show an overlay of the prior and posterior distributions for BMI and age\nPosterior distributions were narrower and shifted away from zero relative to the priors, indicating that the data provided substantial evidence for positive associations of BMI and age with diabetes risk.\nThe shift from diffuse priors to concentrated posteriors demonstrates the model‚Äôs ability to incorporate empirical evidence and refine prior beliefs.\nNarrower posterior distributions reflect reduced uncertainty in parameter estimates.\n\n\n\nPosterior Predictive Checks (PPC)\nWe generates 500 predicted outcomes for each observation using samples from the posterior distribution (ndraws = 500) to simulate predicted data to compare these simulated outcomes with the actual observed data to see if the model can reasonably reproduce the patterns in the data\n\nPredicted outcomes: binary outcome with 0 or 1 simulated\nPresented below are the rows (500) and columns = number of observations (5592)\n\n\n\nCode\n# Posterior predictive draws\n\n#Posterior predictive checks (binary outcome)\npp_samples &lt;- posterior_predict(bayes_fit, ndraws = 500)  # 500 draws\n\n# Check dimensions\ndim(pp_samples)  # rows = draws, cols = observations\n\n\n[1]  500 5592\n\n\nCode\nppc_bars(y = adult_imp1$diabetes_dx, yrep = pp_samples[1:500, ])\n\n\n\n\n\n\n\n\n\nppc_bars() plot - plot compared the observed and predicted diabetes outcomes - Model‚Äôs predictions align with reality where mean(y_rep) = average predicted probability of diabetes for each individual, across all posterior draws of the parameters and y = the actual observed diabetes status (0 = non-diabetic, 1 = diabetic).\n\n\nPosterior Predictive stats: Mean\n\n\nCode\n#PP check for proportions (useful for binary) mean comparison to check if the simulated means match the observed mean\n\n## mean\nppc_stat(y = adult_imp1$diabetes_dx, yrep = pp_samples[1:100, ], stat = \"mean\") +\n  labs(title = \"Posterior Predictive Check: Mean of Replicates\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nA posterior predictive check using 100 replicated datasets from the posterior distribution shows the mean diabetes outcome closely aligned with the observed mean, suggesting that the Bayesian model accurately captures the centraltendency of the outcome.\n\n\n\nPosterior Predictive stats: Standard Deviation\n\n\nCode\n#PP check for proportions (useful for binary) mean and sd comparison to check if the simulated means match the observed mean\n\n## sd\nppc_stat(y = adult_imp1$diabetes_dx, yrep = pp_samples[1:100, ], stat = \"sd\") +\n  labs(title = \"PPC: Standard Deviation of Replicates\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nA posterior predictive check on the standard deviation of diabetes outcomes using 100 replicated datasets from the posterior distribution closely matched the observed value, indicating that the Bayesian model adequately captures the variability in the outcome data.\n\n\n\nObserved vs.¬†Predicted Average diabetes outcome across individuals\n\n\nCode\n# PP checks with bayesplot options\ncolor_scheme_set(\"blue\")\nppc_scatter_avg(y = adult_imp1$diabetes_dx, yrep = pp_samples[1:100, ]) +\n  labs(title = \"Observed vs Predicted (Avg) Posterior Predictive at individual level\")\n\n\n\n\n\n\n\n\n\n\nThe plot shows predicted probabilities of observed outcomes, across individuals and checks model calibration Logistic regression fit at person-level.\nEach point = one observation (an individual).\nx-axis = observed value (0 or 1 in your binary diabetes variable).\ny-axis = average predicted posterior probability for that same individual across simulated datasets.\nPoints near (0, 0) or (1, 1) ‚Üí good prediction.\n\n\n\nObserved vs.¬†Predicted estimates for BMI\n\n\nCode\npredicted &lt;- fitted(bayes_fit, summary = TRUE)\nobserved &lt;- adult_imp1[, c(\"bmi\", \"age\")]\n\n# Plot for **bmi** (obs vs pred)\n\nggplot(data = NULL, aes(x = observed$bmi, y = predicted[, \"Estimate\"])) +\n  geom_point() +\n  geom_errorbar(aes(ymin = predicted[, \"Q2.5\"], ymax = predicted[, \"Q97.5\"])) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"red\") +\n  xlab(\"Observed bmi\") + ylab(\"Predicted bmi\")\n\n\n\n\n\n\n\n\n\n\nComparison of observed vs and posterior predicted estimates for BMI\nEach point represents an individual‚Äôs observed BMI versus the model‚Äôs predicted mean.\nError bars indicate the 95% credible intervals of the predictions.\nThe plot demonstrates that the model‚Äôs predictions generally align with the observed data, with most points closely following the diagonal, indicating good predictive performance for BMI."
  },
  {
    "objectID": "index.html#model-level-outcomes",
    "href": "index.html#model-level-outcomes",
    "title": "Bayesian Logistic Regression - Application in Probability Prediction of disease (Diabetes)",
    "section": "Model-Level Outcomes",
    "text": "Model-Level Outcomes\n\n\nCode\n# Results\n\nfmt_or &lt;- function(or, lcl, ucl, digits = 2) {\n  paste0(\n    formatC(or,  format = \"f\", digits = digits), \" (\",\n    formatC(lcl, format = \"f\", digits = digits), \"‚Äì\",\n    formatC(ucl, format = \"f\", digits = digits), \")\"\n  )\n}\n\n\nsvy_tbl   &lt;- svy_or   %&gt;% mutate(Model = \"Survey-weighted MLE\")\nbayes_tbl &lt;- bayes_or %&gt;% mutate(Model = \"Bayesian\")\n\n# Remove mi_tbl from bind_rows\nall_tbl &lt;- bind_rows(svy_tbl, bayes_tbl) %&gt;% \n  mutate(term = case_when(\n    str_detect(term, \"bmi_c|\\\\bBMI\\\\b\") ~ \"BMI (per 1 SD)\",\n    str_detect(term, \"age_c|\\\\bAge\\\\b\") ~ \"Age (per 1 SD)\",\n    TRUE ~ term\n  )) %&gt;%\n  filter(term %in% c(\"BMI (per 1 SD)\", \"Age (per 1 SD)\")) %&gt;%\n  mutate(OR_CI = fmt_or(OR, LCL, UCL, digits = 2)) %&gt;%\n  select(Model, term, OR_CI) %&gt;%\n  arrange(\n    factor(Model, levels = c(\"Survey-weighted MLE\",\"Bayesian Regression\")),\n    factor(term,  levels = c(\"BMI (per 1 SD)\",\"Age (per 1 SD)\"))\n  )\n\nres_wide &lt;- all_tbl %&gt;%\n  pivot_wider(names_from = term, values_from = OR_CI) %&gt;%\n  rename(\n    `BMI (per 1 SD) OR (95% CI)` = `BMI (per 1 SD)`,\n    `Age (per 1 SD) OR (95% CI)` = `Age (per 1 SD)`\n  )\n\nkable(\n  res_wide,\n  align = c(\"l\",\"c\",\"c\"),\n  caption = \"Odds ratios (per 1 SD) with 95% CIs across models\"\n)\n\n\n\nOdds ratios (per 1 SD) with 95% CIs across models\n\n\n\n\n\n\n\nModel\nBMI (per 1 SD) OR (95% CI)\nAge (per 1 SD) OR (95% CI)\n\n\n\n\nSurvey-weighted MLE\n1.89 (1.65‚Äì2.15)\n3.03 (2.70‚Äì3.40)\n\n\nBayesian\n1.87 (1.71‚Äì2.05)\n2.99 (2.64‚Äì3.37)\n\n\n\n\n\n\n\nCode\n# Compute proportion of diabetes=1 for each draw\npp_proportion &lt;- rowMeans(pp_samples)  # proportion of 1's in each posterior draw\n\n# Optional: visualize the posterior probability distribution\npp_proportion_df &lt;- tibble(proportion = pp_proportion)\n\nggplot(pp_proportion_df, aes(x = proportion)) +\n  geom_histogram(binwidth = 0.01, fill = \"skyblue\", color = \"black\") +\n  labs(\n    title = \"Posterior Distribution of Proportion of Diabetes = 1\",\n    x = \"Proportion of Diabetes = 1\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nModel Comparison Across methods\n\nSurvey-weighted maximum likelihood estimation (MLE)\nBayesian regression\nFor BMI\n\nodds ratios ranged from 1.89 (95% CI: 1.65‚Äì2.15) in survey-weighted model\nOdds rato of 1.87 (95% CrI: 1.71‚Äì2.05) in the Bayesian model\n\nFor age\n\nOdds ration 3.03 (95% CI: 2.70‚Äì3.40) from the survey-weighted MLE model\nOdds ratio 2.99 (95% CrI: 2.64‚Äì3.37) in the Bayesian analysis\n\n\n\n\nPosterior Distribution of Diabetes Prevalence\n\nA histogram of these values shows the distribution of predicted prevalence of diabetes calculated for each draw of the Bayesian model\nReflecting uncertainty in the model estimates, central tendency and variability of diabetes prevalence\nMost posterior predictions cluster around 10‚Äì11%, indicating good alignment with the observed/imputed data and demonstrating that the model captures the underlying population pattern.\n\n\n\nCode\nlibrary(dplyr)\nlibrary(knitr)\nsvy_mean &lt;- svymean(~diabetes_dx, nhanes_design_adult, na.rm = TRUE)\n\n# Create summary table WITHOUT MICE row\nsummary_table &lt;- tibble(\n  Method = c(\"Survey-weighted mean (NHANES)\", \n             \"Posterior predictive mean\"),\n  diabetes_mean = c(\n    coef(svy_mean),        # survey-weighted mean\n    mean(pp_proportion)    # posterior predictive mean\n  ),\n  SE = c(\n    SE(svy_mean),          # survey-weighted SE\n    NA                     # no SE for posterior predictive\n  )\n)\n\n# Render table\nkable(summary_table, digits = 4, caption = \"Comparison of Diabetes Prevalence Across Methods\")\n\n\n\nComparison of Diabetes Prevalence Across Methods\n\n\nMethod\ndiabetes_mean\nSE\n\n\n\n\nSurvey-weighted mean (NHANES)\n0.0890\n0.0048\n\n\nPosterior predictive mean\n0.1091\nNA\n\n\n\n\n\n\n\nPosterior Predicted Diabetes Proportion vs.¬†NHANES Prevalence\n\n\nCode\nlibrary(tidyverse)\n\n# Posterior predicted proportion vector\n# pp_proportion &lt;- rowMeans(pp_samples)  # if not already done\n\nknown_prev &lt;- 0.089   # NHANES prevalence\n\n# Posterior summary\nposterior_mean &lt;- mean(pp_proportion)\nposterior_ci &lt;- quantile(pp_proportion, c(0.025, 0.975))  # 95% credible interval\n\n# Create a data frame for plotting\npp_df &lt;- tibble(proportion = pp_proportion)\n\n# Plot\nggplot(pp_df, aes(x = proportion)) +\n  geom_histogram(binwidth = 0.005, fill = \"skyblue\", color = \"black\") +\n  geom_vline(xintercept = known_prev, color = \"red\", linetype = \"dashed\", size = 1) +\n  geom_vline(xintercept = posterior_mean, color = \"blue\", linetype = \"solid\", size = 1) +\n  labs(\n    title = \"Posterior Predicted Diabetes Proportion vs NHANES Prevalence\",\n    subtitle = paste0(\"Red dashed = NHANES prevalence (\", known_prev, \n                      \"), Blue solid = Posterior mean (\", round(posterior_mean,3), \")\"),\n    x = \"Proportion of Diabetes = 1\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nWe compared the Bayesian posterior-predicted diabetes prevalence with the NHANES survey-weighted estimate (8.9%, SE = 0.0048).\nThe model‚Äôs posterior mean was 10.95%, with a 95% credible interval of 8.5%‚Äì12.8%. Although the model predicts slightly higher prevalence, the credible interval overlaps the NHANES estimate.\nThis indicates that the model is reasonably well-calibrated, with NHANES falling near the lower end of the posterior distribution but still within a plausible range.\n\n\n\nCode\nlibrary(dplyr)\n\n# Posterior predicted proportion\n\nposterior_mean &lt;- mean(pp_proportion)\nposterior_ci &lt;- quantile(pp_proportion, c(0.025, 0.975))  # 95% credible interval\n\n# NHANES prevalence with SE from survey::svymean\n# Suppose you already have:\n# svymean(~diabetes_dx, nhanes_design_adult, na.rm = TRUE)\nknown_prev &lt;- 0.089        # Mean prevalence\nknown_se   &lt;- 0.0048       # Standard error from survey\n\n# Calculate 95% confidence interval\nknown_ci &lt;- c(\n  known_prev - 1.96 * known_se,\n  known_prev + 1.96 * known_se\n)\n\n# Print results\ndata.frame(\n  Type = c(\"Posterior Prediction\", \"NHANES Prevalence\"),\n  Mean = c(posterior_mean, known_prev),\n  Lower_95 = c(posterior_ci[1], known_ci[1]),\n  Upper_95 = c(posterior_ci[2], known_ci[2])\n)\n\n\n                     Type      Mean   Lower_95  Upper_95\n2.5% Posterior Prediction 0.1091237 0.09826091 0.1205293\n        NHANES Prevalence 0.0890000 0.07959200 0.0984080\n\n\nCode\n# Create a data frame for plotting\nci_df &lt;- data.frame(\n  Type = c(\"Posterior Prediction\", \"NHANES Prevalence\"),\n  Mean = c(0.1096674, 0.089),\n  Lower_95 = c(0.09772443, 0.079592),\n  Upper_95 = c(0.1210658, 0.098408)\n)\n\n\n# --- 1. Survey-weighted (Population) prevalence ---\npop_est &lt;- svymean(~diabetes_dx, nhanes_design_adult, na.rm = TRUE)\npop_prev &lt;- as.numeric(pop_est)\npop_se &lt;- as.numeric(SE(pop_est))\npop_ci &lt;- c(pop_prev - 1.96 * pop_se, pop_prev + 1.96 * pop_se)\n\n# --- 2. Bayesian posterior prevalence ---\n# bayes_pred = matrix of posterior draws (iterations √ó individuals)\npp_proportion &lt;- rowMeans(pp_samples)             # prevalence per posterior draw\npost_prev &lt;- mean(pp_proportion)                  # posterior mean prevalence\npost_ci &lt;- quantile(pp_proportion, c(0.025, 0.975))  # 95% credible interval\n\n# --- 3. Combine into one data frame ---\nbar_df &lt;- tibble(\n  Source     = c(\"Survey-weighted (Population)\", \"Bayesian Posterior\"),\n  Prevalence = c(pop_prev, post_prev),\n  CI_low     = c(pop_ci[1], post_ci[1]),\n  CI_high    = c(pop_ci[2], post_ci[2])\n)\n\n# --- 4. Plot ---\nggplot(bar_df, aes(x = Source, y = Prevalence, fill = Source)) +\n  geom_col(alpha = 0.85, width = 0.6) +\n  geom_errorbar(\n    aes(ymin = CI_low, ymax = CI_high),\n    width = 0.15,\n    color = \"black\",\n    linewidth = 0.8\n  ) +\n  guides(fill = \"none\") +\n  labs(\n    title = \"Population vs Posterior Diabetes Prevalence\",\n    subtitle = \"Survey-weighted estimate (design-based) vs Bayesian (model-based)\",\n    y = \"Prevalence (Proportion with Diabetes)\",\n    x = NULL\n  ) +\n  theme_minimal(base_size = 13)\n\n\n\n\n\n\n\n\n\n\n\nComparison of Diabetes Prevalence Across Methods\nThe bar plot compares survey-weighted and Bayesian posterior estimates of diabetes prevalence:\n\nSurvey-weighted prevalence: 8.9% (95% CI: 0.080‚Äì0.098)\nBayesian posterior mean: 10.9% (95% CrI: 0.098‚Äì0.121)\nThe posterior credible interval overlaps the survey 95% CI, indicating that the Bayesian model reproduces population-level prevalence accurately.\nThis demonstrates good model calibration and predictive validity, while visualizing the uncertainty of both survey-based and model-based estimates."
  },
  {
    "objectID": "index.html#model-assumptions-for-bayesian-logistic-regression",
    "href": "index.html#model-assumptions-for-bayesian-logistic-regression",
    "title": "Bayesian Logistic Regression - Application in Probability Prediction of disease (Diabetes)",
    "section": "Model Assumptions for Bayesian Logistic Regression",
    "text": "Model Assumptions for Bayesian Logistic Regression\n\nData & Observations\n\n\nThe outcome is binary (0/1).\nObservations are independent, conditional on predictors.\nPredictors are measured without structural errors (no deterministic misclassification).\n\n\nModel Structure\n\n\nThe relationship between predictors and the log-odds of the outcome is linear in the logit scale\nNo perfect multicollinearity among predictors.\nNo complete or quasi-complete separation, which would create infinite log-odds estimates.\n\n\nPriors\n\n\nPriors are proper (integrate to 1)\nPriors are informative enough to regularize estimation‚Äîespecially important for logistic models with sparse data.\n\n\nPosterior Behavior\n\n\nPosterior distribution is proper (i.e., well-defined).\nMCMC shows good convergence:\n\nR^‚âà1 1R^‚âà1\nNo divergent transitions\nEffective sample sizes are adequate\nTrace plots show good mixing\n\n\n\nModel Fit\n\n\nPosterior predictive checks show that the model can reasonably reproduce key features of the observed data.\nPredicted probabilities are well-calibrated.\nResiduals and diagnostics do not reveal major misfit.\n\n\nMCMC Autocorrelation for age and BMI coefficients\n\n\nCode\nlibrary(tidyr)\nlibrary(bayesplot)\nlibrary(posterior)\n\n# Convert fitted model to draws array\npost_array &lt;- as_draws_array(bayes_fit)  # draws x chains x parameters\n\n# Plot autocorrelation for age and bmi\nmcmc_acf(post_array, pars = c(\"b_age_c\", \"b_bmi_c\"))\n\n\n\n\n\n\n\n\n\nAutocorrelation plots are pairwise plots (mcmc_pairs, posterior) explore correlations between parameters (age and BMI) coefficients - assess chain mixing and convergence showing correlation of each draw with its lagged values across iterations. - Rapid decay of autocorrelation toward zero indicates that the Markov chains are mixing well and successive draws are relatively independent. - Both age and BMI coefficients exhibited low autocorrelation after a few lags, supporting the reliability of posterior estimates. - This diagnostic confirms that the Bayesian model sampling was adequate and stable, ensuring valid inference from the posterior distributions. - Autocorrelation within MCMC samples for a single parameter across iterations ‚Äî i.e., how strongly each sample depends on its previous ones. - It reflects MCMC chain mixing and efficiency, not relationships between parameters\n\n\nCorrelation Matrix\n\n\nCode\nlibrary(bayesplot)\n\n# Extract posterior draws\npost &lt;- as_draws_df(bayes_fit)\n\n# Select numeric parameters of interest\npost_subset &lt;- post %&gt;% \n  dplyr::select(b_age_c, b_bmi_c, b_sexFemale, \n                b_raceMexicanAmerican, b_raceNHBlack)\n\n# Compute correlation matrix\ncor_matrix &lt;- cor(post_subset)\n\n# Visualize\nmcmc_pairs(as_draws_array(bayes_fit), \n           pars = c(\"b_age_c\", \"b_bmi_c\", \"b_sexFemale\"),\n           off_diag_args = list(size = 1.5, alpha = 0.4))\n\n\n\n\n\n\n\n\n\nInterpretation of Posterior correlation matrix among parameters (e.g., between b_age_c, b_bmi_c, b_sexFemale)\nEach diagonal panel shows the posterior density (distribution) of one parameter, while the off-diagonal scatterplots show pairwise relationships (correlations) between parameters across posterior draws.\n\nMarginal Distributions (diagonals)\n\n\nThe histograms show that all parameters (b_age_c, b_bmi_c, b_sexFemale) have approximately symmetric and unimodal posterior distributions.\nThis indicates stable and well-identified estimates with no sampling irregularities or multimodality.\nThe narrow spread (small variance) reflects high precision of parameter estimates.\n\n\nJoint Distributions (off-diagonal scatterplots)\n\n\nThe scatterplots show elliptical clouds centered around the mean, with no strong linear patterns. This means low posterior correlation among parameters ‚Äî i.e., b_age_c, b_bmi_c, and b_sexFemale are largely independent in their posterior estimates.\nThe absence of diagonal streaks or skewed clusters suggests that collinearity is minimal and that the MCMC chains successfully explored the parameter space.\n\n\nSubtle Correlation Notes**\n\n\nA mild negative tilt between b_age_c and b_sexFemale indicates a slight negative correlation, meaning as the posterior estimate for age increases, the effect of being female slightly decreases.\nHowever, this pattern is weak ‚Äî confirming that both predictors contribute distinct information to the diabetes outcome.\nThe absence of strong linear relationships among parameters suggests that age, BMI, and sex independently contributed to the prediction of diabetes status.\nThe smooth, unimodal histograms along the diagonals confirmed stable model convergence and well-behaved posterior samples.\n\n\n\nCode\nbayes_R2(bayes_fit)      # Model fit\n\n\n    Estimate  Est.Error      Q2.5    Q97.5\nR2 0.1313342 0.01265055 0.1064607 0.156078\n\n\n\n\nBayesian ùëÖ^2 (model fit statics):\nModel Fit to quantify predictive performance. - Explains about 13% of the variability in diabetes status, with credible uncertainty bounds suggesting reasonable but modest explanatory power. - but other factors (e.g., genetics, lifestyle, environment) also contribute to outcome variability.\n\n\nComparison of Odds Ratios Across Models\n\n\nCode\n# Combine results from remaining models (Survey-weighted and Bayesian)\nsvy_tbl &lt;- if (exists(\"svy_or\") && nrow(svy_or) &gt; 0)\n  mutate(svy_or, Model = \"Survey-weighted (MLE)\") else NULL\n\nbayes_tbl &lt;- if (exists(\"bayes_or\") && nrow(bayes_or) &gt; 0)\n  mutate(bayes_or, Model = \"Bayesian\") %&gt;%\n  filter(term != \"Intercept\") else NULL\n\n# Long format\nall_tbl &lt;- bind_rows(Filter(Negate(is.null), list(svy_tbl, bayes_tbl))) %&gt;%\n  mutate(\n    term = case_when(\n      grepl(\"bmi\", term, ignore.case = TRUE) ~ \"BMI (per 1 SD)\",\n      grepl(\"age\", term, ignore.case = TRUE) ~ \"Age (per 1 SD)\",\n      grepl(\"^sexFemale$\", term)             ~ \"Female (vs. Male)\",\n      grepl(\"^sexMale$\", term)               ~ \"Male (vs. Female)\",\n      grepl(\"^raceHispanic$\", term)          ~ \"Hispanic (vs. White)\",\n      grepl(\"^raceBlack$\", term)             ~ \"Black (vs. White)\",\n      grepl(\"^raceOther$\", term)             ~ \"Other (vs. White)\",\n      TRUE ~ term\n    ),\n    OR_CI = sprintf(\"%.2f (%.2f ‚Äì %.2f)\", OR, LCL, UCL)\n  ) %&gt;%\n  select(Model, term, OR_CI)\n\n# ‚ûú WIDE TABLE: Predictors as rows, Models as columns\nvertical_tbl &lt;- all_tbl %&gt;%\n  pivot_wider(\n    names_from = Model,\n    values_from = OR_CI\n  ) %&gt;%\n  arrange(term)\n\n# Print as nice table\nkable(vertical_tbl,\n      align = \"lcc\",\n      caption = \"Odds Ratios (95% CI) Across Models\")\n\n\n\nOdds Ratios (95% CI) Across Models\n\n\nterm\nSurvey-weighted (MLE)\nBayesian\n\n\n\n\nAge (per 1 SD)\n3.03 (2.70 ‚Äì 3.40)\n2.99 (2.64 ‚Äì 3.37)\n\n\nBMI (per 1 SD)\n1.89 (1.65 ‚Äì 2.15)\n1.87 (1.71 ‚Äì 2.05)\n\n\nFemale (vs.¬†Male)\n0.53 (0.41 ‚Äì 0.68)\nNA\n\n\nrace:MexicanAmerican\nNA\n2.00 (1.41 ‚Äì 2.84)\n\n\nrace:NHBlack\nNA\n1.71 (1.28 ‚Äì 2.27)\n\n\nrace:Other Hispanic\nNA\n1.54 (0.93 ‚Äì 2.43)\n\n\nrace:Other/Multi\nNA\n2.27 (1.56 ‚Äì 3.28)\n\n\nraceMexican American\n2.04 (1.49 ‚Äì 2.79)\nNA\n\n\nraceNH Black\n1.67 (1.16 ‚Äì 2.40)\nNA\n\n\nraceOther Hispanic\n1.59 (1.17 ‚Äì 2.17)\nNA\n\n\nraceOther/Multi\n2.33 (1.55 ‚Äì 3.50)\nNA\n\n\nsex:Female\nNA\n0.52 (0.42 ‚Äì 0.63)"
  },
  {
    "objectID": "index.html#internal-validation",
    "href": "index.html#internal-validation",
    "title": "Bayesian Logistic Regression - Application in Probability Prediction of disease (Diabetes)",
    "section": "Internal validation",
    "text": "Internal validation\n\nTo illustrate personalized risk estimation using the Bayesian model, we computed the posterior predicted probability of diabetes for a representative participant.\nWe selected one participant from the dataset (adult[1, ]) including all relevant covariates (age, BMI, sex, race).\nUsed posterior_linpred with transform = TRUE to obtain predicted probabilities for logistic regression.\nExtracted posterior draws computed 95% credible interval from the posterior draws.\nDensity plot shows the distribution of plausible probabilities given the participant‚Äôs covariates.\nThe density highlights uncertainty around the individual‚Äôs predicted diabetes risk.\n95% credible interval provides a range of probable outcomes, not just a point estimate.\nThis approach allows personalized risk assessment, enabling clinicians or public health practitioners to identify high-risk individuals\nTailor preventive interventions (e.g., lifestyle modification, monitoring)\nQuantify uncertainty in predictions for decision-making\nPosterior predictive distributions enable probabilistic, individualized predictions, supporting targeted intervention strategies beyond population-level summaries.\n\n\n\nCode\n# Use the first participant \n# using multiple covariates to select someone\nparticipant1_data  &lt;- adult[1, ]\n\n\n# predicted probabilities for patient 1\nphat1 &lt;- posterior_linpred(bayes_fit, newdata = participant1_data, transform = TRUE)\n# 'transform = TRUE' gives probabilities for logistic regression\n\n# Store in a data frame for plotting\npost_pred_df &lt;- data.frame(pred = phat1)\n\n# Compute 95% credible interval\nci_95_participant1 &lt;- quantile(phat1, c(0.025, 0.975))\n\n# Plot\n\nggplot(post_pred_df, aes(x = pred)) + \n  geom_density(color='darkblue', fill='lightblue') +\n  geom_vline(xintercept = ci_95_participant1[1], color='red', linetype='dashed') +\n  geom_vline(xintercept = ci_95_participant1[2], color='red', linetype='dashed') +\n  xlab('Probability of being diabetic (Outcome=1)') +\n  ggtitle('Posterior Predictive Distribution 95% Credible Interval') +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\nCode\nparticipant2_data  &lt;- adult[2, ]\n\n\n# predicted probabilities for patient 1\nphat2 &lt;- posterior_linpred(bayes_fit, newdata = participant2_data, transform = TRUE)\n# 'transform = TRUE' gives probabilities for logistic regression\n\n# Store in a data frame for plotting\npost_pred_df2 &lt;- data.frame(pred = phat2)\n\n# Compute 95% credible interval\nci_95_participant2 &lt;- quantile(phat2, c(0.025, 0.975))\n\n# Plot\n\nggplot(post_pred_df2, aes(x = pred)) + \n  geom_density(color='darkblue', fill='lightblue') +\n  geom_vline(xintercept = ci_95_participant2[1], color='red', linetype='dashed') +\n  geom_vline(xintercept = ci_95_participant2[2], color='red', linetype='dashed') +\n  xlab('Probability of being diabetic (Outcome=1)') +\n  ggtitle('Posterior Predictive Distribution 95% Credible Interval') +\n  theme_bw()"
  },
  {
    "objectID": "index.html#external-validation",
    "href": "index.html#external-validation",
    "title": "Bayesian Logistic Regression - Application in Probability Prediction of disease (Diabetes)",
    "section": "External validation",
    "text": "External validation\n\nUsing the Bayesian model, we computed the posterior predicted probability for a new participant based on age, BMI, sex, and race. The density plot with 95% credible intervals captures both the most likely risk and uncertainty, supporting individualized preventive care\n\n\n\nCode\nlibrary(ggplot2)\n\nnew_participant &lt;- data.frame(\n  age_c = 40,\n  bmi_c = 25,\n  sex   = \"Female\",\n  race  = \"Mexican American\"\n)\n\n# Posterior predicted probabilities\nphat_new &lt;- posterior_linpred(bayes_fit, newdata = new_participant, transform = TRUE)\n\n# Convert to numeric vector\nphat_vec &lt;- as.numeric(phat_new)\n\n# Check the range to see if all values are similar\nrange(phat_vec)\n\n\n[1] 1 1\n\n\nCode\n# Store in a data frame\npost_pred_df_new &lt;- data.frame(pred = phat_vec)\n\n# Compute 95% credible interval from the vector\nci_95_new_participant &lt;- quantile(phat_vec, c(0.025, 0.975))\n\n# Plot\nggplot(post_pred_df_new, aes(x = pred)) + \n  geom_density(color='darkblue', fill='lightblue', alpha = 0.6) +\n  geom_vline(xintercept = ci_95_new_participant[1], color='red', linetype='dashed') +\n  geom_vline(xintercept = ci_95_new_participant[2], color='red', linetype='dashed') +\n  xlim(0, 1) +  # ensures you see the curve even if values are close\n  xlab('Probability of being diabetic (Outcome=1)') +\n  ggtitle('Posterior Predictive Distribution (95% Credible Interval)') +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n## Targeted BMI Estimation for Predicted Diabetes Risk\n\nTo examine how BMI influences the predicted probability of diabetes while holding other covariates constant (age, sex, race), we used the fitted Bayesian logistic regression model:\n\nBMI grid: Generated a range of BMI values (18‚Äì40 kg/m¬≤) for a representative individual: Age = 40, Sex = Female, Race = Mexican American.\nPosterior predictions: Computed the posterior predicted probability of diabetes for each BMI value.\nAveraging: Calculated the mean predicted probability across posterior draws for each BMI.\nTarget probability approach: Defined a clinically relevant target probability (e.g., 0.3) and identified the BMI whose predicted probability is closest to this target, allowing inverse prediction from probability to BMI.\nVisualization Line plot of predicted probability vs BMI - The blue curve shows how likely different probability values are according to your posterior predictive distribution.-\n- The red dashed lines show the 95% credible interval (CI) for this participant‚Äôs probability of being diabetic. - Limits x-axis between 0 and 1 (valid range for probabilities). - Red dashed horizontal line: target probability (0.3). - Red dotted vertical line: BMI corresponding to the target probability (~closest BMI).Annotated to highlight the BMI threshold. - Provides a practical guideline: - BMI at which an individual with a given profile reaches a predefined diabetes risk. - Supports personalized risk communication and preventive interventions. - Translates model output into actionable, clinically relevant thresholds, bridging research findings with public health application. - This approach demonstrates how Bayesian posterior predictions can be used for targeted, individualized risk assessment, informing precision prevention strategies based on modifiable risk factors like BMI.\nPractical Implications\n\nage and BMI as robust and independent predictors of diabetes, underscore the importance of early targeted interventions in mitigating diabetes risk.\nLongitudinal studies and combining other statistical analytical methods with Bayesian can further enhance and provide better informed precision prevention strategies.\n\n\n\nCode\n# Grid of possible BMI values (centered if model used bmi_c)\nbmi_seq &lt;- seq(18, 40, by = 0.5)\n\nnewdata_grid &lt;- data.frame(\n  age_c = 40,\n  bmi_c = bmi_seq,\n  sex   = \"Female\",\n  race  = \"Mexican American\"\n)\n\n# Posterior mean predicted probabilities\npred_probs &lt;- posterior_linpred(bayes_fit, newdata = newdata_grid, transform = TRUE)\n# Average over posterior draws to get the mean predicted probability per BMI\nprob_mean &lt;- colMeans(pred_probs)\n\n# Combine with BMI values\npred_df &lt;- cbind(newdata_grid, prob_mean)\n\ntarget_prob &lt;- 0.3\nclosest &lt;- pred_df[which.min(abs(pred_df$prob_mean - target_prob)), ]\n\nclosest\n\n\n  age_c bmi_c    sex             race prob_mean\n1    40    18 Female Mexican American         1\n\n\nCode\nggplot(pred_df, aes(x = bmi_c, y = prob_mean)) +\n  geom_line(color = \"darkblue\", linewidth = 1.2) +\n  geom_hline(yintercept = target_prob, color = \"red\", linetype = \"dashed\") +\n  geom_vline(xintercept = closest$bmi_c, color = \"red\", linetype = \"dotted\") +\n  annotate(\"text\", x = closest$bmi_c, y = target_prob + 0.05,\n           label = paste0(\"Target BMI ‚âà \", round(closest$bmi_c, 1)),\n           color = \"red\", hjust = -0.1) +\n  labs(\n    x = \"BMI (centered or raw, depending on model)\",\n    y = \"Predicted Probability of Diabetes\",\n    title = \"Inverse Prediction: BMI Needed for Target Diabetes Risk\"\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Posterior predictive prevalence (replicated datasets)\nyrep &lt;- brms::posterior_predict(bayes_fit, ndraws = 2000)  # draws √ó observations (0/1)\npost_prev &lt;- rowMeans(yrep)  # prevalence for each posterior draw\n\n# Survey-weighted observed prevalence (population estimate)\ndes_obs &lt;- survey::svydesign(\n  id = ~SDMVPSU, strata = ~SDMVSTRA, weights = ~WTMEC2YR,\n  nest = TRUE, data = adult_imp1\n)\n\n# Compute survey-weighted diabetes prevalence\nobs_est &lt;- survey::svymean(~diabetes_dx, des_obs, na.rm = TRUE)\n\n# Extract safely\nobs_prev &lt;- as.numeric(obs_est[\"diabetes_dx\"])\nobs_se   &lt;- as.numeric(SE(obs_est)[\"diabetes_dx\"])\n\n# Compute CI only if not NA\nif (is.na(obs_prev) | is.na(obs_se)) {\n  obs_lcl &lt;- NA\n  obs_ucl &lt;- NA\n} else {\n  obs_lcl &lt;- max(0, obs_prev - 1.96 * obs_se)\n  obs_ucl &lt;- min(1, obs_prev + 1.96 * obs_se)\n}\n\n# Cleaned text for CI\nci_text &lt;- if (is.na(obs_lcl) | is.na(obs_ucl)) {\n  \"(95% CI unavailable)\"\n} else {\n  sprintf(\"(95%% CI %.1f‚Äì%.1f%%)\", 100 * obs_lcl, 100 * obs_ucl)\n}\n\n# Plot: posterior density with weighted point estimate and 95% CI band\nlibrary(ggplot2)\nggplot(data.frame(prev = post_prev), aes(x = prev)) +\n  geom_density(alpha = 0.6) +\n  # Show CI band only if valid\n  {if (!is.na(obs_lcl) & !is.na(obs_ucl))\n    annotate(\"rect\", xmin = obs_lcl, xmax = obs_ucl, ymin = 0, ymax = Inf, alpha = 0.15)} +\n  geom_vline(xintercept = obs_prev, linetype = 2) +\n  coord_cartesian(xlim = c(0, 1)) +\n  labs(\n    x = \"Diabetes prevalence\",\n    y = \"Posterior density\",\n    subtitle = sprintf(\n      \"Survey-weighted NHANES prevalence = %.1f%% %s\",\n      100 * obs_prev, ci_text\n    )\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nInterpretation:\n\nObserved NHANES prevalence: ‚âà 8.9%\nPosterior predictive mean: roughly 8‚Äì9%\nConclusion: The Bayesian logistic regression model predicts population-level diabetes prevalence consistent with NHANES survey estimates.\n‚Üí This indicates strong model calibration and external validity."
  },
  {
    "objectID": "Introduction.html",
    "href": "Introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Diabetes mellitus (DM) is a major public health concern closely associated with factors such as obesity, age, race, and gender. Identifying these associated risk factors is essential for targeted interventions D‚ÄôAngelo and Ran (2025). Logistic Regression (traditional) that estimates the association between risk factors and outcomes is insufficient in analyzing the complex healthcare data (DNA sequences, imaging, patient-reported outcomes, electronic health records (EHRs), longitudinal health measurements, diagnoses, and treatments. Zeger et al. (2020). Classical maximum likelihood estimation (MLE) yields unstable results in samples that are small, have missing data, or presents quasi- and complete separation.\nBayesian hierarchical models using Markov Chain Monte Carlo (MCMC) allow analysis of multivariate longitudinal healthcare data with repeated measures within individuals and individuals nested in a population. By integrating prior knowledge and including exogenous (e.g., age, clinical history) and endogenous (e.g., current treatment) covariates, Bayesian models provide posterior distributions and risk predictions for conditions such as pneumonia, prostate cancer, and mental disorders. Parametric assumptions remain a limitation of these models.\nIn Bayesian inference Chatzimichail and Hatjimihail (2023), Bayesian inference has shown that parametric models (with fixed parameters) often underperform compared to nonparametric models, which do not assume a prior distribution. Posterior probabilities from Bayesian approaches improve disease classification and better capture heterogeneity in skewed, bimodal, or multimodal data distributions. Bayesian nonparametric models are flexible and robust, integrating multiple diagnostic tests and priors to enhance accuracy and precision, though reliance on prior information and restricted access to resources can limit applicability. Combining Bayesian methods with other statistical or computational approaches helps address systemic biases, incomplete data, and non-representative datasets.\nThe Bayesian framework described by Schoot et al. (2021) highlights the role of priors, data modeling, inference, posterior sampling, variational inference, and variable selection.Proper variable selection mitigates multicollinearity, overfitting, and limited sampling, improving predictive performance. Priors can be informative, weakly informative, or diffuse, and can be elicited from expert opinion, generic knowledge, or data-driven methods. Sensitivity analysis evaluates the alignment of priors with likelihoods, while MCMC simulations (e.g., brms, blavaan in R) empirically estimate posterior distributions. Spatial and temporal Bayesian models have applications in large-scale cancer genomics, identifying molecular interactions, mutational signatures, patient stratification, and cancer evolution, though temporal autocorrelation and subjective prior elicitation can be limiting.\nBayesian normal linear regression has been applied in metrology for instrument calibration using conjugate Normal‚ÄìInverse-Gamma priors Klauenberg et al. (2015). Hierarchical priors add flexibility by modeling uncertainty across multiple levels, improving robustness and interpretability. Bayesian hierarchical/meta-analytic linear regression incorporates both exchangeable and unexchangeable prior information, addressing multiple testing challenges, small sample sizes, and complex relationships among regression parameters across studies Leeuw and Klugkist (2012)\nA sequential clinical reasoning model Liu et al. (2013) Sequential clinical reasoning models demonstrate screening by adding predictors stepwise: (1) demographics, (2) metabolic components, and (3) conventional risk factors, incorporating priors and mimicking clinical evaluation. This approach captures ecological heterogeneity and improves baseline risk estimation, though interactions between predictors and external cross-validation remain limitations.\nBayesian multiple imputation with logistic regression addresses missing data in clinical research Austin et al. (2021) in clinical research by classifying missing values (e.g., patient refusal, loss to follow-up, mechanical errors) as MAR, MNAR, or MCAR. Multiple imputation generates plausible values across datasets and pools results for reliable classification of patient health status and mortality."
  },
  {
    "objectID": "slides.html#model-diagnostics-7",
    "href": "slides.html#model-diagnostics-7",
    "title": "Bayesian Logistic Regression for Diabetes Risk Prediction (NHANES 2013 - 2014)",
    "section": "Model Diagnostics",
    "text": "Model Diagnostics\nR Square\n`r Model fit: bayes_R2(bayes_fit)\n    Estimate  Est.Error      Q2.5    Q97.5\nR2 0.1313342 0.01265055 0.1064607 0.156078\n\n### Assumptions for Bayesian Logistic Regression\n\n::: nonincremental\n-   Data Binary outcome\n-   Independent observations\n-   Relationship Linear in logit\n-   No perfect collinearity\n-   Priors Properly chosen: informative enough\n-   Posterior Proper and convergent Fit\n-   No complete separation\n-   Good predictive checks\n:::\n\n------------------------------------------------------------------------\n\n### Translational Perspective\n\n::: panel-tabset\n### Internal Validation\n\n#### Personalized Risk Estimation\n\n-   Model enables individualized diabetes risk prediction using age, BMI, sex, race and outputs can guide actionable thresholds (e.g., risk \\&gt;30%)\n-   BMI is modifiable- interventions can lower risk while sex and race remain constant.\n-   Selected one participant (adult\\[1, \\]) with all covariates (age, BMI, sex, race) obtain predicted probabilities from the logistic regression model. **posterior_linpred(transform = TRUE)**\n-   Posterior draws: calculate the 95% credible interval for the predicted probability\n\n### Code\n\n``` r\nparticipant1_data  &lt;- adult[1, ]\nphat1 &lt;- posterior_linpred(bayes_fit, newdata = participant1_data, transform = TRUE)\n# Store in a data frame for plotting\npost_pred_df &lt;- data.frame(pred = phat1)\nci_95_participant1 &lt;- quantile(phat1, c(0.025, 0.975))\n\nggplot(post_pred_df, aes(x = pred)) + \n  geom_density(color='darkblue', fill='lightblue') +\n  geom_vline(xintercept = ci_95_participant1[1], color='red', linetype='dashed') +\n  geom_vline(xintercept = ci_95_participant1[2], color='red', linetype='dashed') +\n  xlab('Probability of being diabetic (Outcome=1)') +\n  ggtitle('Posterior Predictive Distribution 95% Credible Interval') +\n  theme_bw()\n  \nInterpretation\nPosterior predictive distribution for a specific individual with Outcome = 1\n\nMedian/mean predicted probability ‚âà 0.25\nThe peak of the density curve is around 25%: participant has a 1 in 4 chance of being diabetic.\n95% credible interval ‚âà 0.20 to 0.31\nThe red dashed lines represent the 95% credible interval (CI): Lower bound ‚âà 0.20 Upper bound ‚âà 0.31\n\nGiven the model and the participant‚Äôs characteristics, there is a 95% probability that this participant‚Äôs true diabetes risk lies between 20% and 31% :::"
  }
]