---
title: "Bayesian Logistic Regression - Application in Probability Prediction of disease (Diabetes) "
subtitle: "CapStone Project_2025"
author: "Namita Mishra, Autumn Wilcox (Advisor: Dr. Ashraf Cohen)"
date: '`r Sys.Date()`'
course: Capstone Projects in Data Science

format:
  html:
    theme: flatly
    code-fold: true
    toc: true
   

bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

Slides: [slides.html](slides.html){target="_blank"} ( Go to `slides.qmd`
to edit)

------------------------------------------------------------------------

# Introduction

Diabetes mellitus (DM) is a major public health concern closely
associated with factors such as obesity, age, race, and gender.
Identifying these associated risk factors is essential for targeted
interventions @DAngelo2025. **Logistic Regression** (traditional) that
estimates the association between risk factors and outcomes is
insufficient in analyzing the complex healthcare data (DNA sequences,
imaging, patient-reported outcomes, electronic health records (EHRs),
longitudinal health measurements, diagnoses, and treatments. @Zeger2020.
Classical maximum likelihood estimation (MLE) yields unstable results in
samples that are small, have missing data, or presents quasi- and
complete separation.

Bayesian hierarchical models using Markov Chain Monte Carlo (MCMC) allow
analysis of multivariate longitudinal healthcare data with repeated
measures within individuals and individuals nested in a population. By
integrating prior knowledge and including exogenous (e.g., age, clinical
history) and endogenous (e.g., current treatment) covariates, Bayesian
models provide posterior distributions and risk predictions for
conditions such as pneumonia, prostate cancer, and mental disorders.
Parametric assumptions remain a limitation of these models.

In Bayesian inference @Chatzimichail2023, Bayesian inference has shown
that parametric models (with fixed parameters) often underperform
compared to nonparametric models, which do not assume a prior
distribution. Posterior probabilities from Bayesian approaches improve
disease classification and better capture heterogeneity in skewed,
bimodal, or multimodal data distributions. Bayesian nonparametric models
are flexible and robust, integrating multiple diagnostic tests and
priors to enhance accuracy and precision, though reliance on prior
information and restricted access to resources can limit applicability.
Combining Bayesian methods with other statistical or computational
approaches helps address systemic biases, incomplete data, and
non-representative datasets.

The Bayesian framework described by @VandeSchoot2021 highlights the role
of priors, data modeling, inference, posterior sampling, variational
inference, and variable selection.Proper variable selection mitigates
multicollinearity, overfitting, and limited sampling, improving
predictive performance. Priors can be informative, weakly informative,
or diffuse, and can be elicited from expert opinion, generic knowledge,
or data-driven methods. Sensitivity analysis evaluates the alignment of
priors with likelihoods, while MCMC simulations (e.g., brms, blavaan in
R) empirically estimate posterior distributions. Spatial and temporal
Bayesian models have applications in large-scale cancer genomics,
identifying molecular interactions, mutational signatures, patient
stratification, and cancer evolution, though temporal autocorrelation
and subjective prior elicitation can be limiting.

Bayesian normal linear regression has been applied in metrology for
instrument calibration using conjugate Normal–Inverse-Gamma priors
@Klauenberg2015. Hierarchical priors add flexibility by modeling
uncertainty across multiple levels, improving robustness and
interpretability. Bayesian hierarchical/meta-analytic linear regression
incorporates both exchangeable and unexchangeable prior information,
addressing multiple testing challenges, small sample sizes, and complex
relationships among regression parameters across studies @DeLeeuw2012a

**A sequential clinical reasoning model** @Liu2013 Sequential clinical
reasoning models demonstrate screening by adding predictors stepwise:
(1) demographics, (2) metabolic components, and (3) conventional risk
factors, incorporating priors and mimicking clinical evaluation. This
approach captures ecological heterogeneity and improves baseline risk
estimation, though interactions between predictors and external
cross-validation remain limitations.

**Bayesian multiple imputation with logistic regression** addresses
missing data in clinical research @Austin2021 in clinical research by
classifying missing values (e.g., patient refusal, loss to follow-up,
mechanical errors) as MAR, MNAR, or MCAR. Multiple imputation generates
plausible values across datasets and pools results for reliable
classification of patient health status and mortality.

## Aims

The present study aims performs Bayesian logistic regression to predict
diabetes status and evaluate the associations between diabetes and
predictors (body mass index (BMI), age (≥20 years), gender, and race).
The study anakyzes a retrospective dataset (2013–2014 NHANES survey
data). It is based on a complex sampling design, characterized by
stratification, clustering, and oversampling of specific population
subgroups, rather than uniform random sampling. A Bayesian analytical
approach addresses challenges posed by dataset anomalies such as missing
data, complete case analysis, and separation that limit the efficiency
and reliability of traditional logistic regression in predicting health
outcomes.

# Method

## **Bayesian Logistic Regression**

The study employs **Bayesian logistic regression** to estimate
associations between predictors and outcome probabilities.\
The **Bayesian framework** integrates prior information with observed
data to generate posterior distributions, allowing direct probabilistic
interpretation of parameters.\
This approach provides flexibility in model specification, accounts for
parameter uncertainty, and produces **credible intervals** that fully
reflect uncertainty in the estimates.\
Unlike traditional frequentist methods, the Bayesian method enables
inference through **posterior probabilities**, supporting more nuanced
decision-making and interpretation.

------------------------------------------------------------------------

## **Model Structure**

-   **Bayesian logistic regression** is a probabilistic modeling
    framework used to estimate the relationship between one or more
    predictors (continuous or categorical) and a binary outcome (e.g.,
    presence/absence of disease).\

-   It extends classical logistic regression by combining it with
    **Bayesian inference**, treating model parameters as random
    variables with probability distributions rather than fixed point
    estimates.\

-   The logistic model relates the probability of an outcome ( Y = 1 )
    to a linear combination of predictors through the logit link
    function:

    \[ \text{logit}(P(Y = 1)) = \beta\_0 + \beta\_1 X_1 + \beta\_2 X_2 +
    \dots + \beta\_k X_k \]

    logit(pi)=β0+j=1∑pβjxij

	p_i: the probability of the event (e.g., having diabetes) for individual i.
	"logit"(p_i)=log⁡(p_i/(1-p_i )): the log-odds of the event.
	β_0: the intercept — the log-odds of the event when all predictors x_ij=0.
	β_j: the coefficient for predictor x_j, representing the change in log-odds for a one-unit increase in x_ij, holding other variables constant.
	∑_(j=1)^p β_j x_ij: the combined linear effect of all predictors.

-   In the Bayesian framework, the coefficients ( \beta ) are assigned
    prior distributions, which are updated in light of the observed data
    to yield posterior distributions.

------------------------------------------------------------------------

## **Bayesian Approach**

-   The Bayesian approach naturally incorporates **uncertainty** in all
    model parameters.\

-   It combines prior beliefs with observed data to produce posterior
    distributions according to Bayes’ theorem:

    \[\text{Posterior} \propto \text{Likelihood} \times \text{Prior}\]

-   **Likelihood:** Represents the probability of the observed data
    given the model parameters (as in classical logistic regression).\

-   **Prior:** Encodes prior knowledge or beliefs about parameter values
    before observing the data.\

-   **Posterior:** Represents updated beliefs about parameters after
    observing the data.

### **Prior Specification**

A weakly informative **Student’s t-distribution prior**,
`student_t(3, 0, 10)`, was used for regression coefficients (van de
Schoot et al., 2013).\
This prior:\
- Has **3 degrees of freedom** (( \nu = 3 )), producing heavy tails that
allow for occasional large effects.\
- Is **centered at 0** (( \mu = 0 )), reflecting no initial bias toward
positive or negative associations.\
- Has a **scale parameter of 10** (( \sigma = 10 )), allowing broad
variation in possible coefficient values.\
Such priors improve stability in models with small sample sizes, high
collinearity, or potential outliers.

------------------------------------------------------------------------

## **Advantages of Bayesian Logistic Regression**

-   **Uncertainty quantification:** Produces full posterior
    distributions instead of single-point estimates.\
-   **Credible intervals:** Provide the range within which a parameter
    lies with a specified probability (e.g., 95%).\
-   **Flexible priors:** Allow incorporation of expert knowledge or
    results from previous studies.\
-   **Probabilistic predictions:** Posterior predictive distributions
    yield direct probabilities for future observations.\
-   **Comprehensive model checking:** Posterior predictive checks (PPCs)
    evaluate how well simulated outcomes reproduce observed data.

------------------------------------------------------------------------

## **Posterior Predictions**

Posterior distributions of the coefficients are used to estimate the
probability of the outcome for given predictor values.\
This enables statements such as:\
\> “Given the predictors, the probability of the outcome lies between X%
and Y%.”

Posterior predictions incorporate two sources of uncertainty:\
- **Parameter uncertainty:** Variability in estimated model
coefficients.\
- **Predictive uncertainty:** Variability in future outcomes given those
parameters.

------------------------------------------------------------------------

## **Model Evaluation and Diagnostics**

Model quality and convergence were assessed using standard Bayesian
diagnostics:

-   **Convergence diagnostics:** Markov Chain Monte Carlo (MCMC)
    performance was evaluated using ( \hat{R} ) (R-hat) and effective
    sample size (ESS).\
-   **Autocorrelation checks:** Ensured independence between successive
    MCMC draws.\
-   **Posterior predictive checks (PPC):** Compared simulated data from
    posterior distributions to observed outcomes.\
-   **Bayesian R²:** Quantified the proportion of variance explained by
    the predictors, incorporating uncertainty.

In Bayesian analysis, every unknown parameter — such as a regression
coefficient, mean, or variance — is treated as a random variable with a
probability distribution that expresses uncertainty given the observed
data.

```{r}
#| label: Libraries
#| include: false

# loading packages 
options(repos = c(CRAN = "https://cloud.r-project.org"))
install.packages("nhanesA", dependencies = TRUE)

install.packages("nhanesA")
library ("nhanesA")    
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
library(Hmisc)
library(dplyr)
library(tidyr)
library(forcats)
library(ggplot2)
library(classpackage)
library(janitor)
install.packages("gt")   
library(gt)
library(survey)
library(DataExplorer)
library(logistf)

```

# Analysis and Results

### Statistical Tool

R packages and libraries are used to import data, perform data wrangling
and analysis.

### Data source

-   NHANES 2-year data (2013-2014) cross-sectional weighted data
    @CenterforHealthStatistics1999 was imported in R

### Data pre-processing

Adult dataset:Three NHANES datasets (demographics, exam, questionnaire)
in.XPT format are imported (Haven package) in R. Variables of interest
are selected using the original weighted datasets and ID to create a
single adult analytic dataframe.

### Data Variables

**Response Variable** - Binary Type 2 / diagnosed diabetes(excluding
gestational diabetes) diabetes_dx created combning - `DIQ010` - Doctor
told you have diabetes - `DIQ050`- excluded (a secondary variable
describing treatment status (insulin use)). **Predictor Variables** -
Body Mass Index, factor, 4 levels\
**Covariates** - Gender (factor, 2 levels) - Ethnicity (factor, 5
levels) - Age (continuous 20-80years)

```{r message=FALSE, warning=FALSE}
#| label: nhanes tables and merged data
#| include: false

library ("nhanesA")     
 # imported datasets 
                       nhanesTables('EXAM', 2013)
                       nhanesTables('QUESTIONNAIRE', 2013)
                       nhanesTables('DEMOGRAPHICS', 2013)
            
# codebook for variable details

nhanesCodebook("DEMO_H",'RIDRETH1')
nhanesCodebook("DEMO_H",'RIAGENDR')
nhanesCodebook("DEMO_H",'RIDAGEYR')
nhanesCodebook("DIQ_H","DIQ010, DIQ050")
nhanesCodebook("BMX_H",'BMXBMI')

  #  .xpt files read ( 2013–2014)                      
                      bmx_h <- nhanes("BMX_H")         #Exam
                      demo_h <- nhanes("DEMO_H")       #Demo
                      diq_h <- nhanes("DIQ_H")         #diabetes

# variables of interest

library(dplyr)

exam_sub <- bmx_h %>% 
  select(SEQN, BMXBMI) %>%
  rename(
    ID = SEQN,
    BMI = BMXBMI
  )

need_demo <- c("SEQN","RIDAGEYR","RIAGENDR","RIDRETH1","SDMVPSU","SDMVSTRA","WTMEC2YR")
stopifnot(all(c("SEQN","BMXBMI") %in% names(bmx_h)))
stopifnot(all(need_demo %in% names(demo_h)))
if (!("DIQ010" %in% names(diq_h))) {
  stop("DIQ010 is not in DIQ_H. Check the cycle name 'DIQ_H' and nhanesA version.")
}

 #  Select only needed variables
exam_sub <- bmx_h  %>% select(SEQN, BMXBMI)
demo_sub <- demo_h %>% select(all_of(need_demo))
diq_sub  <- diq_h  %>% select(SEQN, DIQ010, dplyr::any_of("DIQ050"))

# merged dataframe

merged_data <- demo_sub %>%
  left_join(exam_sub, by = "SEQN") %>%
  left_join(diq_sub,  by = "SEQN")

names(merged_data)
saveRDS(merged_data, "data/nhanes2013_2014_prepared.rds")


```

```{r message=FALSE, warning=FALSE}
#| label: merged dataframe and character handling
#| include: false

# print(glimpse(merged_data))
print(table(merged_data$BMXBMI, useNA = "ifany"))
print(table(merged_data$DIQ010,  useNA = "ifany"))

 #  ---- Coercion helpers (handle labelled/character) ----
to_num <- function(x) {
  if (is.numeric(x)) return(x)
  xc <- as.character(x)
  n <- suppressWarnings(readr::parse_number(xc))
  if (mean(is.na(n)) > 0.80) {
    xlow <- tolower(trimws(xc))
    n <- dplyr::case_when(
      xlow %in% c("1","yes","yes, told") ~ 1,
      xlow %in% c("2","no","no, not told") ~ 2,
      xlow %in% c("3","borderline") ~ 3,
      xlow %in% c("7","refused") ~ 7,
      xlow %in% c("9","don't know","dont know","unknown") ~ 9,
      TRUE ~ NA_real_
    )
  }
  as.numeric(n)
}

merged_data <- merged_data %>%
  mutate(
    DIQ010   = to_num(DIQ010),
    DIQ050   = to_num(if (!"DIQ050" %in% names(.)) NA_real_ else DIQ050),
    BMXBMI   = suppressWarnings(as.numeric(BMXBMI)),
    RIDAGEYR = suppressWarnings(as.numeric(RIDAGEYR)),
    RIAGENDR = suppressWarnings(as.numeric(RIAGENDR)),
    RIDRETH1 = suppressWarnings(as.numeric(RIDRETH1)),
    SDMVPSU  = suppressWarnings(as.numeric(SDMVPSU)),
    SDMVSTRA = suppressWarnings(as.numeric(SDMVSTRA)),
    WTMEC2YR = suppressWarnings(as.numeric(WTMEC2YR))
  )

# ---- Diagnostics BEFORE save ----
cat("DIQ010 counts BEFORE save:\n")
print(table(merged_data$DIQ010, useNA = "ifany"))
cat("Count with DIQ010 in {1,2}:", sum(merged_data$DIQ010 %in% c(1,2), na.rm = TRUE), "\n")

# ---- Save to file for reuse ----
dir.create("data", showWarnings = FALSE)
# ---- Save ----
dir.create("data", showWarnings = FALSE, recursive = TRUE)
saveRDS(merged_data, "data/merged_2013_2014.rds")
message("Saved: data/merged_2013_2014.rds")


```

```{r}
#| label: Varaibles table
#| echo: false
library(knitr)
library(kableExtra)

# -----------------------------
# Variable descriptions
# -----------------------------
var_descriptions <- data.frame(
  Variable = c(
    "diabetes_dx", "age", "bmi", "sex", "race",
    "WTMEC2YR", "SDMVPSU", "SDMVSTRA",
    "age_c", "bmi_c", "wt_norm"
  ),
  Description = c(
    "Diabetes diagnosis (1 = Yes, 0 = No) based on medical questionnaire.",
    "Age of participant in years.",
    "Body Mass Index (BMI) in kilograms per square meter (kg/m²), calculated from measured height and weight.",
    "Sex of participant (Male or Female).",
    "Race/Ethnicity (e.g., Non-Hispanic White, Non-Hispanic Black, Mexican American, etc.).",
    "Examination sample weight for MEC (Mobile Examination Center) participants.",
    "Primary Sampling Unit (PSU) used for variance estimation in complex survey design.",
    "Stratum variable used to define strata for complex survey design.",
    "Age variable centered and standardized (z-score).",
    "BMI variable centered and standardized (z-score).",
    "Normalized survey weight (WTMEC2YR divided by its mean, for model weighting)."
  ),
  Type = c(
    "Categorical", "Continuous", "Continuous",
    "Categorical", "Categorical",
    "Weight", "Design", "Design",
    "Continuous", "Continuous", "Weight"
  )
)

# -----------------------------
# Display formatted table
# -----------------------------
kable(var_descriptions, caption = "Variable Descriptions: Adult Dataset") %>%
  kable_styling(full_width = FALSE, position = "center", bootstrap_options = c("striped", "hover"))


```

## Exploratory Data Analysis (Adult, 20 - 80 years)

-   Discrete vs Continuous Columns: 25% of columns are discrete
    (categorical) while 75% are continuous, indicating that the dataset
    primarily contains continuous measurements such as age and BMI.
-   Complete Rows: 92.7% of rows have complete information for all
    variables, meaning most participants have fully observed data across
    predictors and outcomes.

Survey design: - It is a national survey based on complex sampling
designs (oversampling certain groups (e.g., minorities, older adults) to
ensure representation. - They use multistage sampling to represent the
U.S. population, so we apply sampling weights, strata, and PSU (primary
sampling units) for valid estimates. - We use survey design in
regression anlaysis to avoid to avoid bias prevalence estimates (e.g.,
mean BMI or diabetes %), underestimation of standard errors and
incorrect inference for population-level parameters. - It includes
auxillary variables: SDMVPSU, SDMVSTRA, WTMEC2YR - - Diabetes grouped
from (DIQ010 excluding DIQ050): diabetes_dx (numeric 0/1) - Covariates:
ethnicity (5 levels), age range (20-80 years), gender (male and female),
BMI as continuous - Centered covariates: age_c, bmi_c BMI categories:
bmi_cat - Presented here is the mean, standard error and variance of the
survey weighted data

| Step | Description |
|------------------------|------------------------------------------------|
| **Weighting** | Used the `survey` package to calculate weighted means and standard deviations for all variables. |
| **Standardization** | Standardized BMI and age variables for analysis. |
| **Age Categorization** | Recoded into intervals: 20–\<30, 30–\<40, 40–\<50, 50–\<60, 60–\<70, and 70–80 years. |
| **BMI Categorization** | Recoded and categorized as: \<18.5 (Underweight), 18.5–\<25 (Normal), 25–\<30 (Overweight), 30–\<35 (Obesity I), 35–\<40 (Obesity II), ≥40 (Obesity III). |
| **Ethnicity Recoding** | Recoded as: 1 = Mexican American, 2 = Other Hispanic, 3 = Non-Hispanic White, 4 = Non-Hispanic Black, 5 = Other/Multi. |
| **Special Codes** | Special codes (e.g., 3, 7) were transformed to `NA`. These codes are not random and could introduce bias if ignored (MAR or MNAR). |
| **Missing Data** | Missing values were retained and visualized to assess their pattern and informativeness. |
| **Final Dataset** | Created a cleaned analytic dataset (`adult`) using *Non-Hispanic White* and *Male* as reference groups for analysis. |

```{r}
#| label: Adults Dataset preparation
## 
# ---------------- Basic Exploration (adults) ----------------

# Keep adults only and build analysis variables
adult <- merged_data %>%
  dplyr::filter(RIDAGEYR >= 20) %>%
  dplyr::transmute(
    # --- keep survey design variables so svydesign() can see them ---
    SDMVPSU, SDMVSTRA, WTMEC2YR,

    # --- outcome: DIQ010 (1 yes, 2 no; 3/7/9 -> NA) ---
    diabetes_dx = dplyr::case_when(
      DIQ010 == 1 ~ 1,
      DIQ010 == 2 ~ 0,
      DIQ010 %in% c(3, 7, 9) ~ NA_real_,
      TRUE ~ NA_real_
    ),

    # --- predictors (raw) ---
    bmi  = BMXBMI,
    age  = RIDAGEYR,

    # sex (1=Male, 2=Female)
    sex  = forcats::fct_recode(factor(RIAGENDR), Male = "1", Female = "2"),

    # race (5-level)
    race = forcats::fct_recode(
      factor(RIDRETH1),
      "Mexican American" = "1",
      "Other Hispanic"   = "2",
      "NH White"         = "3",
      "NH Black"         = "4",
      "Other/Multi"      = "5"
    ),

    # keep DIQ050 so we can safely reference it (may be absent/NA in some rows)
    
    DIQ050 = DIQ050
  ) %>%
  # standardize continuous predictors
  dplyr::mutate(
    age_c = as.numeric(scale(age)),
    bmi_c = as.numeric(scale(bmi)),
    bmi_cat = cut(
      bmi,
      breaks = c(-Inf, 18.5, 25, 30, 35, 40, Inf),
      labels = c("<18.5","18.5–<25","25–<30","30–<35","35–<40","≥40"),
      right = FALSE
    )
  ) %>%
  # adjust outcome: if female & DIQ050==1 ("only when pregnant"), set to 0 (not diabetes)
  dplyr::mutate(
    diabetes_dx = ifelse(sex == "Female" & !is.na(DIQ050) & DIQ050 == 1, 0, diabetes_dx)
  )

# Make NH White the reference level for race (clearer interpretation)
adult <- adult %>%
  dplyr::mutate(
    race = forcats::fct_relevel(race, "NH White")
  )

# --- sanity checks ---
cat("Adults n =", nrow(adult), "\n")

```

```{r}
#| label: survey data results
#| include: false

#| label: survey design
#| echo: false
# survey design
# ---------------- Survey Design ----------------
# Use exam weights because BMI (BMXBMI) is an MEC variable

nhanes_design_adult <- survey::svydesign(
  id = ~SDMVPSU,
  strata = ~SDMVSTRA,
  weights = ~WTMEC2YR,
  nest = TRUE,
  data = adult
)

# quick weighted checks
survey::svymean(~age, nhanes_design_adult, na.rm = TRUE)
survey::svymean(~diabetes_dx, nhanes_design_adult, na.rm = TRUE)

# Calculate effective sample size for diabetes

# Variance ignoring survey design (i.e., assuming SRS)
v <- svyvar(~diabetes_dx, nhanes_design_adult, na.rm = TRUE)
p <- mean(adult$diabetes_dx, na.rm = TRUE)
v_srs <- p * (1 - p) / nrow(adult)

# Design effect = actual variance / SRS variance
deff <- v / v_srs
deff  # design effect
n_total <- sum(weights(nhanes_design_adult))
ess <- n_total / deff
cat("Effective sample size for diabetes_dx:", round(ess), "\n")

```

```{r}
#| label: Adult Data and missingness
#| echo: true

library(dplyr)
library(skimr)
library(knitr)
library(tidyr)
library(purrr)
library(forcats)
library(kableExtra)

str(adult)
plot_str(adult)
head(adult)

plot_intro(adult, title="Figure 1 (Adult dataset). Structure of variables and missing observations.")
plot_missing(adult, title="Figure 2(Adult dataset). Breakdown of missing observations.")

```

## Study population (Adult, NHANES)

-   Number of participants in Adult dataset (n = 5769) after cleaning
    and analysis Variables
     mean     SE
age 47.496 0.3805
                mean     SE
diabetes_dx 0.089016 0.0048
            variance     SE
diabetes_dx   4759.9 0.0039
Effective sample size for diabetes_dx: 48142 

### Population characterisitcs

-   Age: Participants are fairly evenly distributed across adult age
    groups, with no sharp skewness.
-   Sex: sample includes a higher proportion of females than males.
-   BMI: Most participants have BMI values within the normal to
    overweight range, with fewer in the obese category.
-   BMI by Diabetes Status: Individuals diagnosed with diabetes tend to
    have higher BMI values compared to non-diabetics.
-   Diabetes Prevalence by Age Group: The proportion of diabetes
    increases with advancing age, highlighting age as a strong risk
    factor.
-   Diabetes Prevalence by Race/Ethnicity: Differences are observed
    across racial/ethnic groups, with some showing higher prevalence
    rates than others.

Visualization - Below are histogram, bar graph, boxplot to display age,
bmi and their association with diabetes status. Plot below shows males
and females with and without diabetes (including missing data) across
different racial groups. Bars are side by side for each sex, with counts
displayed on top

```{r}
#| label: Data exploration
#| include: false
# data exploration

if (sum(!is.na(adult$diabetes_dx)) == 0) {
  stop("Too few non-missing outcomes for modeling (n = 0). Check DIQ010 upstream.")
}

# (optional plots omitted for brevity)

# save for downstream
if (!dir.exists("data")) dir.create("data", recursive = TRUE)
saveRDS(adult, "data/adult_cleaned_2013_2014.rds")


```

```{r}
#| label: EDA visualization (adult dataset)

ggplot(adult, aes(x = age)) +
  geom_histogram(binwidth = 5, fill = "skyblue", color = "white") +
  labs(
    title = "Distribution of Age >20 years",
    x = "Age (years)",
    y = "Count"
  ) +
  theme_minimal()

ggplot(adult, aes(factor(diabetes_dx))) +
  geom_bar(fill = "steelblue") +
  labs(title="Diabetes Outcome Distribution in >20 years age group", x="diabetes_dx (0=No, 1=Yes)", y="Count")

ggplot(adult, aes(factor(bmi_cat))) +
  geom_bar(fill = "steelblue") +
  labs(title="Diabetes Outcome Distribution by BMI in >20 years age group", x="bmi_cat")

ggplot(adult, aes(x = factor(diabetes_dx), y = bmi)) +
  geom_boxplot(fill = "skyblue") +
  labs(
    title = "BMI Distribution by Diabetes Diagnosis in >20 years age group",
    x = "Diabetes Diagnosis (0 = No, 1 = Yes)",
    y = "BMI"
  ) +
  theme_minimal()

# plots for adult data bmi categories and race categories

ggplot(adult, aes(x = factor(race), fill = factor(diabetes_dx))) +
  geom_bar(position = "dodge") +
  labs(
    title = "Diabetes Diagnosis by Race in >20 years age group",
    x = "Race/Ethnicity",
    y = "Count",
    fill = "Diabetes Diagnosis\n(0 = No, 1 = Yes)"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

ggplot(adult, aes(x = factor(bmi_cat), fill = factor(diabetes_dx))) +
  geom_bar(position = "dodge") +
  labs(
    title = "Diabetes Diagnosis by BMI in >20 years age group",
    x = "BMI",
    y = "Count",
    fill = "Diabetes Diagnosis\n(0 = No, 1 = Yes)"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


```

```{r}

# Example: create your dataset
adult1 <- data.frame(
  race = rep(c("NH White","Mexican American","Other Hispanic","NH Black","Other/Multi"), each = 6),
  sex = rep(c("Male","Male","Male","Female","Female","Female"), times = 5),
  diabetes_dx = rep(c(0,1,NA,0,1,NA), times = 5),
  count = c(
    1019,119,38,1164,96,36,
    304,60,14,329,49,11,
    183,26,10,255,25,9,
    461,100,19,515,65,17,
    351,46,8,393,32,15
  )
)

# Clean NA for plotting or convert to "Missing"
adult1$diabetes_dx <- as.character(adult1$diabetes_dx)
adult1$diabetes_dx[is.na(adult1$diabetes_dx)] <- "Missing"

# Plot grouped bar chart
ggplot(adult1, aes(x = diabetes_dx, y = count, fill = sex)) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_wrap(~race) +
  labs(title = "Diabetes Diagnosis by Sex and Race",
       x = "Diabetes Diagnosis",
       y = "Count") +
  theme_minimal() +
  scale_fill_manual(values = c("skyblue", "orange"))

```

## Abnormalities detected in Adult dataset

### Missingness

-   Only 1.3% of individual data points are missing across the dataset,
    reflecting minimal missingness.
-   No column is entirely missing (0%), indicating all variables have at
    least some observed data.
-   Overall missingness: \~4% → low, but non-trivial given the small
    number of variables involved.
-   Missingness is not completely at random (MNAR or MAR) - If the
    probability of missingness depends on other observed variables
    (e.g., older adults missing BMI due to illness), imputation helps
    reduce bias. It is possible and should consider MICE and test with
    logistic regression of missingness indicators
-   Missingness affects outcome or key covariates - Even small
    missingness in important variables can bias posterior estimates.
    Since BMI and diabetes are central we should perform MICE
-   Sufficient auxiliary variables available - MICE works best when you
    have other correlated variables to inform imputation (e.g., age,
    sex, race, WTMEC2YR).\
-   Bayesian model assumes complete data - Standard Bayesian logistic
    models (e.g., brms, rstanarm) cannot directly handle NAs — you must
    impute or model missingness.

# Statistical Modeling Approach

### Data Preparation and Survey Design Specification
-Survey design variables — primary sampling unit (SDMVPSU), strata (SDMVSTRA), and examination weights (WTMEC2YR) — were retained to account for the complex, multistage sampling of NHANES.
- A survey design object ensures a population-representative estimates and valid variance estimation.
- Frequentist Survey-Weighted Logistic Regression (Complete-Case Analysis) - fitted using only complete cases to assess the associations between diabetes status (binary outcome) and predictors such as BMI, age, sex, and race/ethnicity.
- Survey weights were applied to correct for unequal probabilities of selection and nonresponse, ensuring generalizability to the U.S. adult population.

### Handling Missing Data: Multivariate Imputation by Chained Equations (MICE)
- Performed to address missing values in BMI and other covariates. 
- Variable with missing data imputed conditionally on all others through iterative regression models.
- Multiple (m = 5–10) imputed datasets generated, were analyzed separately, and combined using Rubin’s rules to obtain pooled parameter estimates and standard errors.

### Bayesian Logistic Regression (Post-Imputation Analysis)
- Bayesian logistic regression model applied to the imputed datasets, incorporated prior distributions for regression coefficients and allowed direct estimation of posterior distributions, credible intervals, and posterior predictive checks.
- Bayesian inference provided a probabilistic interpretation of parameter uncertainty, complementing the frequentist findings.

### Model Validation and Interpretation
- Diagnostic checks performed below evaluate model convergence, goodness-of-fit, and predictive accuracy.

### Model Comparison
- The results from both frameworks (frequentist and Bayesian) were compared to ensure robustness of conclusions regarding predictors of diabetes.


```{r}
#| label: Modeling-Survey-weighted complete-case
#| include: false

# Modeling

library(broom)
library(mice)
library(brms)
library(posterior)
library(bayesplot)
library(knitr)

# --- Guardrails for modeling ---
n_outcome <- sum(!is.na(adult$diabetes_dx))
if (n_outcome == 0) stop("Too few non-missing outcomes for modeling. n = 0")

# Ensure factors and >=2 observed levels among complete outcomes
adult <- adult %>%
  dplyr::mutate(
    sex  = if (!is.factor(sex))  factor(sex)  else sex,
    race = if (!is.factor(race)) factor(race) else race
  )

if (nlevels(droplevels(adult$sex[!is.na(adult$diabetes_dx)]))  < 2)
  stop("sex has <2 observed levels after filtering; check data availability.")
if (nlevels(droplevels(adult$race[!is.na(adult$diabetes_dx)])) < 2)
  stop("race has <2 observed levels after filtering; check Data Prep.")

   #  Survey-weighted complete-case 
# Build a logical filter on the original adult data (same length as design$data)
keep_cc <- with(
  adult,
  !is.na(diabetes_dx) & !is.na(age_c) & !is.na(bmi_c) &
  !is.na(sex) & !is.na(race)
)

# Subset the survey design using the logical vector (same length as original)
des_cc <- subset(nhanes_design_adult, keep_cc)

# Corresponding complete-case data (optional)
cc <- adult[keep_cc, ] |> droplevels()
cat("\nComplete-case N for survey-weighted model:", nrow(cc), "\n")

print(table(cc$race))
print(table(cc$diabetes_dx))
print(table(cc$sex))

form_cc <- diabetes_dx ~ age_c + bmi_c + sex + race
svy_fit <- survey::svyglm(formula = form_cc, design = des_cc, family = quasibinomial())
summary(svy_fit)

```


```{r}
#| label: Survey weighted data results
#| echo: false

svy_or <- broom::tidy(svy_fit, conf.int = TRUE) %>%
  dplyr::mutate(OR = exp(estimate), LCL = exp(conf.low), UCL = exp(conf.high)) %>%
  dplyr::select(term, OR, LCL, UCL, p.value) %>%
  dplyr::filter(term != "(Intercept)")
knitr::kable(svy_or, caption = "Survey-weighted odds ratios (per 1 SD)")
```
### Summary 
Multiple Logistic Regression model (Survey weighted)
-   Identified several significant predictors of diabetes diagnosis after adjusting for
demographic and anthropometric factors.
-   Age (OR = 2.90, 95% CI: 2.60–3.24, p \< 0.001): Older adults had
    nearly three times higher odds of diabetes compared with younger
    participants, indicating a strong positive association between age
    and diabetes risk.
-   BMI (OR = 1.73, 95% CI: 1.58–1.89, p \< 0.001): Higher body mass
    index was significantly associated with increased odds of diabetes,
    confirming obesity as a key risk factor.
-   Sex (Female vs. Male: OR = 0.54, 95% CI: 0.45–0.65, p \< 0.001):
    Females had significantly lower odds of diabetes compared to males.
-   Race/Ethnicity: Mexican American (OR = 2.43, 95% CI: 1.86–3.18, p \<
    0.001) Other Hispanic (OR = 1.75, 95% CI: 1.24–2.47, p = 0.001)
    Non-Hispanic Black (OR = 1.98, 95% CI: 1.56–2.50, p \< 0.001)
    Other/Multi-racial (OR = 2.11, 95% CI: 1.56–2.85, p \< 0.001) All
    minority racial/ethnic groups had significantly higher odds of
    diabetes compared with the reference group (Non-Hispanic Whites).

Multivariate Imputation by Chained Equations (Pooled Logistic Regression)

-   We conducted MICE to manage missiging data as an alternative to the
    Bayesian Approach @JSSv045i03
-   Flatness of the density, heavy tails, non-zero peakedness, skewness
    and multimodality do not hamper the good performance of multiple
    imputation for the mean structure in samples n \> 400 even for high
    percentages (75%) of missing data in one variable @van2012flexible.
-   Multiple Imputation (MI) can be performed using mice package in R
-   Iterative mice imputes missing values of one variable at a time,
    using regression models based on the other variables in the dataset.
-   In the chain process, each imputed variable become a predictor for
    the subsequent imputation, and the entire process is repeated
    multiple times to create several complete datasets, each reflecting
    different possibilities for the missing data.

```{r}
#| label: Imputation 
#| echo: true
 
# ----- Multiple Imputation (predictors only) 
mi_dat <- adult %>%
  dplyr::select(diabetes_dx, age, bmi, sex, race, WTMEC2YR, SDMVPSU, SDMVSTRA)

meth <- mice::make.method(mi_dat)
pred <- mice::make.predictorMatrix(mi_dat)

# Do not impute outcome
meth["diabetes_dx"] <- ""
pred["diabetes_dx", ] <- 0
pred[,"diabetes_dx"] <- 1

# Imputation methods
meth["age"]  <- "norm"
meth["bmi"]  <- "pmm"
meth["sex"]  <- "polyreg"
meth["race"] <- "polyreg"

# Survey design vars as auxiliaries only
meth[c("WTMEC2YR","SDMVPSU","SDMVSTRA")] <- ""
pred[, c("WTMEC2YR","SDMVPSU","SDMVSTRA")] <- 1

glimpse(mi_dat)

imp <- mice::mice(mi_dat, m = 5, method = meth, predictorMatrix = pred, seed = 123)

```

Results: MICE (pooled imputed dataset): 
-  In the final analytic
dataset consisting of 5,769 participants with 8 variables, with missing
values were addressed using Multivariate Imputation by Chained Equations
(MICE). 
-  Five imputations were performed across five iterations each,
with BMI imputed conditionally based on other predictors (age, sex,
race, and diabetes status). 
-  The iterative process showed stable
convergence, indicating reliable estimation of missing BMI values for
subsequent survey-weighted and Bayesian modeling analyses.


```{r}
#| label: fit mutated model
#| echo: true

fit_mi <- with(imp, {
  age_c <- as.numeric(scale(age))
  bmi_c <- as.numeric(scale(bmi))
  glm(diabetes_dx ~ age_c + bmi_c + sex + race, family = binomial())
})
pool_mi <- pool(fit_mi)
summary(pool_mi)

## table 

mi_or <- summary(pool_mi, conf.int = TRUE, exponentiate = TRUE) %>%
  dplyr::rename(
    term = term, OR = estimate, LCL = `2.5 %`, UCL = `97.5 %`, p.value = p.value
  ) %>%
  dplyr::filter(term != "(Intercept)")
knitr::kable(mi_or, caption = "MI pooled odds ratios (per 1 SD)")

```

-   Age remained the strongest predictor of diabetes (OR = 2.90, 95% CI:
    2.60–3.24, p \< 0.001).
-   BMI continued to show a significant positive association (OR = 1.73,
    95% CI: 1.58–1.89, p \< 0.001).
-   Female sex was associated with lower odds of diabetes compared to
    males (OR = 0.54, 95% CI: 0.45–0.65, p \< 0.001).
-   Compared to Non-Hispanic Whites, higher odds of diabetes were
    observed among: Mexican Americans (OR = 2.43, 95% CI: 1.86–3.18)
    Other Hispanics (OR = 1.75, 95% CI: 1.24–2.47) Non-Hispanic Blacks
    (OR = 1.98, 95% CI: 1.56–2.50) Other/Multi-racial groups (OR = 2.11,
    95% CI: 1.56–2.85)
-   All associations were statistically significant (p \< 0.01).

```{r}
#| label: Imputed dataset
#| echo: true
library(gt)

# Bayesian Logistic Regression (formula weights) 
adult_imp1 <- complete(imp, 1) %>%
  dplyr::mutate(
    age_c  = as.numeric(scale(age)),
    bmi_c  = as.numeric(scale(bmi)),
    wt_norm = WTMEC2YR / mean(WTMEC2YR, na.rm = TRUE),
    # ensure factor refs match survey/mice:
    race = forcats::fct_relevel(race, "NH White"),
    sex  = forcats::fct_relevel(sex,  "Male")
  ) %>%
  dplyr::filter(!is.na(diabetes_dx), !is.na(age_c), !is.na(bmi_c),
                !is.na(sex), !is.na(race)) %>%
  droplevels()

stopifnot(all(is.finite(adult_imp1$wt_norm)))

glimpse(adult_imp1)

library(tableone)

vars <- c("age", "bmi", "age_c", "bmi_c", "wt_norm", "sex", "race", "diabetes_dx")

table1 <- CreateTableOne(vars = vars, data = adult_imp1, factorVars = c("sex", "race", "diabetes_dx"))
print(table1, showAllLevels = TRUE)

```


```{r}
#| label: Visulaiztion imputed dataset 

## correlation matrix
library(ggplot2)
library(reshape2)

correlation_matrix <- cor(adult_imp1[, c("diabetes_dx", "age", "bmi")], use = "complete.obs", method = "pearson")
correlation_melted <- melt(correlation_matrix)

ggplot(correlation_melted, aes(Var1, Var2, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0,
                       limit = c(-1, 1), space = "Lab", name = "Correlation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Correlation Heatmap", x = "Features", y = "Features")

```

### Visualization of imputed dataset
Pairwise correlations heatmap: show the
strength and direction of correlations (Pearson correlation) which
measures linear association between diabetes_dx, age, and bmi 

```{r}
#| label: tables_adult_imp1
#| echo: true

# Class distribution

ggplot(adult_imp1, aes(x = factor(diabetes_dx))) +
  geom_bar(fill = "steelblue") +
  labs(
    title = "Diabetes Diagnosis Distribution",
    x = "Diabetes Diagnosis (0 = No, 1 = Yes)",
    y = "Count"
  ) +
  theme_minimal()

prop.table(table(adult_imp1$diabetes_dx))

# Visualization of Diabetes vs BMI (adult_data1)

library(ggplot2)

# Create the plot
ggplot(adult_imp1, aes(x = factor(diabetes_dx), y = bmi, fill = factor(diabetes_dx))) +
  geom_boxplot(alpha = 0.7) +
  scale_x_discrete(labels = c("0" = "No Diabetes", "1" = "Diabetes")) +
  labs(
    x = "Diabetes Diagnosis",
    y = "BMI",
    title = "BMI Distribution by Diabetes Status"
  ) +
  theme_minimal() +
  theme(legend.position = "none")

# logistic regression curve
ggplot(adult_imp1, aes(x = bmi, y = diabetes_dx)) +
  geom_point(aes(y = diabetes_dx), alpha = 0.2, position = position_jitter(height = 0.02)) +
  geom_smooth(method = "glm", method.args = list(family = "binomial"), se = TRUE, color = "blue") +
  labs(
    x = "BMI",
    y = "Probability of Diabetes",
    title = "Predicted Probability of Diabetes vs BMI"
  ) +
  theme_minimal()

```

```{r}
#| label: saved data in csv
# Save your dataset as CSV
write.csv(adult_imp1, "adult_imp1.csv", row.names = FALSE)

```

After MICE, below are the number of rows and column of the three datasets created 

- Rows: 10175 and Columns: 10 (survey-weighted, merged data) 
- Rows: 5,769 and Columns: 12 (filtered data, adult)
- Rows: 5,592 and Columns: 11 (imputed data, adult_imp1)

# Bayesian Logistic Regression analysis on imputed dataset (adult_imp1)
1.  Model Overview 
    - A Bayesian logistic regression model was fitted on
    the first imputed dataset (adult_imp1) to assess predictors of
    diabetes diagnosis.
    - Survey weights-Normalized MEC exam weights (wt_norm) with mean 1.00 (SD
0.79) 
  - No missing values remain in selected predictors or outcome. 
  - Continuous variables
are standardized, which facilitates prior specification. 
  - Categorical
variables are correctly re-leveled for reference categories. 
  - Weights
are available for inclusion in the likelihood to account for survey
design.

2.  Prior Specification 
    - to stabilize estimation in the
    presence of correlated predictors or outliers while retaining
    interpretability of model parameters.
    - Intercept prior: student_t(3, 0, 10) — allowing heavy tails for
    flexibility in the intercept estimate. @VanDeSchoot2013
    - Regression coefficients prior: normal(0, 2.5) — providing weakly
    informative regularization provide gentle regularization,
    constraining extreme values without overpowering the data
    @VandeSchoot2021

3.  Model Estimation
  - The model estimates using four Markov Chain Monte Carlo (MCMC)
chains, each with 2000 iterations (50% warm-up), and an adaptive delta
of 0.95 ensure good chain convergence and reduce divergent
transitions. 
  - Posterior summaries represent the central tendency and
uncertainty around the model parameters through credible intervals
(CrI).

```{r}
#| label: adult_imp1 - Prior, Bayesian model and summary
library(gt)

priors <- c(
  set_prior("normal(0, 2.5)", class = "b"),
  set_prior("student_t(3, 0, 10)", class = "Intercept") 
)

bayes_fit <- brm(
  formula = diabetes_dx | weights(wt_norm) ~ age_c + bmi_c + sex + race,
  data    = adult_imp1,
  family  = bernoulli(link = "logit"),
  prior   = priors,
  chains  = 4, iter = 2000, seed = 123,
  control = list(adapt_delta = 0.95),
  refresh = 0   # quiet Stan output
)

prior_summary(bayes_fit)
summary(bayes_fit)            # Bayesian model summary

```

# Results and Visualization 
  - Age and BMI both were positively associated with
diabetes risk. 
  - Higher standardized values corresponded to higher
posterior odds of diabetes. 
  Sex: Females showed lower odds of diabetes
compared to males. 
  - Race/Ethnicity: Certain racial/ethnic groups
demonstrated reduced odds of diabetes compared to the Non-Hispanic White
reference group.

```{r}
#| label: Visualization of prior and adult_imp1 (bmi, age)

library(ggplot2)

# adult_imp1 plot 

# Convert to long format
adult_long <- adult_imp1 %>%
  select(bmi_c, age_c) %>%
  pivot_longer(cols = everything(), names_to = "Coefficient", values_to = "Value")

# Plot
ggplot(adult_long, aes(x = Value, fill = Coefficient)) +
  geom_density(alpha = 0.5) +
  theme_minimal() +
  labs(title = "Distributions for Coefficients from adult_imp1 data",
       x = "Coefficient Value", y = "Density") +
  scale_fill_manual(values = c("bmi_c" = "skyblue", "age_c" = "orange"))

## prior draws 

prior_draws <- tibble(
  term = rep(c("Age (per 1 SD)", "BMI (per 1 SD)"), each = 4000),
  value = c(rnorm(4000, 0, 2.5), rnorm(4000, 0, 2.5))
)

## Plot (prior) (age and bmi) 
ggplot(prior_draws, aes(x = value, fill = term)) +
  geom_density(alpha = 0.5) +
  theme_minimal() +
  labs(title = "Prior Distributions for Coefficients",
       x = "Coefficient Value", y = "Density") +
  scale_fill_manual(values = c("skyblue", "orange"))



```

### Visualization of Priors and Data Distributions
a.  Data-Derived Coefficient Distributions
- The density plots of standardized BMI and Age from the imputed dataset
(adult_imp1) show approximately normal distributions centered near zero,
consistent with z-score standardization confirming that both predictors
were properly centered and scaled prior to Bayesian modeling, ensuring
comparability and numerical stability during estimation.

b.  Prior Distributions
- Priors for regression coefficients were drawn from a Normal(0, 2.5)
distribution, representing weakly informative assumptions centered at
zero with moderate spread. The prior density plots for Age (per 1 SD)
and BMI (per 1 SD) demonstrate symmetric bell-shaped distributions,
indicating no strong bias toward positive or negative effects before
observing data.

# Predictive checking and validation (Bayesian model)
1.  Posterior Summaries 
-   Provides posterior mean, median, 95% credible intervals
-   Convergence diagnostics (R-hat, effective sample size)
-   plots to visualizes posterior distributions with high uncertainty, narrow distributions
    indicating precise estimates.

2.  Posterior Odds Ratios - provides interpretation of the model
    coefficients on a multiplicative scale.
-   odds ratios (OR = exp(Estimate)) with reference categories: NH White (race), Male
    (sex).
-   ORs with 95% credible intervals (CrI) allow direct
    interpretation of associations between predictors and outcome.

3.  Posterior Predictive Checks (PPC)
-   pp_check to assesses how the model reproduces observed data
    and validate model fit.
Visualizations of generated simulated datasets compared with the observed data.
show density overlays for both  mea and SD   
There was no large discrepancies indicating potential misfit; there was
good alignment suggesting reliable predictions.

4.  MCMC Convergence endures reliable posterior estimates.
-   MCMC Trace plots show chains for each parameter over iterations.
-   Well-mixed chains without trends indicate convergence and stable
    posterior estimates.

5.  Model Fit -provided details to quantify predictive performance.
-   The proportion of variance explained by the
    model: R² = 0.13 (13%) shows predictors are relevant but other factors (e.g.,
    genetics, lifestyle, environment) also contribute to outcome
    variability.

6.  Correlation and Parameter Relationships (Optional)
-   Pairwise plots (mcmc_pairs, posterior) – explore correlations between
    parameters.
-   Histograms or density plots mcmc_hist() or mcmc_areas() of
    specific parameters detects no collinearity or dependencies among predictors

```{r}
#| label: Assumptions (Bayesian)

library(brms)
summary(bayes_fit)
plot(bayes_fit)   # Posterior distributions
pp_check(bayes_fit)      # Posterior predictive checks
mcmc_trace(bayes_fit)    # Convergence (optional)
bayes_R2(bayes_fit)      # Model fit

```



```{r}
#| label: Posterior ORs and tables
#| echo: true
#| 
# Posterior ORs (drop intercept, clean labels)

bayes_or <- posterior_summary(bayes_fit, pars = "^b_") %>%
  as.data.frame() %>%
  tibble::rownames_to_column("raw") %>%
  dplyr::mutate(
    term = gsub("^b_", "", raw),
    term = gsub("race", "race:", term),
    term = gsub("sex",  "sex:",  term),
    term = gsub("OtherDMulti", "Other/Multi", term),
    term = gsub("OtherHispanic", "Other Hispanic", term),
    OR   = exp(Estimate),
    LCL  = exp(Q2.5),
    UCL  = exp(Q97.5)
  ) %>%
  dplyr::select(term, OR, LCL, UCL) %>%
  dplyr::filter(term != "Intercept")

knitr::kable(
  bayes_or %>%
    dplyr::mutate(dplyr::across(c(OR,LCL,UCL), ~round(.x, 2))),
  digits = 2,
  caption = "Bayesian posterior odds ratios (95% CrI) — reference: NH White (race), Male (sex)"
)

```

```{r}
#| include: false
# Combined table

if (!dir.exists("outputs")) dir.create("outputs", recursive = TRUE)
saveRDS(svy_fit,   "outputs/svy_fit.rds")
saveRDS(pool_mi,   "outputs/pool_mi.rds")
saveRDS(bayes_fit, "outputs/bayes_fit.rds")
saveRDS(svy_or,    "outputs/survey_OR_table.rds")
saveRDS(mi_or,     "outputs/mi_OR_table.rds")
saveRDS(bayes_or,  "outputs/bayes_OR_table.rds")
```

Compact table compare odds ratios (ORs) and 95% confidence/credible intervals (CIs) across three
models for BMI and Age 
-   Survey-weighted maximum likelihood estimation (MLE)
-   Multiple imputation pooled estimates (mice)
-   Bayesian Logistic Regression

```{r}
#| label: Model results OR
# Results

 #Build compact results table (BMI & Age only) 
library(dplyr); 
library(tidyr); 
library(knitr); 
library(stringr)

# pretty "OR (LCL–UCL)" string

  fmt_or <- function(or, lcl, ucl, digits = 2) {
  paste0(
    formatC(or,  format = "f", digits = digits), " (",
    formatC(lcl, format = "f", digits = digits), "–",
    formatC(ucl, format = "f", digits = digits), ")"
  )
}

# guardrails: require these to exist from Modeling
stopifnot(exists("svy_or"), exists("mi_or"), exists("bayes_or"))
for (nm in c("svy_or","mi_or","bayes_or")) {
  if (!all(c("term","OR","LCL","UCL") %in% names(get(nm)))) {
    stop(nm, " must have columns: term, OR, LCL, UCL")
  }
}

svy_tbl   <- svy_or   %>% mutate(Model = "Survey-weighted MLE")
mi_tbl    <- mi_or    %>% mutate(Model = "mice pooled")
bayes_tbl <- bayes_or %>% mutate(Model = "Bayesian")

all_tbl <- bind_rows(svy_tbl, mi_tbl, bayes_tbl) %>%
  mutate(term = case_when(
    str_detect(term, "bmi_c|\\bBMI\\b") ~ "BMI (per 1 SD)",
    str_detect(term, "age_c|\\bAge\\b") ~ "Age (per 1 SD)",
    TRUE ~ term
  )) %>%
  filter(term %in% c("BMI (per 1 SD)", "Age (per 1 SD)")) %>%
  mutate(OR_CI = fmt_or(OR, LCL, UCL, digits = 2)) %>%
  select(Model, term, OR_CI) %>%
  arrange(
    factor(Model, levels = c("Survey-weighted MLE","mice pooled","Bayesian")),
    factor(term,  levels = c("BMI (per 1 SD)","Age (per 1 SD)"))
  )

res_wide <- all_tbl %>%
  pivot_wider(names_from = term, values_from = OR_CI) %>%
  rename(
    `BMI (per 1 SD) OR (95% CI)` = `BMI (per 1 SD)`,
    `Age (per 1 SD) OR (95% CI)` = `Age (per 1 SD)`
  )

kable(
  res_wide,
  align = c("l","c","c"),
  caption = "Odds ratios (per 1 SD) with 95% CIs across models"
)

```

```{r}
#| label: post visualization for converence check

# Posterior predictive draws

#Posterior predictive checks (binary outcome)
pp_samples <- posterior_predict(bayes_fit, ndraws = 500)  # 500 draws

# Check dimensions
dim(pp_samples)  # rows = draws, cols = observations

# Plot overlay of observed vs predicted counts (duplicate image)
ppc_dens_overlay(y = adult_imp1$diabetes_dx, yrep = pp_samples[1:50, ]) +
  labs(title = "Posterior Predictive Check: Density Overlay") +
  theme_minimal()

ppc_bars(y = adult_imp1$diabetes_dx, yrep = pp_samples[1:50, ])

#PP check for proportions (useful for binary) mean and sd comparison to check if the simulated means match the observed mean

## mean
ppc_stat(y = adult_imp1$diabetes_dx, yrep = pp_samples[1:100, ], stat = "mean") +
  labs(title = "Posterior Predictive Check: Mean of Replicates") +
  theme_minimal()

## sd
ppc_stat(y = adult_imp1$diabetes_dx, yrep = pp_samples[1:100, ], stat = "sd") +
  labs(title = "PPC: Standard Deviation of Replicates") +
  theme_minimal()


```

```{r}
#| include: false

# PP checks with bayesplot options
color_scheme_set("blue")
ppc_scatter_avg(y = adult_imp1$diabetes_dx, yrep = pp_samples[1:100, ]) +
  labs(title = "Observed vs Predicted (Avg) Posterior Predictive")

```

## Comparative Visualizations

-   Predicted vs observed - to check how well the model’s predictions
    align with reality where mean(y_rep) = average predicted probability
    of diabetes for each individual, across all posterior draws of the
    parameters. y = the actual observed diabetes status (0 =
    non-diabetic, 1 = diabetic).
-   mcmc dens plots - compare observed and posterior parameter
    values (estimates) for bmi_c, age_c, sex_female, and by race
    categories
-   Fitted (Predicted) vs observed for bmi using point and error bars
-   Fitted (Predicted) vs observed for bmi using line plot

```{r}
#| label: mcmc, post vs obs
#| include: false
library(brms)
library(dplyr)

# Posterior summary
post_sum <- posterior_summary(bayes_fit)
colnames(post_sum)

library(posterior)
library(bayesplot)

# Extract posterior draws as a draws_df # simulate posterior outcomes
post <- as_draws_df(bayes_fit)

# Check parameter names
colnames(post)

# Density overlay for age and bmi
mcmc_areas(post, pars = c( "b_age_c","b_bmi_c","b_sexFemale","b_raceMexicanAmerican", "b_raceOtherHispanic","b_raceNHBlack","b_raceOtherDMulti" ))

predicted <- fitted(bayes_fit, summary = TRUE)
observed <- adult_imp1[, c("bmi", "age")]

# Plot for **bmi** (obs vs pred)

library(ggplot2)
ggplot(data = NULL, aes(x = observed$bmi, y = predicted[, "Estimate"])) +
  geom_point() +
  geom_errorbar(aes(ymin = predicted[, "Q2.5"], ymax = predicted[, "Q97.5"])) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  xlab("Observed bmi") + ylab("Predicted bmi")


```

### Comparative Visualizations for Model Assessment
Visualization shows comparative plots of observed and posterior-predicted values to evaluate the Bayesian model predicts of diabetes-related outcomes:

1.  Posterior Parameter Distributions to assess uncertainty and effect
    sizes of predictors, we extracted posterior draws using
    as_draws_df(bayes_fit).
-   Plotted density overlays (mcmc_areas) for key predictors: age, BMI,
    sex, and race categories: shows density plots with distribution of
    posterior estimates to visualize uncertainty and parameter
    magnitude.

2.  Predicted vs Observed Values to evaluate model fit by comparing
    predicted outcomes to actual data using fitted(bayes_fit, summary =
    TRUE).
-   Compared with the observed values for continuous predictors (e.g.,
    BMI and age).
-   Visualization in Scatter plots with point estimates and 95% credible
    intervals as error bars shows Line of perfect agreement (slope = 1,
    dashed red line) for reference.
-   Scatter and error bar plots indicate predicted BMI values align with
    observed BMI across individuals.
-   Good alignment along the 45° line suggests reliable predictions;
    deviations highlight areas where the model may under- or
    over-predict.
-   Overall, these visualizations complement posterior summaries and
    predictive checks, supporting model validation and interpretation.

### Predicted vs Observed BMI
To evaluate model fit at the individual level, we compared observed BMI
values to posterior-predicted BMI estimates from the Bayesian model:

1.  Data Preparation
A combined comaprative results from observed BMI (bmi) and predicted
posterior estimates (predicted_bmi) into a single dataset with 95%
credible intervals (lower_ci, upper_ci) from the posterior draws in a
Line plot of BMI over observations:

-   Observed BMI: solid line.
-   Predicted BMI: solid line of posterior means.
-   Shaded ribbon: 95% credible interval around predicted values to
    visualize uncertainty.

3.  Interpretation
-   Close alignment of predicted lines with observed BMI indicates good
    model fit.
-   Wider ribbons highlight greater posterior uncertainty for individual
    predictions.
-   Summary statistics for bmi and standardized bmi_c help contextualize
    the observed range and variability in the sample.

```{r}
#| label: pred vs obs

# Combine observed and predicted into one data frame
plot_data <- adult_imp1 %>%
  mutate(
    predicted_bmi = predicted[, "Estimate"],
    lower_ci = predicted[, "Q2.5"],
    upper_ci = predicted[, "Q97.5"],
    obs_index = 1:nrow(adult_imp1)  # index for x-axis
  )

# Line plot
ggplot(plot_data, aes(x = obs_index)) +
  geom_line(aes(y = bmi, color = "Observed")) +               # observed BMI
  geom_line(aes(y = predicted_bmi, color = "Predicted")) +   # predicted BMI
  geom_ribbon(aes(ymin = lower_ci, ymax = upper_ci), alpha = 0.2) +  # uncertainty
  labs(x = "Observation", y = "BMI", color = "Legend") +
  theme_minimal()

summary(adult_imp1$bmi)
summary(plot_data$bmi_c)

```

Summary of original bmi (observed data) and centered
version of BMI. 
  - Centering doesn’t change the distribution shape, only
shifts it so the mean is zero. 
  - Centering is useful in
regression/Bayesian models to improve numerical stability and
interpretability of intercepts 
  - Plots showing predicted values of bmi
and age (prior vs predicted) and the proportion of diabetes=1 for each
draw

### Visualization on Prior vs Posterior Distributions
-   To assess how the Bayesian model updates beliefs from prior
    information to posterior estimates, we compared prior vs posterior
    coefficient distributions for key predictors: BMI and age.

1.  Prior Draws
-   Simulated from a standard normal distribution (mean = 0, SD = 1) for
    both BMI and age coefficients. Represent initial beliefs about
    coefficient values before seeing the data.

2.  Posterior Draws
-   Extracted from the fitted model (bayes_fit) for b_bmi_c and b_age_c.
-   Pivoted to long format and labeled as "Posterior".

3.  Visualization Combined prior and posterior draws
-   Plotted density overlays with facets for BMI and age.
-   Posterior distributions are narrower and often shifted from prior,
    reflecting information gained from the data.
-   Differences between prior and posterior highlight the model’s
    learning about effect sizes.
-   Posterior Predictive Proportions of Diabetes
-   Computed the proportion of diabetes cases (diabetes = 1) for each
    posterior draw (pp_samples).

Interpretaion: 
  - Prior vs posterior plots demonstrate that the Bayesian
model updates prior beliefs in a data-informed way. 
  - Posterior
predictive proportions closely match observed prevalence, supporting
model reliability for inference and prediction.

```{r}
#| label: Prior vs pred draws (age and bmi)


prior_summary(bayes_fit)
prior_draws <- tibble(
  term = rep(c("BMI (per 1 SD)", "Age (per 1 SD)"), each = 4000),
  estimate = c(rnorm(4000, 0, 1), rnorm(4000, 0, 1)),
  type = "Prior"
)

post
head(post)

library(brms)
library(tidyr)

# Extract posterior draws as dataframe

post <- as_draws_df(bayes_fit) %>%           # bayes_fit = your brms model
  select(b_bmi_c, b_age_c) %>%               # select your coefficient columns
  pivot_longer(
    everything(),
    names_to = "term",
    values_to = "estimate"
  ) %>%
  mutate(
    term = case_when(
      term == "b_bmi_c" ~ "BMI (per 1 SD)",
      term == "b_age_c" ~ "Age (per 1 SD)"
    ),
    type = "Posterior"
  )

## visualization of prior and predicted draws
combined_draws <- bind_rows(prior_draws, post) 

library(ggplot2)

ggplot(combined_draws, aes(x = estimate, fill = type)) +
  geom_density(alpha = 0.4) +
  facet_wrap(~ term, scales = "free", ncol = 2) +
  theme_minimal(base_size = 13) +
  labs(
    title = "Prior vs Posterior Distributions",
    x = "Coefficient estimate",
    y = "Density",
    fill = ""
  )

# Compute proportion of diabetes=1 for each draw
pp_proportion <- rowMeans(pp_samples)  # proportion of 1's in each posterior draw

# Summary of posterior proportions
summary(pp_proportion)

# Optional: visualize the posterior probability distribution
pp_proportion_df <- tibble(proportion = pp_proportion)

ggplot(pp_proportion_df, aes(x = proportion)) +
  geom_histogram(binwidth = 0.01, fill = "skyblue", color = "black") +
  labs(
    title = "Posterior Distribution of Proportion of Diabetes = 1",
    x = "Proportion of Diabetes = 1",
    y = "Frequency"
  ) +
  theme_minimal()

svy_mean <- svymean(~diabetes_dx, nhanes_design_adult, na.rm = TRUE)


```

```{r}
#| echo: false
library(dplyr)
library(knitr)


# Create summary table
summary_table <- tibble(
  Method = c("Survey-weighted mean (NHANES)", 
             "Imputed dataset mean", 
             "Posterior predictive mean"),
  diabetes_mean = c(
    coef(svy_mean),           # survey-weighted mean
    mean(adult_imp1$diabetes_dx, na.rm = TRUE),  # imputed dataset
    mean(pp_proportion)       # posterior predictive mean
  ),
  SE = c(
    SE(svy_mean),             # survey-weighted SE
    NA,                       # not available for raw mean
    NA                        # not available for posterior predictive
  )
)

# Render table
kable(summary_table, digits = 4, caption = "Comparison of Diabetes Prevalence Across Methods")

```

## Comaprison of Prior and Predicted draws for both Age and BMI

-   Plot shows the posterior distributions are much more concentrated
    around \~0.5 (example) than the priors indicating the data provided
    strong evidence about the effect size of these covariates.
-   The priors were diffuse, showing initial uncertainty
-   The posteriors are precise, showing learning from the data.

## Propotion of diabetes in the posterior draws

-   States from the predicted probability of diabetes = 1 in the
    population (Bayesian model).
      -Min = 0.085 → In some posterior draws, only \~8.5% of the population
    is predicted to have diabetes. 
      -1st Quartile = 0.105 → 25% of posterior draws predict diabetes prevalence below 10.5%.
      -Median = 0.109 → Half of the simulated draws predict a prevalence
    below \~10.9%, half above.
      -Mean = 0.109 → The average predicted prevalence is \~10.9%, very
    close to the median → roughly symmetric distribution. 
      -3rd Quartile = 0.113 → 75% of draws predict prevalence below \~11.3%.
      -Max = 0.128 → The highest predicted prevalence across all draws is
    \~12.8%.

  - Bayesian model predicts that about 10–11% of this population has
diabetes, with a relatively narrow range across posterior draws,
reflects uncertainty in the estimate 
  - While most predictions cluster around 10–11%, the model allows for values as low as 8.5% and as high as
12.8%. 
  - On comparing this with the raw imputed data proportion show
that the the model predictions align with the observed/imputed data.

Clinical relevance of predicted proportion 
  - it accounts for
uncertainty in the model and imputed data. 
  - Policy makers or clinicians
could plan interventions accordingly anticipating \~1 in 10 adults in
this population might have diabetes.


```{r}
#| label: DM prediction vs prevalence

library(tidyverse)

# Posterior predicted proportion vector
# pp_proportion <- rowMeans(pp_samples)  # if not already done

known_prev <- 0.089   # NHANES prevalence

# Posterior summary
posterior_mean <- mean(pp_proportion)
posterior_ci <- quantile(pp_proportion, c(0.025, 0.975))  # 95% credible interval

# Create a data frame for plotting
pp_df <- tibble(proportion = pp_proportion)

# Plot
ggplot(pp_df, aes(x = proportion)) +
  geom_histogram(binwidth = 0.005, fill = "skyblue", color = "black") +
  geom_vline(xintercept = known_prev, color = "red", linetype = "dashed", size = 1) +
  geom_vline(xintercept = posterior_mean, color = "blue", linetype = "solid", size = 1) +
  geom_rect(aes(xmin = posterior_ci[1], xmax = posterior_ci[2], ymin = 0, ymax = Inf),
            fill = "blue", alpha = 0.1, inherit.aes = FALSE) +
  labs(
    title = "Posterior Predicted Diabetes Proportion vs NHANES Prevalence",
    subtitle = paste0("Red dashed = NHANES prevalence (", known_prev, 
                      "), Blue solid = Posterior mean (", round(posterior_mean,3), ")"),
    x = "Proportion of Diabetes = 1",
    y = "Frequency"
  ) +
  theme_minimal()

```

## Comparing proportion of Diabetes between Posterior predicted vs NHANES prevalence of Diabetes 
To evaluate the Bayesian model’s predictive accuracy for diabetes
prevalence, we compared the posterior predicted proportion of diabetes
cases to the known NHANES prevalence:
1.  Posterior Predictions - Calculated the proportion of diabetes = 1
    for each posterior draw (pp_proportion). Derived posterior mean and
    95% credible interval to summarize predictive uncertainty.
2.  Visualization - Histogram of posterior predicted proportions
    illustrates the variability in model predictions.

Red dashed line: NHANES observed prevalence (0.089). Blue solid line:
Posterior mean predicted prevalence. Shaded blue region: 95% credible
interval around the posterior mean.

Interpretation 
  - Close alignment of the posterior mean and credible
interval with the observed NHANES prevalence indicates that the model
accurately captures the population-level prevalence of diabetes. 
  - This
visualization complements prior vs posterior and predicted vs observed
checks, supporting overall model validity.

```{r}
#| label: posterior DM prediction vs population prevalence
library(dplyr)

# Posterior predicted proportion

posterior_mean <- mean(pp_proportion)
posterior_ci <- quantile(pp_proportion, c(0.025, 0.975))  # 95% credible interval

# NHANES prevalence with SE from survey::svymean
# Suppose you already have:
# svymean(~diabetes_dx, nhanes_design_adult, na.rm = TRUE)
known_prev <- 0.089        # Mean prevalence
known_se   <- 0.0048       # Standard error from survey

# Calculate 95% confidence interval
known_ci <- c(
  known_prev - 1.96 * known_se,
  known_prev + 1.96 * known_se
)

# Print results
data.frame(
  Type = c("Posterior Prediction", "NHANES Prevalence"),
  Mean = c(posterior_mean, known_prev),
  Lower_95 = c(posterior_ci[1], known_ci[1]),
  Upper_95 = c(posterior_ci[2], known_ci[2])
)

library(ggplot2)
library(dplyr)

# Create a data frame for plotting
ci_df <- data.frame(
  Type = c("Posterior Prediction", "NHANES Prevalence"),
  Mean = c(0.1096674, 0.089),
  Lower_95 = c(0.09772443, 0.079592),
  Upper_95 = c(0.1210658, 0.098408)
)

# Plot
ggplot(ci_df, aes(x = Type, y = Mean, color = Type)) +
  geom_point(size = 4) +
  geom_errorbar(aes(ymin = Lower_95, ymax = Upper_95), width = 0.2) +
  ylim(0, max(ci_df$Upper_95) + 0.02) +
  labs(
    title = "Comparison of Posterior Predicted Diabetes Proportion vs NHANES Prevalence",
    y = "Proportion of Diabetes",
    x = ""
  ) +
  theme_minimal(base_size = 14) +
  theme(legend.position = "none")


# --- Load libraries ---
library(survey)
library(tibble)
library(ggplot2)

# --- 1. Survey-weighted (Population) prevalence ---
pop_est <- svymean(~diabetes_dx, nhanes_design_adult, na.rm = TRUE)
pop_prev <- as.numeric(pop_est)
pop_se <- as.numeric(SE(pop_est))
pop_ci <- c(pop_prev - 1.96 * pop_se, pop_prev + 1.96 * pop_se)

# --- 2. Bayesian posterior prevalence ---
# bayes_pred = matrix of posterior draws (iterations × individuals)
pp_proportion <- rowMeans(pp_samples)             # prevalence per posterior draw
post_prev <- mean(pp_proportion)                  # posterior mean prevalence
post_ci <- quantile(pp_proportion, c(0.025, 0.975))  # 95% credible interval

# --- 3. Combine into one data frame ---
bar_df <- tibble(
  Source     = c("Survey-weighted (Population)", "Bayesian Posterior"),
  Prevalence = c(pop_prev, post_prev),
  CI_low     = c(pop_ci[1], post_ci[1]),
  CI_high    = c(pop_ci[2], post_ci[2])
)

# --- 4. Plot ---
ggplot(bar_df, aes(x = Source, y = Prevalence, fill = Source)) +
  geom_col(alpha = 0.85, width = 0.6) +
  geom_errorbar(
    aes(ymin = CI_low, ymax = CI_high),
    width = 0.15,
    color = "black",
    linewidth = 0.8
  ) +
  guides(fill = "none") +
  labs(
    title = "Population vs Posterior Diabetes Prevalence",
    subtitle = "Survey-weighted estimate (design-based) vs Bayesian (model-based)",
    y = "Prevalence (Proportion with Diabetes)",
    x = NULL
  ) +
  theme_minimal(base_size = 13)


```


1.  Survey-Weighted Prevalence (8.9%) - using svymean(\~diabetes_dx, nhanes_design_adult).
-   The mean prevalence and 95% confidence interval:
-   Mean = 0.089, SE = 0.0048 → 95% CI = 0.080–0.098.
-   - Representative estimate of diabetes prevalence in the population, as
    it adjusts for NHANES’ complex sampling design. It’s slightly lower
    because survey weights give less influence to overrepresented groups
    (e.g., those with higher diabetes prevalence).
    
2. Imputed (Unweighted) Prevalence (11.1%) - Reflects the diabetes proportion from the imputed dataset. It’s
    unweighted, it doesn’t correct for sampling bias, so it might
    overrepresent some subgroups (e.g., older or overweight
    participants).

2.  Posterior Predictive Mean (10.9%)
-   Bayesian model replicates the imputed data mean, suggesting good
    model calibration.
-   The model “learned” the sample pattern correctly — no over- or
    underestimation relative to the data.
-   It’s close to the imputed value but slightly below it shows the
    posterior distribution pulled toward the population-level mean,
    consistent with Bayesian shrinkage.
-   Summarized by posterior mean and 95% credible interval.
-   Posterior mean = 0.110, 95% CrI = 0.098–0.121.

Visualization 
-   Bar plots to compare population vs posterior prevalence with Error bars: 95% CI (survey) or 95% credible interval (Bayesian) 
-  Posterior mean slightly higher than survey-weighted
prevalence but largely overlaps with the population 95% CI. 
-  Indicates
that the Bayesian model reliably reproduces population-level diabetes
prevalence, supporting both predictive accuracy and model validity.


### Summarizing
Implications 
  - Health departments can estimate diabetes burden at the
state or county level using Bayesian small-area estimation. 
  - Clinicians
and public health researchers can plan targeted screening where
predicted prevalence is higher than observed. 
  - Epidemiologists can
validate disease models before applying them to regions without survey
data.

```{r}
#| label: autocorrelation_mcmc
#| include: false

colnames(post)

library(tidyr)
library(bayesplot)
library(posterior)

# Convert fitted model to draws array
post_array <- as_draws_array(bayes_fit)  # draws x chains x parameters

# Plot autocorrelation for age and bmi
mcmc_acf(post_array, pars = c("b_age_c", "b_bmi_c"))

```

## MCMC Autocorrelation for Key Parameters
-   We examined autocorrelation of MCMC chains for key predictors: age and BMI to evaluate the independence of posterior samples and ensure reliable Bayesian inference 
-   Plotted autocorrelation functions (mcmc_acf) for b_age_c and
    b_bmi_c show how each MCMC sample is
related to previous iterations. 
  - Low autocorrelation (quick decay to
zero) indicates good chain mixing and independent samples. 
  - High
autocorrelation suggests slower mixing and may require more iterations
or tuning.
-   Visual inspection of autocorrelation for age and BMI confirms
    adequate independence of posterior draws, supporting the reliability
    of parameter estimates and subsequent inference.
-   mcmc_acf() - produces autocorrelation plots for the posterior
    samples of specified parameters (b_age_c and b_bmi). 
    
# Model Overview and Significant Predictors

### a.  Multiple Linear Regression (Survey-weighted MLE)
Significant predictors: 
- Age: strong positive association (p \< 0.001)
- BMI: strong positive association (p \< 0.001) 
- Sex (female): negative
association (p = 0.0004) 
- Race/Ethnicity: Mexican American (p = 0.0008)
Other Hispanic (p = 0.0087) NH Black (p = 0.0117) Other/Multi (p =
0.0014)

### b.  Multiple Imputation (MICE)
- All predictors remain statistically significant. 
  - Positive associations:age, BMI, race categories. 
  - Negative association: female sex.

### c.  Bayesian Logistic Regression
- Sampling via NUTS: 4 chains × 2000 iterations (1000 warmup, 4000
post-warmup draws). 
- Convergence diagnostics: Rhat = 1.00 → excellent
convergence Bulk/Tail ESS \> 2000 → reliable posterior estimates
- Posterior R² = 0.13 (95% CrI: 0.106–0.156) → 13% of variance in diabetes
explained by predictors.

### Key Predictor Effects                                       
- predictor Effect Age (per 1 SD) Log-odds of diabetes ↑1.09 per unit increase; 
- strongest positive predictor BMI  (per 1 SD) - Higher BMI increases diabetes risk (\~1.7–1.9× odds per
    SD)
- Sex (female) Lower odds of diabetes compared to males
- Race/Ethnicity Mexican American, NH Black, Other/Multi:
    significantly higher odds; Other Hispanic: uncertain effect

### Posterior density plots to illustrate parameter uncertainty 
- Posterior predictive checks (PPC) simulate new datasets from posterior draws to
assess model fit. 
- Combining parameter uncertainty and predictive
uncertainty provides credible intervals for predictions (e.g., given
BMI, diabetes probability \~40–55%).

# Conclusion
  - Our model is reasonable but slightly conservative
(predicting higher risk) relative to the observed population prevalence.
  - Across multiple modeling approaches (survey-weighted maximum likelihood,
multiple imputation, and Bayesian regression) — both age and BMI were
consistently strong predictors of diabetes. 
  - Each standard deviation increase in age nearly tripled the odds of diabetes, while a similar
increase in BMI elevated the odds by approximately 1.7–1.9 times. The
consistency of these results across models highlights the robustness of
the associations and underscores the importance of age and BMI as key
risk factors for diabetes in this population.
  - Effect stability: point estimates in the Bayesian model’s closely
aligned with those from the frequentist, indicating that prior
regularization stabilized the estimates in the presence of modest
missingness.
  - Uncertainty quantification: Bayesian credible intervals of odds ration
were slightly narrower yet overlapped the frequentist confidence
intervals, suggest comparable inferential precision while offering
improved interpretability.

Design considerations: 
Survey-weighted Maximum Likelihood Estimator 
- incorporates each observation weighted according to its
survey weight. 
- provide estimates that reflect the population-level
parameters, not just the sample- produces population-representative
estimates. 

Bayesian model with normalized weights
 - instead of fully
modeling the survey design, it used normalized sampling weights as
importance weights 
 - the scaled weights that sum to the sample size
approximates the effect of survey weights, but does not fully account
for: Stratification, clustering, design-based variance adjustments.  
 -
Bayesian inference treats the weighted likelihood as from a regular
model, ignoring some survey design features.

# Discussions
-  The use of multiple imputation allowed for robust analysis despite
missing data, increasing power and reducing bias.
-  Comparison of
frequentist and Bayesian models demonstrated consistency in significant
predictors, while Bayesian approaches provided the advantage of
posterior distributions and probabilistic interpretation. 
 -  Across
all models, both age and BMI emerged as strong and consistent predictors
of diabetes. 
-  The consistency across modeling approaches strengthens the
validity of these findings Multiple imputation accounted for potential
biases due to missing data, and Bayesian modeling provided robust
credible intervals that closely matched frequentist estimates align
with previous epidemiological research indicating that increasing age
and higher BMI are among the most important determinants of type 2
diabetes risk.
-  Cumulative exposure to metabolic and lifestyle risk
factors over time, and the role of excess adiposity and insulin related
effects account for diabetes. 
-  Survey weighted dataset strenghthens
ensuring population representativeness, multiple imputation to handle
missing data, and rigorous Bayesian estimation provided high effective
sample sizes and R̂ ≈ 1.00 across parameters confirmed excellent model
convergence. 
-  Bayesian logistic regression provided inference
statistically consistent and interpretable achieving the aim of this
study. In future research hierarchical model using NHANES cycles or
adding variables (lab tests) could assess nonlinear effects of metabolic
risk factors.

# Limitations
1.  The study is based on cross-sectional/observational NHANES data,
    which limits the ability to make causal inferences. Associations
    observed between BMI, age, diabetes status cannot confirm causation.
2.  The project relies on multiple imputation for missing values, even
    though imputation reduces bias, it assumes missingness is at random
    (MAR); if data are missing not at random (MNAR), results may be
    biased.
3.  Potential Residual Confounding - Models included key predictors
    (age, BMI, sex, race), but unmeasured factors like diet, physical
    activity, socioeconomic status, or genetic predisposition could
    confound associations.
4.  Bayesian models depend on prior choices, which could influence
    posterior estimates if priors are informative or mis-specified.
5.  Variable Measurement - BMI is measured at a single time point, which
    may not reflect long-term exposure or risk.
6.  Self-reported variables - are subjective to recall or reporting
    bias.
7.  Interactions are not tested in the study (bmi × age) and so other
    potential synergistic effects might be missed.
8.  Predicted probabilities are model-based estimates, not validated for
    clinical decision-making. External validation in independent cohorts
    is needed before application.

#### Targeted therapy
-  Translational Perspective from the Bayesian Diabetes Prediction Project.
This project further demonstrates the translational potential of
Bayesian modeling in clinical decision-making and public health
strategy. 
-  By using patient-level predictors such as age, BMI, sex, and
race to estimate the probability of diabetes, the model moves beyond
descriptive statistics toward individualized risk prediction. 
-  The
translational move lies in converting these probabilistic outputs into
actionable thresholds—such as identifying the BMI or age at which the
predicted risk of diabetes exceeds a clinically meaningful level (e.g.,
30%). 
-  Such insights can guide early screening, personalized lifestyle
interventions, and targeted prevention programs for populations at
higher risk. 
-  This approach embodies precision public health—bridging
data science and medical decision-making to deliver tailored,
evidence-based strategies that can ultimately improve diabetes
prevention and management outcomes.

What changes in modifiable predictors would lower diabetes risk?

### Translational Research Implications: 
  - We can use the model to guide prevention or intervention.
  - Only BMI is a modifiable risk factor 
  - We can make changes in BMI (behavior or lifestyle) to achieve a
lower risk threshold 
  - we hold non modifiable predictors as
constant (sex, race). 
  - Vary modifiable predictors (BMI) until the model
predicts the desired probability.

### Internal validation
-   To illustrate personalized risk estimation using the Bayesian model,
    we computed the posterior predicted probability of diabetes for a
    representative participant.
-   We selected one participant from the dataset (adult\[1, \]) including
    all relevant covariates (age, BMI, sex, race).
-   Used posterior_linpred with transform = TRUE to obtain predicted
    probabilities for logistic regression.
-   Extracted posterior draws computed 95% credible interval from the posterior draws.
-   Density plot shows the distribution of plausible probabilities given
    the participant’s covariates.
  - The density highlights
uncertainty around the individual’s predicted diabetes risk. 
  - 95%
credible interval provides a range of probable outcomes, not just a
point estimate. 
  - This approach allows personalized risk assessment,
enabling clinicians or public health practitioners to identify high-risk individuals 
  - Tailor preventive interventions (e.g., lifestyle modification, monitoring) 
  - Quantify uncertainty in predictions for decision-making 
  - Posterior predictive distributions enable probabilistic, individualized
predictions, supporting targeted intervention strategies beyond
population-level summaries.

```{r}
#| label: participant1
#| echo: true

# Use the first participant 
# using multiple covariates to select someone
participant1_data  <- adult[1, ]


# predicted probabilities for patient 1
phat1 <- posterior_linpred(bayes_fit, newdata = participant1_data, transform = TRUE)
# 'transform = TRUE' gives probabilities for logistic regression

# Store in a data frame for plotting
post_pred_df <- data.frame(pred = phat1)

# Compute 95% credible interval
ci_95_participant1 <- quantile(phat1, c(0.025, 0.975))

# Plot

ggplot(post_pred_df, aes(x = pred)) + 
  geom_density(color='darkblue', fill='lightblue') +
  geom_vline(xintercept = ci_95_participant1[1], color='red', linetype='dashed') +
  geom_vline(xintercept = ci_95_participant1[2], color='red', linetype='dashed') +
  xlab('Probability of being diabetic (Outcome=1)') +
  ggtitle('Posterior Predictive Distribution 95% Credible Interval') +
  theme_bw()


```

```{r}
#| label: participant2
#| include: false

participant2_data  <- adult[2, ]


# predicted probabilities for patient 1
phat2 <- posterior_linpred(bayes_fit, newdata = participant2_data, transform = TRUE)
# 'transform = TRUE' gives probabilities for logistic regression

# Store in a data frame for plotting
post_pred_df2 <- data.frame(pred = phat2)

# Compute 95% credible interval
ci_95_participant2 <- quantile(phat2, c(0.025, 0.975))

# Plot

ggplot(post_pred_df2, aes(x = pred)) + 
  geom_density(color='darkblue', fill='lightblue') +
  geom_vline(xintercept = ci_95_participant2[1], color='red', linetype='dashed') +
  geom_vline(xintercept = ci_95_participant2[2], color='red', linetype='dashed') +
  xlab('Probability of being diabetic (Outcome=1)') +
  ggtitle('Posterior Predictive Distribution 95% Credible Interval') +
  theme_bw()

```

### External validation
- Predicting Diabetes Risk for a New Participant to demonstrate the
application of the Bayesian model for personalized prediction, we
applied the trained model to a new participant not included in the
original dataset.

-   Selected a new participant with specific covariates (age, BMI, sex,
    race).
-   Used posterior_linpred with transform = TRUE to compute posterior
    predicted probabilities of diabetes. Generated posterior draws to
    capture predictive uncertainty.
-   Created a density plot of predicted probabilities. Computed 95%
    credible interval to summarize the range of likely outcomes.
-   Red dashed lines indicate the lower and upper bounds of the
    interval.
-  The distribution shows not
only the most probable risk but also the uncertainty around it. 
  - Credible intervals help quantify confidence in individual-level
predictions. 
  - Supports personalized decision-making, such as targeted
lifestyle interventions, early monitoring, or preventive care. 
  -
Bayesian posterior predictive draws allow probabilistic, individualized
predictions for new participants, providing both point estimates and
uncertainty measures for actionable risk assessment.

```{r}
#| label: new participant
#| echo: true
library(ggplot2)

new_participant <- data.frame(
  age_c = 40,
  bmi_c = 25,
  sex   = "Female",
  race  = "Mexican American"
)

# Posterior predicted probabilities
phat_new <- posterior_linpred(bayes_fit, newdata = new_participant, transform = TRUE)

# Convert to numeric vector
phat_vec <- as.numeric(phat_new)

# Check the range to see if all values are similar
range(phat_vec)

# Store in a data frame
post_pred_df_new <- data.frame(pred = phat_vec)

# Compute 95% credible interval from the vector
ci_95_new_participant <- quantile(phat_vec, c(0.025, 0.975))

# Plot
ggplot(post_pred_df_new, aes(x = pred)) + 
  geom_density(color='darkblue', fill='lightblue', alpha = 0.6) +
  geom_vline(xintercept = ci_95_new_participant[1], color='red', linetype='dashed') +
  geom_vline(xintercept = ci_95_new_participant[2], color='red', linetype='dashed') +
  xlim(0, 1) +  # ensures you see the curve even if values are close
  xlab('Probability of being diabetic (Outcome=1)') +
  ggtitle('Posterior Predictive Distribution (95% Credible Interval)') +
  theme_bw()

```

### To estimate Targeted BMI for Predicted Diabetes Risk
-   To analyze the relationship between BMI and the predicted
probability of diabetes, holding other covariates (age, sex, race)
constant, via fitted Bayesian logistic regression model, we generated a grid of BMI values (e.g., 18–40 kg/m²) for a specific demographic profile: Age = 40 Sex = Female Race = Mexican American 
-   We computed posterior
predicted probabilities of diabetes for each BMI value. 
-   Averaged across
posterior draws to obtain the mean predicted probability per BMI.
-   Target Probability Approach Defined a target probability of diabetes
    (e.g., 0.3). Identified the BMI value whose predicted probability is
    closest to the target. This enables inverse prediction, linking
    statistical inference to clinically meaningful thresholds.
-   Visualization Line plot of predicted probability vs BMI shows 
    - Red dashed
    horizontal line: target probability (0.3). 
    - Red dotted vertical line:
    BMI corresponding to the target probability (\~closest BMI).Annotated to highlight the BMI threshold.
-   Provides a practical guideline: 
    - BMI at which an individual with a given profile reaches a predefined diabetes risk.
    - Supports personalized risk communication and preventive
    interventions. 
    - Translates model output into actionable, clinically
    relevant thresholds, bridging research findings with public health
    application. 
    - This approach demonstrates how Bayesian posterior
    predictions can be used for targeted, individualized risk
    assessment, informing precision prevention strategies based on
    modifiable risk factors like BMI.
    
### Clinical Implications
-   age and BMI as robust and independent predictors of diabetes,
    underscore the importance of early targeted interventions in
    mitigating diabetes risk.
-   Longitudinal studies and combining other statistical analytical
    methods with Bayesian can further enhance and provide better
    informed precision prevention strategies.

```{r}
#| label: predict BMI_targeted therapy
#| include: false

# Grid of possible BMI values (centered if model used bmi_c)
bmi_seq <- seq(18, 40, by = 0.5)

newdata_grid <- data.frame(
  age_c = 40,
  bmi_c = bmi_seq,
  sex   = "Female",
  race  = "Mexican American"
)

# Posterior mean predicted probabilities
pred_probs <- posterior_linpred(bayes_fit, newdata = newdata_grid, transform = TRUE)
# Average over posterior draws to get the mean predicted probability per BMI
prob_mean <- colMeans(pred_probs)

# Combine with BMI values
pred_df <- cbind(newdata_grid, prob_mean)

target_prob <- 0.3
closest <- pred_df[which.min(abs(pred_df$prob_mean - target_prob)), ]

closest

ggplot(pred_df, aes(x = bmi_c, y = prob_mean)) +
  geom_line(color = "darkblue", linewidth = 1.2) +
  geom_hline(yintercept = target_prob, color = "red", linetype = "dashed") +
  geom_vline(xintercept = closest$bmi_c, color = "red", linetype = "dotted") +
  annotate("text", x = closest$bmi_c, y = target_prob + 0.05,
           label = paste0("Target BMI ≈ ", round(closest$bmi_c, 1)),
           color = "red", hjust = -0.1) +
  labs(
    x = "BMI (centered or raw, depending on model)",
    y = "Predicted Probability of Diabetes",
    title = "Inverse Prediction: BMI Needed for Target Diabetes Risk"
  ) +
  theme_bw()

```


## References
