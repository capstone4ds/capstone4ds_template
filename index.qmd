---
title: "Diagnosing diseases using KNN- Spring 2025"
subtitle: "An application of KNN to diagnose disease"
author: "Jacqueline Razo (Advisor: Dr. Cohen)"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

Slides: [slides.html](slides.html){target="_blank"} ( Go to `slides.qmd`
to edit)


## Introduction

kNN is an algorithm that is being used in a variety of fields to classify or predict data. The kNN algorithm is a simple algorithm that classifies data based on how similar a datapoint is to a class of datapoints. One of the benefits of using this algorithmic model is how simple it is to use and the fact it’s non-parametric which means it fits a wide variety of datasets. One drawback from using this model is that it does have a higher computational cost than other models which means that it doesn’t perform as well or fast on big data. Despite this, the model’s simplicity makes it easy to understand and easy to implement in a variety of fields. One such field is the field of healthcare where kNN models have been successfully used to predict diseases such as diabetes and hypertension. In this paper we will focus on the methodology and application of kNN models in the field of healthcare to predict diabetes, a pressing public health problem. 

Machine learning is a promising field where computers are able to learn from prior data to perform a function such as regression, classification or even create new values or images. KNNs have emerged as easy to understand algorithms that are able to perform regression, classification, data imputation and many other uses. 
KNNs are supervised learning algorithms that work by comparing a data point to other similar data points in order to label it. It works on the assumption that data points that are similar to each other must be in close proximity to each other. In the thesis [@zhang2016introduction], the author gave the reader an introduction into how KNN works and how to run a KNN model in R studio. He describes the methodology as assigning an unlabeled observation to a class by using labeled examples that are similar to it. It also describes the euclidean distance equation which is the default distance equation that is used for KNNs. The author also describes the impact the k parameter has on the algorithm. The K parameter is the parameter that tells the model how many neighbors it will use when trying to classify a datapoint. Zhang recommends setting the k parameter equal to the square root of the number of observations in the training dataset. 

Zhang’s recommendation to set the k parameter could be a great starting point but the thesis [@zhang2017efficient] mentions how important the k parameter can be in the accuracy of the model. The authors of this thesis propose using a training stage where we use a decision tree to select the ideal number of K values and thus make the KNN more efficient. The authors deployed and tested two more efficient knn methods called kTree and the k*Tree methods. They found their method did reduce running cost and increased classification accuracy. 

Another big impact on accuracy is the distance the model uses to classify neighbors. Although the euclidean distance is the default distance that is used in kNNs there are other distances that can be used. In the thesis [@kataria2013review] the authors compare different distances in classification algorithms with a focus on the kNN algorithm. It starts off explaining how the KNN algorithm uses the nearest k-neighbors in order to classify data points and then describes how the euclidean distance does this by putting a line segment between point a and point b and then measuring the distance using the euclidean distance formula. It moves on to describe the “cityblock” or taxican distance and describes it as “the sum of the length of the projections of the line segment”. It also describes the cosine distance and the correlation distance and then compares the performance of the default euclidean distance to the performance of using city block, cosine and correlation distances. In the end it found the euclidean distance was more efficient than the others in their observations. 

Figuring out the distance can be a computationally expensive endeavor with bigger datasets. This can be a problem when we enter the world of big data where datasets can get very big and computationally prohibitive. Researchers are already trying to solve this problem by trying to make kNN’s more efficient. In the thesis [@deng2016efficient] the authors propose using a k-means clustering technique to separate big data dataset into several parts to make the KNN more efficient. This paper is so important because the traditional method of using the euclidean distance with big datasets to see what the k neighbors are can be time consuming and computationally expensive. After several experiments the authors concluded the LC-KNN they proposed worked the best because it had the highest accuracy and best time.[@deng2016efficient] 

The advances in kNN research has led to more optimized models that can be accurate enough to be used in the field of healthcare. The thesis [@ali2020diabetes] discusses the use of different types of KNN in order to classify diabetes. The authors were able to successfully identify diabetes by using matlab and six different types of KNN algorithms to classify glucose concentrations in the blood. The KNN algorithm took the closest K number of neighbors and assigned the class based on the nearest neighbors. Out of the six algorithms that were used the fine KNN was the one that was the most successful at correctly classifying the data. [@ali2020diabetes]

Another promising paper by  Saxena et al [@saxena2014diagnosis], showed the authors diagnosing diabetes mellitus using a KNN. The paper described diabetes mellitus and the variety of health problems it causes. This paper describes what diabetes mellitus is and the health problems it causes in individuals. The algorithm is described as taking a sample dataset with input variables and one output variable, taking a test dataset from the sample, finding the distance between each dataset in the training sample with the euclidean formula, then deciding the random variable of k where k is the number of nearest neighbors you will use to classify. Then it does the same thing to the test dataset and uses the K nearest members to find the output variable. After the classification, the authors go into detail of how they analyzed the results based on specificity. Finally, the paper describes the way they used mathlab to check for the accuracy of their results. The accuracy for a K of 3 was 70% and the accuracy for a K of 5 was 75%. 

## Methods

-   Detail the models or algorithms used.

-   Justify your choices based on the problem and data.

*The common non-parametric regression model is*
$Y_i = m(X_i) + \varepsilon_i$*, where* $Y_i$ *can be defined as the sum
of the regression function value* $m(x)$ *for* $X_i$*. Here* $m(x)$ *is
unknown and* $\varepsilon_i$ *some errors. With the help of this
definition, we can create the estimation for local averaging i.e.*
$m(x)$ *can be estimated with the product of* $Y_i$ *average and* $X_i$
*is near to* $x$*. In other words, this means that we are discovering
the line through the data points with the help of surrounding data
points. The estimation formula is printed below [@R-base]:*

$$
M_n(x) = \sum_{i=1}^{n} W_n (X_i) Y_i  \tag{1}
$$$W_n(x)$ *is the sum of weights that belongs to all real numbers.
Weights are positive numbers and small if* $X_i$ *is far from* $x$*.*


*Another equation:*

$$
y_i = \beta_0 + \beta_1 X_1 +\varepsilon_i
$$

## Analysis and Results

### Data Exploration and Visualization

-   Describe your data sources and collection process.

-   Present initial findings and insights through visualizations.

-   Highlight unexpected patterns or anomalies.

A study was conducted to determine how...

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
```

```{r, warning=FALSE, echo=TRUE}
# Load Data
kable(head(murders))

ggplot1 = murders %>% ggplot(mapping = aes(x=population/10^6, y=total)) 

  ggplot1 + geom_point(aes(col=region), size = 4) +
  geom_text_repel(aes(label=abb)) +
  scale_x_log10() +
  scale_y_log10() +
  geom_smooth(formula = "y~x", method=lm,se = F)+
  xlab("Populations in millions (log10 scale)") + 
  ylab("Total number of murders (log10 scale)") +
  ggtitle("US Gun Murders in 2010") +
  scale_color_discrete(name = "Region")+
      theme_bw()
  

```

### Modeling and Results

-   Explain your data preprocessing and cleaning steps.

-   Present your key findings in a clear and concise manner.

-   Use visuals to support your claims.

-   **Tell a story about what the data reveals.**

```{r}

```

### Conclusion

-   Summarize your key findings.

-   Discuss the implications of your results.

## References
