---
title: "Bayesian Logistic Regression - Application in Probability Prediction of disease (Diabetes) "
subtitle: "CapStone Project_2025"
author: "Namita Mishra, Autumn Wilcox, Ecil Teodoro (Advisor: Dr. Ashraf Cohen)"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

Slides: [slides.html](slides.html){target="_blank"} ( Go to `slides.qmd`
to edit)

------------------------------------------------------------------------

## Group Project Workflow and Contributions

- Autumn Wilcox ‚Äì Contributed to analytic coding, content draft, structured the project workflow, and collaborated actively via GitHub.
- Ecil Teodoro ‚Äì Drafted and submitted sections of the Introduction.
- Namita Mishra ‚Äì Developed the project plan, content draft, analytic coding, and coordinated commits and collaboration with the group on GitHub.

Advisor: Dr. Ashraf Cohen ‚Äì Provided expert guidance on statistical methods and workflow management across platforms.

## Introduction

Diabetes mellitus (DM) is a major public health concern closely associated with factors such as obesity, age, race, and gender.
Identifying these associated risk factors is essential for targeted interventions @DAngelo2025. **Logistic Regression** (traditional) that estimates the association between risk factors and outcomes is insufficient in analyzing the complex
healthcare data (DNA sequences, imaging, patient-reported outcomes,
electronic health records (EHRs), longitudinal health measurements,
diagnoses, and treatments. @Zeger2020. Classical maximum likelihood
estimation (MLE) yields unstable results in samples that are small, have missing
data, or presents quasi- and complete separation.

Bayesian hierarchical models using Markov Chain Monte Carlo (MCMC) allow analysis of multivariate longitudinal healthcare data with repeated measures within individuals and individuals nested in a population. By integrating prior knowledge and including exogenous (e.g., age, clinical history) and endogenous (e.g., current treatment) covariates, Bayesian models provide posterior distributions and risk predictions for conditions such as pneumonia, prostate cancer, and mental disorders. Parametric assumptions remain a limitation of these models.

In Bayesian inference @Chatzimichail2023, Bayesian inference has shown that parametric models (with fixed parameters) often underperform compared to nonparametric models, which do not assume a prior distribution. Posterior probabilities from Bayesian approaches improve disease classification and better capture heterogeneity in skewed, bimodal, or multimodal data distributions. Bayesian nonparametric models are flexible and robust, integrating multiple diagnostic tests and priors to enhance accuracy and precision, though reliance on prior information and restricted access to resources can limit applicability. Combining Bayesian methods with other statistical or computational approaches helps address systemic biases, incomplete data, and non-representative datasets.

The Bayesian framework described by @VandeSchoot2021 highlights the role of priors, data modeling, inference, posterior sampling, variational inference, and variable selection.Proper variable selection mitigates multicollinearity, overfitting, and limited sampling, improving predictive performance. Priors can be informative, weakly informative, or diffuse, and can be elicited from expert opinion, generic knowledge, or data-driven methods. Sensitivity analysis evaluates the alignment of priors with likelihoods, while MCMC simulations (e.g., brms, blavaan in R) empirically estimate posterior distributions. Spatial and temporal Bayesian models have applications in large-scale cancer genomics, identifying molecular interactions, mutational signatures, patient stratification, and cancer evolution, though temporal autocorrelation and subjective prior elicitation can be limiting.

Bayesian normal linear regression has been applied in metrology for instrument calibration using conjugate Normal‚ÄìInverse-Gamma priors  @Klauenberg2015. Hierarchical priors add flexibility by modeling uncertainty across multiple levels, improving robustness and interpretability. Bayesian hierarchical/meta-analytic linear regression incorporates both exchangeable and unexchangeable prior information, addressing multiple testing challenges, small sample sizes, and complex relationships among regression parameters across studies  @DeLeeuw2012a 

**A sequential clinical reasoning model** @Liu2013 Sequential clinical reasoning models demonstrate screening by adding predictors stepwise: (1) demographics, (2) metabolic components, and (3) conventional risk factors, incorporating priors and mimicking clinical evaluation. This approach captures ecological heterogeneity and improves baseline risk estimation, though interactions between predictors and external cross-validation remain limitations.

**Bayesian multiple imputation with logistic regression** addresses
missing data in clinical research @Austin2021 in clinical research by classifying missing values (e.g., patient refusal, loss to follow-up, mechanical errors) as MAR, MNAR, or MCAR. Multiple imputation generates plausible values across datasets and pools results for reliable classification of patient health status and mortality.

**Aims**

The study aims to apply Bayesian logistic regression to predict diabetes
status and to evaluate the associations between body mass index (BMI),
age (‚â•20 years), gender, and race as predictors, using a retrospective
dataset from the 2013‚Äì2014 NHANES survey. NHANES employs a complex
sampling design, including stratification, clustering, and oversampling
of specific population subgroups, rather than uniform random sampling. A
Bayesian analytical approach is used to address challenges posed by
dataset anomalies such as missing data, complete case analysis, and
separation that limit the efficiency and reliability of traditional
logistic regression in predicting health outcomes.

## Method and Data Preparation

In this study, Bayesian logistic regression is applied to predict diabetes status and estimate the association of key predictors including body mass index (BMI), age, gender, and race using the 2013‚Äì2014 NHANES dataset. The complex survey design of NHANES, includes stratification, clustering, and oversampling, was accounted for to ensure accurate estimation. Posterior predictive distributions are generated to quantify the probability of diabetes for each individual, providing a flexible and robust framework for uncertainty quantification. Model predictions are also compared against known population prevalence, enabling validation and assessment of model realism. By integrating prior information with observed data, this approach identifies individuals at high risk for diabetes and informs targeted public health interventions, demonstrating the utility of Bayesian methods in analyzing complex, multivariate health data.

**Statistical Tool**: we use R, R packages and libraries to import data,
perform data wrangling and analysis.

**Data source:** NHANES 2-year data is a cross-sectional weighted data
(2013-2014 year) @CenterforHealthStatistics1999. Three datasets
(demographics, exam, questionnaire) imported (Haven package to coverted
.XPT files in R to dataframe (df)) and after selecting variables of
interest a merged dataset was created.

**Modeling** - We conduct bayesian logisitic regression to estimate the
association between BMI, age, sex, and race/ethnicity and predict
doctor-diagnosed diabetes (`DIQ010`).

```{r}
#| label: Libraries
#| echo: true


# loading packages 
options(repos = c(CRAN = "https://cloud.r-project.org"))
install.packages("nhanesA")
library ("nhanesA")    

library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
library(Hmisc)
library(dplyr)
library(tidyr)
library(forcats)
library(ggplot2)
library(classpackage)
library(janitor)
install.packages("gt")   
library(gt)
library(survey)
library(DataExplorer)
library(logistf)


```

**Data pre-processing and cleaning** Subsets are created from the
original weighted datasets (demographics, exam, questionnaire) with
selected variables are merged using ID to create a single dataframe.

1.  Response Variable: Binary, **type 2 / diagnosed diabetes**
    (excluding gestational diabetes) -‚ÄúDoctor told you have diabetes?‚Äù
    DIQ010 combined with `DIQ050` a **secondary variable** describing
    **treatment status (insulin use) to exclude those cases**
2.  Predictor Variables (Body Mass Index, factor, 4 levels are analyzed
    after standardization).\
    o Underweight (\<5th percentile)\
    o Normal (5th‚Äì\<85th)\
    o Overweight (85th‚Äì\<95th) o Obese (‚â•95th percentile)\
    o Missing We kept it as it is as categorization provides clinically
    interpretable groups
3.  Covariates:
    1.  Gender (factor, 2 levels): Male: Female
    2.  Ethnicity (factor, 5 levels): Mexican American, Non-Hispanic,
        White Non-Hispanic, Black Other Hispanic, Other Race - Including
        multi-racial
    3.  Age (number, continuous)

```{r message=FALSE, warning=FALSE}
#| label: nhanes tables and merged data
#| include: false

            

library ("nhanesA")     
 # imported datasets 
                       nhanesTables('EXAM', 2013)
                       nhanesTables('QUESTIONNAIRE', 2013)
                       nhanesTables('DEMOGRAPHICS', 2013)
         
                                     
 # codebook for variable details

nhanesCodebook("DEMO_H",'RIDRETH1')
nhanesCodebook("DEMO_H",'RIAGENDR')
nhanesCodebook("DEMO_H",'RIDAGEYR')
nhanesCodebook("DIQ_H","DIQ010, DIQ050")
nhanesCodebook("BMX_H",'BMDBMIC')

  #  .xpt files read ( 2013‚Äì2014)                      
                      bmx_h <- nhanes("BMX_H")         #Exam
                      demo_h <- nhanes("DEMO_H")       #Demo
                      diq_h <- nhanes("DIQ_H")         #diabetes


# variables of interest

library(dplyr)

exam_sub <- bmx_h %>% 
  select(SEQN, BMDBMIC) %>%
  rename(
    ID = SEQN,
    BMI = BMDBMIC
  )


need_demo <- c("SEQN","RIDAGEYR","RIAGENDR","RIDRETH1","SDMVPSU","SDMVSTRA","WTMEC2YR")
stopifnot(all(c("SEQN","BMXBMI") %in% names(bmx_h)))
stopifnot(all(need_demo %in% names(demo_h)))
if (!("DIQ010" %in% names(diq_h))) {
  stop("DIQ010 is not in DIQ_H. Check the cycle name 'DIQ_H' and nhanesA version.")
}


      # ---- Select only needed variables ----
exam_sub <- bmx_h  %>% select(SEQN, BMXBMI)
demo_sub <- demo_h %>% select(all_of(need_demo))
diq_sub  <- diq_h  %>% select(SEQN, DIQ010, dplyr::any_of("DIQ050"))

# merged dataframe

merged_data <- demo_sub %>%
  left_join(exam_sub, by = "SEQN") %>%
  left_join(diq_sub,  by = "SEQN")

names(merged_data)
saveRDS(merged_data, "data/nhanes2013_2014_prepared.rds")


```

Merged dataset created and cleaned. Exploratory data analysis results and visualization presented.


```{r}
#| label: data structure and missing data
#| echo: true

# weighted means of each variable                       
str(merged_data)
plot_str(merged_data)
introduce(merged_data)
glimpse(merged_data)

p1 <- plot_intro(merged_data, title="Figure 1 (Merged dataset). Structure of variables and missing observations.")


p2 <- plot_missing(merged_data, title="Figure 2(Merged dataset). Breakdown of missing observations.")


# Save it as a PNG file
ggsave("Figure1_MergedDataset.png", plot = p1, width = 8, height = 6, dpi = 300)


ggsave("Figure2_MergedDataset.png", plot = p2, width = 8, height = 6, dpi = 300)

```

```{r message=FALSE, warning=FALSE}
#| label: merged dataframe and character handling
#| echo: true


# print(glimpse(merged_data))
print(table(merged_data$BMDBMIC, useNA = "ifany"))
print(table(merged_data$DIQ010,  useNA = "ifany"))

# ---- Coercion helpers (handle labelled/character) ----
to_num <- function(x) {
  if (is.numeric(x)) return(x)
  xc <- as.character(x)
  n <- suppressWarnings(readr::parse_number(xc))
  if (mean(is.na(n)) > 0.80) {
    xlow <- tolower(trimws(xc))
    n <- dplyr::case_when(
      xlow %in% c("1","yes","yes, told") ~ 1,
      xlow %in% c("2","no","no, not told") ~ 2,
      xlow %in% c("3","borderline") ~ 3,
      xlow %in% c("7","refused") ~ 7,
      xlow %in% c("9","don't know","dont know","unknown") ~ 9,
      TRUE ~ NA_real_
    )
  }
  as.numeric(n)
}

merged_data <- merged_data %>%
  mutate(
    DIQ010   = to_num(DIQ010),
    DIQ050   = to_num(if (!"DIQ050" %in% names(.)) NA_real_ else DIQ050),
    BMXBMI   = suppressWarnings(as.numeric(BMXBMI)),
    RIDAGEYR = suppressWarnings(as.numeric(RIDAGEYR)),
    RIAGENDR = suppressWarnings(as.numeric(RIAGENDR)),
    RIDRETH1 = suppressWarnings(as.numeric(RIDRETH1)),
    SDMVPSU  = suppressWarnings(as.numeric(SDMVPSU)),
    SDMVSTRA = suppressWarnings(as.numeric(SDMVSTRA)),
    WTMEC2YR = suppressWarnings(as.numeric(WTMEC2YR))
  )

# ---- Diagnostics BEFORE save ----
cat("DIQ010 counts BEFORE save:\n")
print(table(merged_data$DIQ010, useNA = "ifany"))
cat("Count with DIQ010 in {1,2}:", sum(merged_data$DIQ010 %in% c(1,2), na.rm = TRUE), "\n")

# ---- Save to file for reuse ----
dir.create("data", showWarnings = FALSE)
# ---- Save ----
dir.create("data", showWarnings = FALSE, recursive = TRUE)
saveRDS(merged_data, "data/merged_2013_2014.rds")
message("Saved: data/merged_2013_2014.rds")


```

Exploratior data analysis
-   Used library(survey) to get weighted means and sd of the variables.
    The BMI and age were standardized.
-   Age was recoded into different variables, including only \>20 years
    in the analysis.
-   BMI is recoded and categorized
    as-"18.5,18.5‚Äì\<25,25‚Äì\<30,30‚Äì\<35,35‚Äì\<40,‚â•40 years).
-   Ethnicity is recoded as "Mexican American" = "1", "Other Hispanic" =
    "2", "NH White" = "3", "NH Black" = "4", "Other/Multi" = "5"
-   Since special codes are not random, cannot be dropped; the
    informative missingness if ignored (MAR or MNAR) could introduce
    bias. We transformed special codes (3,7,) to NA and included all NAs
    in the analysis. Visulaization of missing data presented below.
-   A final analytic dataset was created ('adult') with "NH White" and
    "Male" as the reference group

```{r}
#| label: Basic Exploration (adults)
## 
# ---------------- Basic Exploration (adults) ----------------

# Keep adults only and build analysis variables
adult <- merged_data %>%
  dplyr::filter(RIDAGEYR >= 20) %>%
  dplyr::transmute(
    # --- keep survey design variables so svydesign() can see them ---
    SDMVPSU, SDMVSTRA, WTMEC2YR,

    # --- outcome: DIQ010 (1 yes, 2 no; 3/7/9 -> NA) ---
    diabetes_dx = dplyr::case_when(
      DIQ010 == 1 ~ 1,
      DIQ010 == 2 ~ 0,
      DIQ010 %in% c(3, 7, 9) ~ NA_real_,
      TRUE ~ NA_real_
    ),

    # --- predictors (raw) ---
    bmi  = BMXBMI,
    age  = RIDAGEYR,

    # sex (1=Male, 2=Female)
    sex  = forcats::fct_recode(factor(RIAGENDR), Male = "1", Female = "2"),

    # race (5-level)
    race = forcats::fct_recode(
      factor(RIDRETH1),
      "Mexican American" = "1",
      "Other Hispanic"   = "2",
      "NH White"         = "3",
      "NH Black"         = "4",
      "Other/Multi"      = "5"
    ),

    # keep DIQ050 so we can safely reference it (may be absent/NA in some rows)
    
    DIQ050 = DIQ050
  ) %>%
  # standardize continuous predictors
  dplyr::mutate(
    age_c = as.numeric(scale(age)),
    bmi_c = as.numeric(scale(bmi)),
    bmi_cat = cut(
      bmi,
      breaks = c(-Inf, 18.5, 25, 30, 35, 40, Inf),
      labels = c("<18.5","18.5‚Äì<25","25‚Äì<30","30‚Äì<35","35‚Äì<40","‚â•40"),
      right = FALSE
    )
  ) %>%
  # adjust outcome: if female & DIQ050==1 ("only when pregnant"), set to 0 (not diabetes)
  dplyr::mutate(
    diabetes_dx = ifelse(sex == "Female" & !is.na(DIQ050) & DIQ050 == 1, 0, diabetes_dx)
  )

# Make NH White the reference level for race (clearer interpretation)
adult <- adult %>%
  dplyr::mutate(
    race = forcats::fct_relevel(race, "NH White")
  )

# --- sanity checks ---
cat("Adults n =", nrow(adult), "\n")

glimpse(adult)

```
Adult dataset and variables

Observations - 5769 observations 
Survey design: SDMVPSU, SDMVSTRA, WTMEC2YR
Outcome: diabetes_dx (numeric 0/1)
Covariates: bmi, age, sex, race, DIQ050
Centered covariates: age_c, bmi_c
BMI categories: bmi_cat


```{r}
#| label: Data exploration
# data exploration

print(table(adult$diabetes_dx, useNA = "ifany"))
print(table(adult$sex, useNA = "ifany"))
print(table(adult$race, useNA = "ifany"))

if (sum(!is.na(adult$diabetes_dx)) == 0) {
  stop("Too few non-missing outcomes for modeling (n = 0). Check DIQ010 upstream.")
}

# (optional plots omitted for brevity)

# save for downstream
if (!dir.exists("data")) dir.create("data", recursive = TRUE)
saveRDS(adult, "data/adult_cleaned_2013_2014.rds")

```

```{r}
#| label: survey design
# survey design
# ---------------- Survey Design ----------------
# Use exam weights because BMI (BMXBMI) is an MEC variable
nhanes_design_adult <- survey::svydesign(
  id = ~SDMVPSU,
  strata = ~SDMVSTRA,
  weights = ~WTMEC2YR,
  nest = TRUE,
  data = adult
)

# quick weighted checks
survey::svymean(~age, nhanes_design_adult, na.rm = TRUE)
survey::svymean(~diabetes_dx, nhanes_design_adult, na.rm = TRUE)

# Calculate effective sample size for diabetes
# ------------------------------
# Variance ignoring survey design (i.e., assuming SRS)
v <- svyvar(~diabetes_dx, nhanes_design_adult, na.rm = TRUE)
p <- mean(adult$diabetes_dx, na.rm = TRUE)
v_srs <- p * (1 - p) / nrow(adult)

# Design effect = actual variance / SRS variance
deff <- v / v_srs
deff  # design effect
n_total <- sum(weights(nhanes_design_adult))
ess <- n_total / deff
cat("Effective sample size for diabetes_dx:", round(ess), "\n")


```

```{r}
#| label: EDA visualization

ggplot(adult, aes(x = age)) +
  geom_histogram(binwidth = 5, fill = "skyblue", color = "white") +
  labs(
    title = "Distribution of Age",
    x = "Age (years)",
    y = "Count"
  ) +
  theme_minimal()

ggplot(adult, aes(factor(diabetes_dx))) +
  geom_bar(fill = "steelblue") +
  labs(title="Diabetes Outcome Distribution", x="Outcome (0=No, 1=Yes)", y="Count")


ggplot(adult, aes(factor(bmi_cat))) +
  geom_bar(fill = "steelblue") +
  labs(title="Diabetes Outcome Distribution", x="bmi_cat")


ggplot(adult, aes(x = factor(diabetes_dx), y = bmi)) +
  geom_boxplot(fill = "skyblue") +
  labs(
    title = "BMI Distribution by Diabetes Diagnosis",
    x = "Diabetes Diagnosis (0 = No, 1 = Yes)",
    y = "BMI"
  ) +
  theme_minimal()

# plots for adult data bmi categories and race categories

ggplot(adult, aes(x = factor(race), fill = factor(diabetes_dx))) +
  geom_bar(position = "dodge") +
  labs(
    title = "Diabetes Diagnosis by Race",
    x = "Race/Ethnicity",
    y = "Count",
    fill = "Diabetes Diagnosis\n(0 = No, 1 = Yes)"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))



ggplot(adult, aes(x = factor(bmi_cat), fill = factor(diabetes_dx))) +
  geom_bar(position = "dodge") +
  labs(
    title = "Diabetes Diagnosis by Race",
    x = "Race/Ethnicity",
    y = "Count",
    fill = "Diabetes Diagnosis\n(0 = No, 1 = Yes)"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))



```

**Statistical Methods**

## Modeling

**Multiple Logistic regression** on survey weighted dataset
-We conducted frequentist method **Multiple Logistic regression** on a
survey-weighted dataset, for complete case analysis and data exploration

**Multivariate Imputation by Chained Equations (MICE)** - We then
conducted MICE to manage missiging data - Considering the small sample
size, Multivariate Imputation by Chained Equations (MICE) was conducted
as an alternative to the Bayesian Approach @JSSv045i03
-   Multiple imputation (MI) is an alternative analytic approach for
    small dataset with missingness.
-   Flatness of the density, heavy tails, non-zero peakedness, skewness
    and multimodality do not hamper the good performance of multiple
    imputation for the mean structure in samples n \> 400 even for high
    percentages (75%) of missing data in one variable
    @van2012flexible.
-   Multiple Imputation (MI) can be performed using mice package in R
-   Iterative MICE imputes missing values of one variable at a time,
    using regression models based on the other variables in the dataset.
-   In the chain process, each imputed variable become a predictor for
    the subsequent imputation, and the entire process is repeated
    multiple times to create several complete datasets, each reflecting
    different possibilities for the missing data.

**Bayesian Logistic Regression**

Bayesian statistics is about updating beliefs with evidence:
Posterior ‚àù Likelihood √ó Prior
-   Prior (p(Œ∏)): Your initial belief about a parameter before seeing
    the data.
-   Likelihood (p(y\|Œ∏)): How probable the observed data are given the
    parameters. This is derived from the model (e.g., logistic
    regression likelihood).
-   Posterior (p(Œ∏\|y)): Your updated belief about the parameter after
    seeing the data.
-   We selected Bayesian Logistic Regression since our study response
    variable is a binary outcome (Diabetes:yes/no)
-   Bayesian Logistic Regression is based on binomial probability Bayes'
    rules, and predicts probability of disease outcome
-   Bayes analyzes linear relation between the predictor (Age, Race,
    BMI, Gender) and outcome response variable (Diabetes).
-   it considers that predictors and response variables are independent.
-   Regression a of a discrete variable (0 or 1) is a Bernoulli
    probability model that classifies categorical response variables -
    predicting Diabetes.
-   Logit link provides probabilities for the response variable.
-   We use Weakly informative priors Normal (0, 2.5) for logistic
    regression coefficients and intercept, Normal(0, 10), allows a wide
    range of baseline log-odds and helps with convergence and avoids
    extreme estimates. Good default for most applications in social,
    health, or epidemiological studies.
-   In Bayesian statistics, every unknown parameter (like a regression
    coefficient, mean, or variance) is treated as a random variable with
    a probability distribution that reflects uncertainty.

Summaries od Bayesian regression include - 
-Posterior Predictive Probabilities
-Posterior Mean, Median, credible Intervals
-Posterior Probability (Outcome=1)
-Comparison with External Prevalence (population prevalence)
-Posterior Model Fit Metrics
-Prior versus Posterior Coefficient Distributions
-Posterior Predictive Checks
-Uncertainty Quantification 

## Model Equation

**Bayesian Logistic Regression model**

$$ \text{logit}(P(Y_i=1)) = \beta_0 + \beta_1 \cdot Age_i + \beta_2 \cdot BMI_i + \beta_3 \cdot Race_i + \beta_4 \cdot Gender_i $$

**Linear Regression equation:**

$$ y_i = \beta_0 + \beta_1 X_1 +\varepsilon_i $$

**Diagnostics for Bayesian Logictic Regression**

Trace Plots for Markov Chain Monte Carlo Convergence
Autocorrelation Plots
Potential Scale Reduction Factor Assessment
Posterior Predictive Checks
Residual Analysis
Bayes Factor Comparison
Prior Sensitivity Analysis
Model Fit Assessment
Posterior Predictive Probability Plots
Posterior Interval Coverage Evaluation
Convergence Diagnostics across Chains

```{r}
#| label: modeling-Survey-weighted complete-case 

# Modeling

library(broom)
library(mice)
library(brms)
library(posterior)
library(bayesplot)
library(knitr)

# --- Guardrails for modeling ---
n_outcome <- sum(!is.na(adult$diabetes_dx))
if (n_outcome == 0) stop("Too few non-missing outcomes for modeling. n = 0")

# Ensure factors and >=2 observed levels among complete outcomes
adult <- adult %>%
  dplyr::mutate(
    sex  = if (!is.factor(sex))  factor(sex)  else sex,
    race = if (!is.factor(race)) factor(race) else race
  )

if (nlevels(droplevels(adult$sex[!is.na(adult$diabetes_dx)]))  < 2)
  stop("sex has <2 observed levels after filtering; check data availability.")
if (nlevels(droplevels(adult$race[!is.na(adult$diabetes_dx)])) < 2)
  stop("race has <2 observed levels after filtering; check Data Prep.")

# ------------------------- 1) Survey-weighted complete-case -------------------------
# Build a logical filter on the original adult data (same length as design$data)
keep_cc <- with(
  adult,
  !is.na(diabetes_dx) & !is.na(age_c) & !is.na(bmi_c) &
  !is.na(sex) & !is.na(race)
)

# Subset the survey design using the logical vector (same length as original)
des_cc <- subset(nhanes_design_adult, keep_cc)

# Corresponding complete-case data (optional)
cc <- adult[keep_cc, ] |> droplevels()
cat("\nComplete-case N for survey-weighted model:", nrow(cc), "\n")

print(table(cc$race))
print(table(cc$diabetes_dx))
print(table(cc$sex))

form_cc <- diabetes_dx ~ age_c + bmi_c + sex + race
svy_fit <- survey::svyglm(formula = form_cc, design = des_cc, family = quasibinomial())
summary(svy_fit)

plot(residuals(svy_fit, type='deviance'))



# Survey-weighted OR table (no intercept)
svy_or <- broom::tidy(svy_fit, conf.int = TRUE) %>%
  dplyr::mutate(OR = exp(estimate), LCL = exp(conf.low), UCL = exp(conf.high)) %>%
  dplyr::select(term, OR, LCL, UCL, p.value) %>%
  dplyr::filter(term != "(Intercept)")
knitr::kable(svy_or, caption = "Survey-weighted odds ratios (per 1 SD)")



```

The residual plot looks okay and does not show any pattern.

```{r}
#| label: MICE
#| include: false
 
# ------------------------- 2) Multiple Imputation (predictors only) 
mi_dat <- adult %>%
  dplyr::select(diabetes_dx, age, bmi, sex, race, WTMEC2YR, SDMVPSU, SDMVSTRA)

meth <- mice::make.method(mi_dat)
pred <- mice::make.predictorMatrix(mi_dat)

# Do not impute outcome
meth["diabetes_dx"] <- ""
pred["diabetes_dx", ] <- 0
pred[,"diabetes_dx"] <- 1

# Imputation methods
meth["age"]  <- "norm"
meth["bmi"]  <- "pmm"
meth["sex"]  <- "polyreg"
meth["race"] <- "polyreg"

# Survey design vars as auxiliaries only
meth[c("WTMEC2YR","SDMVPSU","SDMVSTRA")] <- ""
pred[, c("WTMEC2YR","SDMVPSU","SDMVSTRA")] <- 1


glimpse(mi_dat)


imp <- mice::mice(mi_dat, m = 5, method = meth, predictorMatrix = pred, seed = 123)


```


```{r}
#| label: fit mutated model
#| echo: true


fit_mi <- with(imp, {
  age_c <- as.numeric(scale(age))
  bmi_c <- as.numeric(scale(bmi))
  glm(diabetes_dx ~ age_c + bmi_c + sex + race, family = binomial())
})
pool_mi <- pool(fit_mi)
summary(pool_mi)

## table 

mi_or <- summary(pool_mi, conf.int = TRUE, exponentiate = TRUE) %>%
  dplyr::rename(
    term = term, OR = estimate, LCL = `2.5 %`, UCL = `97.5 %`, p.value = p.value
  ) %>%
  dplyr::filter(term != "(Intercept)")
knitr::kable(mi_or, caption = "MI pooled odds ratios (per 1 SD)")



```




```{r}
#| label: adult_imp1 - Bayesian model and summary
library(gt)

# 3) Bayesian Logistic Regression (formula weights) 
adult_imp1 <- complete(imp, 1) %>%
  dplyr::mutate(
    age_c  = as.numeric(scale(age)),
    bmi_c  = as.numeric(scale(bmi)),
    wt_norm = WTMEC2YR / mean(WTMEC2YR, na.rm = TRUE),
    # ensure factor refs match survey/MICE:
    race = forcats::fct_relevel(race, "NH White"),
    sex  = forcats::fct_relevel(sex,  "Male")
  ) %>%
  dplyr::filter(!is.na(diabetes_dx), !is.na(age_c), !is.na(bmi_c),
                !is.na(sex), !is.na(race)) %>%
  droplevels()

stopifnot(all(is.finite(adult_imp1$wt_norm)))


glimpse(adult_imp1)


priors <- c(
  set_prior("normal(0, 2.5)", class = "b"),
  set_prior("student_t(3, 0, 10)", class = "Intercept")
)

bayes_fit <- brm(
  formula = diabetes_dx | weights(wt_norm) ~ age_c + bmi_c + sex + race,
  data    = adult_imp1,
  family  = bernoulli(link = "logit"),
  prior   = priors,
  chains  = 4, iter = 2000, seed = 123,
  control = list(adapt_delta = 0.95),
  refresh = 0   # quiet Stan output
)

summary(bayes_fit)            # Bayesian model summary

prior_summary(bayes_fit)



```


```{r}
#| label: tables_adult_imp1
#| eval: false
#| include: false

# Class distribution

ggplot(adult_imp1, aes(x = factor(diabetes_dx))) +
  geom_bar(fill = "steelblue") +
  labs(
    title = "Diabetes Diagnosis Distribution",
    x = "Diabetes Diagnosis (0 = No, 1 = Yes)",
    y = "Count"
  ) +
  theme_minimal()

prop.table(table(adult_imp1$diabetes_dx))

# Summaries of numeric predictors
numeric_vars <- sapply(adult_imp1, is.numeric)
summary(adult_imp1[, numeric_vars])

# Summaries of factor predictors
factor_vars <- sapply(adult_imp1, is.factor)
summary(adult_imp1[, factor_vars])


```



```{r}
#| label: Visualization prior, bmi, age
###
library(ggplot2)

# priors for two coefficients (age and bmi) prior = N(0,2.5)
prior_draws <- tibble(
  term = rep(c("Age (per 1 SD)", "BMI (per 1 SD)"), each = 4000),
  value = c(rnorm(4000, 0, 2.5), rnorm(4000, 0, 2.5))
)

ggplot(prior_draws, aes(x = value, fill = term)) +
  geom_density(alpha = 0.5) +
  theme_minimal() +
  labs(title = "Prior Distributions for Coefficients",
       x = "Coefficient Value", y = "Density") +
  scale_fill_manual(values = c("skyblue", "orange"))

# Diabetes vs BMI

library(ggplot2)

# Create the plot
p3 <- ggplot(adult_imp1, aes(x = factor(diabetes_dx), y = bmi, fill = factor(diabetes_dx))) +
  geom_boxplot(alpha = 0.7) +
  scale_x_discrete(labels = c("0" = "No Diabetes", "1" = "Diabetes")) +
  labs(
    x = "Diabetes Diagnosis",
    y = "BMI",
    title = "BMI Distribution by Diabetes Status"
  ) +
  theme_minimal() +
  theme(legend.position = "none")

# Save the plot as an image file (PNG)
ggsave("BMI_Distribution_by_Diabetes_Status.png", plot = p3, width = 7, height = 5, dpi = 300)


# logistic regression curve
p4 <- ggplot(adult_imp1, aes(x = bmi, y = diabetes_dx)) +
  geom_point(aes(y = diabetes_dx), alpha = 0.2, position = position_jitter(height = 0.02)) +
  geom_smooth(method = "glm", method.args = list(family = "binomial"), se = TRUE, color = "blue") +
  labs(
    x = "BMI",
    y = "Probability of Diabetes",
    title = "Predicted Probability of Diabetes vs BMI"
  ) +
  theme_minimal()

ggsave("Predicted Probability of Diabetes vs BMI.png", plot = p4, width = 7, height = 5, dpi = 300)


```

Once we get posterior draws, we study Summary stats Mean, median, 95%
credible intervals summary(bayes_fit) or posterior_summary(bayes_fit) -
Visualization Distribution shape mcmc_hist(posterior, pars = c("b_age"))
or mcmc_areas() - Pairwise plots Correlations between parameters
mcmc_pairs(posterior) - Posterior predictive checks Compare model
predictions vs observed data pp_check(bayes_fit) - Model comparison
Using LOO or WAIC loo(bayes_fit) or waic(bayes_fit)

Assumptions for Bayesian logistic regression - posterior check - plots
for linearity - mcmc trace plots for convergence - bayes_R2 for model
fit

```{r}
#| label: Assumptions (Bayesian)

summary(bayes_fit)
p5 <- plot(bayes_fit)   # Posterior distributions
             

p6 <- pp_check(bayes_fit)      # Posterior predictive checks
mcmc_trace(bayes_fit)    # Convergence (optional)
bayes_R2(bayes_fit)      # Model fit
    # Leave-one-out cross-validation

ggsave("bayes_fit_plot.png", plot = p5, width = 8, height = 6, dpi = 300)
ggsave("pp_check.png", plot = p6, width = 8, height = 6, dpi = 300)


```

Estimated R¬≤ = 0.13 (13.1%) - Predictors are relevant, most of the variability remains unexplained. Other factors (like genetics, lifestyle, environment, etc.) might be contributing.
Below are reported odds ratio from the posterior predicted values and
the Bayesian regression summary

```{r}
#| label: Posterior ORs and tables
#| echo: true
#| 
# Posterior ORs (drop intercept, clean labels)

bayes_or <- posterior_summary(bayes_fit, pars = "^b_") %>%
  as.data.frame() %>%
  tibble::rownames_to_column("raw") %>%
  dplyr::mutate(
    term = gsub("^b_", "", raw),
    term = gsub("race", "race:", term),
    term = gsub("sex",  "sex:",  term),
    term = gsub("OtherDMulti", "Other/Multi", term),
    term = gsub("OtherHispanic", "Other Hispanic", term),
    OR   = exp(Estimate),
    LCL  = exp(Q2.5),
    UCL  = exp(Q97.5)
  ) %>%
  dplyr::select(term, OR, LCL, UCL) %>%
  dplyr::filter(term != "Intercept")

knitr::kable(
  bayes_or %>%
    dplyr::mutate(dplyr::across(c(OR,LCL,UCL), ~round(.x, 2))),
  digits = 2,
  caption = "Bayesian posterior odds ratios (95% CrI) ‚Äî reference: NH White (race), Male (sex)"
)

```


```{r}
# Combined table

if (!dir.exists("outputs")) dir.create("outputs", recursive = TRUE)
saveRDS(svy_fit,   "outputs/svy_fit.rds")
saveRDS(pool_mi,   "outputs/pool_mi.rds")
saveRDS(bayes_fit, "outputs/bayes_fit.rds")
saveRDS(svy_or,    "outputs/survey_OR_table.rds")
saveRDS(mi_or,     "outputs/mi_OR_table.rds")
saveRDS(bayes_or,  "outputs/bayes_OR_table.rds")
```




```{r}
#| label: Model results OR
# Results

# ---- Build compact results table (BMI & Age only) ----
library(dplyr); 
library(tidyr); 
library(knitr); 
library(stringr)

# pretty "OR (LCL‚ÄìUCL)" string

fmt_or <- function(or, lcl, ucl, digits = 2) {
  paste0(
    formatC(or,  format = "f", digits = digits), " (",
    formatC(lcl, format = "f", digits = digits), "‚Äì",
    formatC(ucl, format = "f", digits = digits), ")"
  )
}

# guardrails: require these to exist from Modeling
stopifnot(exists("svy_or"), exists("mi_or"), exists("bayes_or"))
for (nm in c("svy_or","mi_or","bayes_or")) {
  if (!all(c("term","OR","LCL","UCL") %in% names(get(nm)))) {
    stop(nm, " must have columns: term, OR, LCL, UCL")
  }
}

svy_tbl   <- svy_or   %>% mutate(Model = "Survey-weighted MLE")
mi_tbl    <- mi_or    %>% mutate(Model = "MICE pooled")
bayes_tbl <- bayes_or %>% mutate(Model = "Bayesian")

all_tbl <- bind_rows(svy_tbl, mi_tbl, bayes_tbl) %>%
  mutate(term = case_when(
    str_detect(term, "bmi_c|\\bBMI\\b") ~ "BMI (per 1 SD)",
    str_detect(term, "age_c|\\bAge\\b") ~ "Age (per 1 SD)",
    TRUE ~ term
  )) %>%
  filter(term %in% c("BMI (per 1 SD)", "Age (per 1 SD)")) %>%
  mutate(OR_CI = fmt_or(OR, LCL, UCL, digits = 2)) %>%
  select(Model, term, OR_CI) %>%
  arrange(
    factor(Model, levels = c("Survey-weighted MLE","MICE pooled","Bayesian")),
    factor(term,  levels = c("BMI (per 1 SD)","Age (per 1 SD)"))
  )

res_wide <- all_tbl %>%
  pivot_wider(names_from = term, values_from = OR_CI) %>%
  rename(
    `BMI (per 1 SD) OR (95% CI)` = `BMI (per 1 SD)`,
    `Age (per 1 SD) OR (95% CI)` = `Age (per 1 SD)`
  )

kable(
  res_wide,
  align = c("l","c","c"),
  caption = "Odds ratios (per 1 SD) with 95% CIs across models"
)


```

```{r}
#| label: post visualization
# Posterior predictive draws

#Posterior predictive checks (binary outcome)
pp_samples <- posterior_predict(bayes_fit, ndraws = 500)  # 500 draws

# Check dimensions
dim(pp_samples)  # rows = draws, cols = observations

# Plot overlay of observed vs predicted counts (duplicate image)
ppc_dens_overlay(y = adult_imp1$diabetes_dx, yrep = pp_samples[1:50, ]) +
  labs(title = "Posterior Predictive Check: Density Overlay") +
  theme_minimal()

ppc_bars(y = adult_imp1$diabetes_dx, yrep = pp_samples[1:50, ])


# Alternative PP plots (histogram / barplot) for binary outcome (bar chart preferred being discrete outcome)

p7 <- ppc_bars(y = adult_imp1$diabetes_dx, yrep = pp_samples[1:50, ]) +
  labs(title = "Posterior Predictive Check: Barplot of Counts") +
  theme_minimal()
ggsave("ppc_bars.png", plot = p7, width = 8, height = 6, dpi = 300)


#PP check for proportions (useful for binary) # mean comparison
## to check if the simulated means match the observed mean

## mean
p8 <- ppc_stat(y = adult_imp1$diabetes_dx, yrep = pp_samples[1:100, ], stat = "mean") +
  labs(title = "Posterior Predictive Check: Mean of Replicates") +
  theme_minimal()
ggsave("ppc_stat_mean.png", plot = p8, width = 8, height = 6, dpi = 300)


## sd
p9 <- ppc_stat(y = adult_imp1$diabetes_dx, yrep = pp_samples[1:100, ], stat = "sd") +
  labs(title = "PPC: Standard Deviation of Replicates") +
  theme_minimal()
ggsave("ppc_stat_sd.png", plot = p9, width = 8, height = 6, dpi = 300)


# PP checks with bayesplot options
color_scheme_set("blue")
p10 <- ppc_scatter_avg(y = adult_imp1$diabetes_dx, yrep = pp_samples[1:100, ]) +
  labs(title = "Observed vs Predicted (Avg) Posterior Predictive")

ggsave("ppc_scatter_avg.png", plot = p10, width = 8, height = 6, dpi = 300)


```

```{r}
#| label: mcmc, post vs obs


library(brms)
library(dplyr)

# Posterior summary

post_sum <- posterior_summary(bayes_fit)
colnames(post_sum)


library(posterior)
library(bayesplot)

# Extract posterior draws as a draws_df # simulate posterior outcomes
post <- as_draws_df(bayes_fit)

# Check parameter names
colnames(post)


# Density overlay for age and bmi
p11 <- mcmc_areas(post, pars = c( "b_age_c","b_bmi_c","b_sexFemale","b_raceMexicanAmerican", "b_raceOtherHispanic","b_raceNHBlack","b_raceOtherDMulti" ))

ggsave("mcmc_areas.png", plot = p11, width = 8, height = 6, dpi = 300)

###

predicted <- fitted(bayes_fit, summary = TRUE)
observed <- adult_imp1[, c("bmi", "age")]

# Plot for **bmi** (obs vs pred)

library(ggplot2)
p12 <- ggplot(data = NULL, aes(x = observed$bmi, y = predicted[, "Estimate"])) +
  geom_point() +
  geom_errorbar(aes(ymin = predicted[, "Q2.5"], ymax = predicted[, "Q97.5"])) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  xlab("Observed bmi") + ylab("Predicted bmi")

ggsave("plot_bmi_obs_pred.png", plot = p12, width = 8, height = 6, dpi = 300)


```
```{r}
#| label: pred vs obs

# Combine observed and predicted into one data frame
plot_data <- adult_imp1 %>%
  mutate(
    predicted_bmi = predicted[, "Estimate"],
    lower_ci = predicted[, "Q2.5"],
    upper_ci = predicted[, "Q97.5"],
    obs_index = 1:nrow(adult_imp1)  # index for x-axis
  )

# Line plot
p13 <- ggplot(plot_data, aes(x = obs_index)) +
  geom_line(aes(y = bmi, color = "Observed")) +               # observed BMI
  geom_line(aes(y = predicted_bmi, color = "Predicted")) +   # predicted BMI
  geom_ribbon(aes(ymin = lower_ci, ymax = upper_ci), alpha = 0.2) +  # uncertainty
  labs(x = "Observation", y = "BMI", color = "Legend") +
  theme_minimal()
ggsave("Line plot_bmi_obs_pred.png", plot = p13, width = 8, height = 6, dpi = 300)


###
summary(adult_imp1$bmi)
summary(plot_data$bmi_c)

```



```{r}
#| label: Pred vs obs age and bmi


prior_summary(bayes_fit)
prior_draws <- tibble(
  term = rep(c("BMI (per 1 SD)", "Age (per 1 SD)"), each = 4000),
  estimate = c(rnorm(4000, 0, 1), rnorm(4000, 0, 1)),
  type = "Prior"
)

summary(prior_draws)

post
head(post)
names(prior_draws)

library(brms)
library(tidyr)

# Extract posterior draws
post <- as_draws_df(bayes_fit) %>%      # bayes_fit = your brms model
  select(b_bmi_c, b_age_c) %>%               # select your coefficient columns
  pivot_longer(
    everything(),
    names_to = "term",
    values_to = "estimate"
  ) %>%
  mutate(
    term = case_when(
      term == "b_bmi_c" ~ "BMI (per 1 SD)",
      term == "b_age_c" ~ "Age (per 1 SD)"
    ),
    type = "Posterior"
  )


## visualization of prior and predicted draws
combined_draws <- bind_rows(prior_draws, post) 

library(ggplot2)

p14 <- ggplot(combined_draws, aes(x = estimate, fill = type)) +
  geom_density(alpha = 0.4) +
  facet_wrap(~ term, scales = "free", ncol = 2) +
  theme_minimal(base_size = 13) +
  labs(
    title = "Prior vs Posterior Distributions",
    x = "Coefficient estimate",
    y = "Density",
    fill = ""
  )

ggsave("Prior vs Posterior Distributions_bmi_age.png", plot = p14, width = 8, height = 6, dpi = 300)

#### Compute proportion of diabetes=1 for each draw
pp_proportion <- rowMeans(pp_samples)  # proportion of 1's in each posterior draw


# Summary of posterior proportions
summary(pp_proportion)

# Optional: visualize the posterior probability distribution
pp_proportion_df <- tibble(proportion = pp_proportion)

p15 <- ggplot(pp_proportion_df, aes(x = proportion)) +
  geom_histogram(binwidth = 0.01, fill = "skyblue", color = "black") +
  labs(
    title = "Posterior Distribution of Proportion of Diabetes = 1",
    x = "Proportion of Diabetes = 1",
    y = "Frequency"
  ) +
  theme_minimal()

ggsave("Posterior Distribution of Proportion of Diabetes = 1.png", plot = p15, width = 8, height = 6, dpi = 300)


```




```{r}
#| label: DM prediction vs prevalence

####

library(tidyverse)

# Posterior predicted proportion vector
# pp_proportion <- rowMeans(pp_samples)  # if not already done

known_prev <- 0.089   # NHANES prevalence

# Posterior summary
posterior_mean <- mean(pp_proportion)
posterior_ci <- quantile(pp_proportion, c(0.025, 0.975))  # 95% credible interval

# Create a data frame for plotting
pp_df <- tibble(proportion = pp_proportion)

# Plot
p16 <- ggplot(pp_df, aes(x = proportion)) +
  geom_histogram(binwidth = 0.005, fill = "skyblue", color = "black") +
  geom_vline(xintercept = known_prev, color = "red", linetype = "dashed", size = 1) +
  geom_vline(xintercept = posterior_mean, color = "blue", linetype = "solid", size = 1) +
  geom_rect(aes(xmin = posterior_ci[1], xmax = posterior_ci[2], ymin = 0, ymax = Inf),
            fill = "blue", alpha = 0.1, inherit.aes = FALSE) +
  labs(
    title = "Posterior Predicted Diabetes Proportion vs NHANES Prevalence",
    subtitle = paste0("Red dashed = NHANES prevalence (", known_prev, 
                      "), Blue solid = Posterior mean (", round(posterior_mean,3), ")"),
    x = "Proportion of Diabetes = 1",
    y = "Frequency"
  ) +
  theme_minimal()

ggsave("Posterior Distribution of Proportion of Diabetes = 1.png", plot = p16, width = 8, height = 6, dpi = 300)


```


```{r}
#| label: posterior DM prediction vs population prevalence
library(dplyr)

# Posterior predicted proportion
posterior_mean <- mean(pp_proportion)
posterior_ci <- quantile(pp_proportion, c(0.025, 0.975))  # 95% credible interval

# NHANES prevalence with SE from survey::svymean
# Suppose you already have:
# svymean(~diabetes_dx, nhanes_design_adult, na.rm = TRUE)
known_prev <- 0.089        # Mean prevalence
known_se   <- 0.0048       # Standard error from survey

# Calculate 95% confidence interval
known_ci <- c(
  known_prev - 1.96 * known_se,
  known_prev + 1.96 * known_se
)

# Print results
data.frame(
  Type = c("Posterior Prediction", "NHANES Prevalence"),
  Mean = c(posterior_mean, known_prev),
  Lower_95 = c(posterior_ci[1], known_ci[1]),
  Upper_95 = c(posterior_ci[2], known_ci[2])
)

####
library(ggplot2)
library(dplyr)

# Create a data frame for plotting
ci_df <- data.frame(
  Type = c("Posterior Prediction", "NHANES Prevalence"),
  Mean = c(0.1096674, 0.089),
  Lower_95 = c(0.09772443, 0.079592),
  Upper_95 = c(0.1210658, 0.098408)
)

# Plot
p17 <- ggplot(ci_df, aes(x = Type, y = Mean, color = Type)) +
  geom_point(size = 4) +
  geom_errorbar(aes(ymin = Lower_95, ymax = Upper_95), width = 0.2) +
  ylim(0, max(ci_df$Upper_95) + 0.02) +
  labs(
    title = "Comparison of Posterior Predicted Diabetes Proportion vs NHANES Prevalence",
    y = "Proportion of Diabetes",
    x = ""
  ) +
  theme_minimal(base_size = 14) +
  theme(legend.position = "none")

ggsave("Population Prevalence vs Proportion of Diabetes = 1.png", plot = p17, width = 8, height = 6, dpi = 300)
p17

```


```{r}
# Save your dataset as CSV
write.csv(adult_imp1, "adult_imp1.csv", row.names = FALSE)

```


```{python}
#| label: test-train
#| eval: false
#| include: false
#Code here

import pandas as pd

# Load the data exported from R
adult_data_py = pd.read_csv("adult_imp1.csv")  # your R dataset saved as CSV

# Now you can rename it freely in Python
df = adult_data_py  # just create a new variable pointing to the same DataFrame

# Check the data
df.head(adult_data_py)

import pandas as pd
from sklearn.model_selection import train_test_split

# Optional: rename columns if needed
df = df.rename(columns={"diabetes_dx": "diabetes_status"})

# Separate predictors (X) and outcome (y)
X = df.drop(columns=["diabetes_status"])
y = df["diabetes_status"]

# Create 80/20 train-test split, stratifying by outcome to keep class balance
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=1234, stratify=y
)

# Combine X and y if you want full train/test datasets
train_data = pd.concat([X_train, y_train], axis=1)
test_data  = pd.concat([X_test, y_test], axis=1)

# Check sizes
print("Training set:", train_data.shape)
print("Testing set: ", test_data.shape)

# Check class balance
print("\nTraining set class distribution:\n", y_train.value_counts(normalize=True))
print("\nTesting set class distribution:\n", y_test.value_counts(normalize=True))


####
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Combine train and test distributions into a DataFrame
train_dist = pd.DataFrame({'Class': y_train.unique(), 
                           'Proportion': y_train.value_counts(normalize=True).values,
                           'Set': 'Train'})

test_dist = pd.DataFrame({'Class': y_test.unique(), 
                          'Proportion': y_test.value_counts(normalize=True).values,
                          'Set': 'Test'})

dist_df = pd.concat([train_dist, test_dist])

# Plot
plt.figure(figsize=(8,5))
sns.barplot(x='Class', y='Proportion', hue='Set', data=dist_df)
plt.title('Class Distribution in Training and Testing Sets')
plt.ylabel('Proportion')
plt.xlabel('Diabetes Status')
plt.ylim(0,1)
plt.show()


###

# Numeric summaries for predictors
print("Training set numeric summary:\n", X_train.describe())
print("\nTesting set numeric summary:\n", X_test.describe())

```


```{r}
#| label: autocorrelation_mcmc
#| eval: false
#| include: false

colnames(post)
library(tidyr)
library(bayesplot)

library(posterior)

# Convert fitted model to draws array
post_array <- as_draws_array(bayes_fit)  # draws x chains x parameters

# Plot autocorrelation for age and bmi
mcmc_acf(post_array, pars = c("b_age_c", "b_bmi_c"))


```

# Results

1.  **Multiple Linear Regression**

    Model**:**
    `svyglm(formula = form_cc, design = des_cc, family = quasibinomial())`

All predictors are significant: **age (p \< 0.001)** and **BMI (p \<
0.001)** show strong positive associations with the outcome, while
**being female (p = 0.0004)** is negatively associated. Other
significant associations include **raceMexican American (p = 0.0008)**,
**raceOther Hispanic (p = 0.0087)**, **raceNH Black (p = 0.0117)**, and
**raceOther/Multi (p = 0.0014)**.

2.  **MICE**

    All predictors are statistically significant.

    Positive associations: age (p \< 0.001), BMI (p \< 0.001), and all
    race categories compared to reference.

    Negative association: being female (p \< 0.001)

<!-- -->

3.  **Bayesian Regression**

    Sampling**:** NUTS (4 chains, 2000 iterations each; 1000 warmup,
    4000 post-warmup draws)

Convergence & Diagnostics
-   Rhat = 1.00 for all parameters ‚Üí excellent convergence
-   Bulk_ESS / Tail_ESS: Large values (\>2000) ‚Üí high effective
        sample sizes, reliable posterior estimates.

Interpretation

-   Strong predictors: Age and BMI are strongly positively
        associated with diabetes risk.
-   Sex effect: Females have a lower probability of diabetes
        compared to males
-   R¬≤ = 0.13 shows 13% of the variance in the outcome (diabetes_dx)
        is explained by your predictors (age, BMI, sex, race), 95%
        Credible Interval: 0.106‚Äì0.156, indicates that, given your model
        and data, the true proportion of variance explained is plausibly
        between 10.6% and 15.6% and shows uncertainty in the explained
        variance, which is natural for probabilistic models.
-   Est.Error = 0.0127, reflects the standard error of the R¬≤
        estimate across posterior samples. The small SE indicates that
        the R¬≤ estimate is fairly precise.
-   Race/ethnicity: Mexican American, NH Black, and ‚ÄúOther/Diverse‚Äù
        groups have higher odds of diabetes. Other Hispanic group has a
        less certain effect.
-   age_c-Each 1-unit increase in centered age increases the
        log-odds of diabetes by 1.09. Strong positive effect.
-   bmi_c-Higher BMI is associated with higher diabetes risk.
        sexFemale-Females have lower odds of diabetes compared to males.
-   raceMexicanAmerican-Higher odds of diabetes vs. reference race
        (likely NH White)
-   raceOtherHispanic-Slightly higher odds vs reference, but
        interval crosses zero ‚Üí uncertain effect.
-   raceNHBlack-Significantly higher odds of diabetes compared to
        reference. raceOtherDMulti-Higher odds of diabetes vs reference
        group.

**Posterior distribution** of all parameters in the model. (1)Density
plot of posterior samples each parameter (e.g., intercept, slope) into a
smoothed density curve, showing most of the posterior probability mass
lies for best estimates and uncertainty.

**Posterior Predictive Distribution** - generated from posterior
predictive draws: ùë¶rep‚àºùëù(ùë¶new‚à£ùúÉ)yrep‚Äã‚àºp(ynew‚Äã‚à£Œ∏) simulate the data given
posterior parameter estimates.Posterior predictive checks (PPC) compare
these simulations to real data to assess model fit.

**Incorporating Uncertainty** two sources of uncertainty: Parameter
uncertainty: captured in the posterior distributions Predictive
uncertainty: captured in posterior predictive draws

Combining the two provide credible intervals for predictions, not just
point predictions and specifies - Given the BMI, the probability of
diabetes is likely between 40‚Äì55%.‚Äù

**Comparing Models**

-   All three models (survey-weighted MLE, multiple imputation,
    Bayesian) agree closely on the direction and magnitude of the
    effects of BMI and age.
-   Age is a stronger predictor than BMI, nearly tripling the odds per 1
    SD.
-   BMI significantly increases diabetes risk (\~1.7‚Äì1.9√ó per 1 SD).
-   Differences between models are minor, indicating robust and reliable
    findings despite missing data or modeling approach.
    
## **Diabetes Predicted proportion vs population prevalence**
    
 - The posterior predicted proportion is slightly higher than the observed NHANES prevalence (10.97% vs 8.9%).
 - Intervals overlap slightly, but the posterior tends to overestimate diabetes compared to NHANES.

The difference could be due to:

- Model assumptions (logistic link, priors)
- Predictor effects (BMI, age, sex, race)
- Sample characteristics vs population weighting in NHANES

The results find our model is reasonable but slightly conservative (predicting higher risk) relative to the observed population prevalence.

# Conclusion

Across multiple modeling approaches‚Äîsurvey-weighted maximum
    likelihood, multiple imputation, and Bayesian regression‚Äîboth age
    and BMI were consistently strong predictors of diabetes. 
    Eachstandard deviation increase in age nearly tripled the odds of
    diabetes, while a similar increase in BMI elevated the odds by
    approximately 1.7‚Äì1.9 times.
    The consistency of these results across
    models highlights the robustness of the associations and underscores
    the importance of age and BMI as key risk factors for diabetes in
    this population.

Effect stability: point estimates in rhe Bayesian model‚Äôs closely
  aligned with those from the frequentist, indicating that prior
  regularization stabilized the estimates in the presence of modest
  missingness.

Uncertainty quantification: Bayesian credible intervals of odds ration
  were slightly narrower yet overlapped the frequentist confidence
intervals, suggest comparable inferential precision while offering
improved interpretability.

Design considerations: \# Survey-weighted MLE (Maximum Likelihood
Estimator) - incorporates each observation weighted according to its
survey weight. - provide estimates that reflect the population-level
parameters, not just the sample- produces population-representative
estimates. \# Bayesian model with normalized weights- - instead of fully
modeling the survey design, it used normalized sampling weights as
importance weights - the scaled weights that sum to the sample size
approximates the effect of survey weights, but does not fully account
for: Stratification, clustering, design-based variance adjustments. -
Bayesian inference treats the weighted likelihood as from a regular
model, ignoring some survey design features.

Autocorrelation Interpretation

Autocorrelation shows how correlated a sample is with previous samples (lags) in the MCMC chain. Lag indicates the number of steps between samples. Lag 0 is always 1 (perfect correlation with itself). Each chain (1‚Äì4) shown separately for b_age_c (age coefficient) and b_bmi_c (BMI coefficient) presents autocorrelation drops quickly to near zero after lag 1‚Äì2 for both coefficients in all chains suggesting good mixing: successive samples are mostly independent after a short lag. No persistent high autocorrelation indicates MCMC chains are converging well.Low autocorrelation, as in your plot, is desirable because it means your posterior estimates are reliable and not biased by chain dependence.

# Discussions

The use of multiple imputation allowed for robust analysis despite
missing data, increasing power and reducing bias. Comparison of
frequentist and Bayesian models demonstrated consistency in significant
predictors, while Bayesian approaches provided the advantage of
posterior distributions and probabilistic interpretation. The =

Across all models, both age and BMI emerged as strong and consistent
predictors of diabetes. The consistency across modeling approaches
strengthens the validity of these findings Multiple imputation accounted
for potential biases due to missing data, and Bayesian modeling provided
robust credible intervals that closely matched frequentist estimates.
align with previous epidemiological research indicating that increasing
age and higher BMI are among the most important determinants of type 2
diabetes risk.Cumulative exposure to metabolic and lifestyle risk
factors over time, and the role of excess adiposity and insulin related
effects account for diabetes.

Survey weighted dataset strenghthens ensuring population
representativeness, multiple imputation to handle missing data, and
rigorous Bayesian estimation provided high effective sample sizes and RÃÇ
‚âà 1.00 across parameters confirmed excellent model convergence. Bayesian
logistic regression provided inference statistically consistent and
interpretable achieving the aim of this study. In future research
hierarchical model using NHANES cycles or adding variables (lab tests)
could assess nonlinear effects of metabolic risk factors.

# Limitations

Our study was a cross-sectional study design - precludes potential
residual confounding from unmeasured factors such as diet, physical
activity, and genetic predisposition.


## QUESTION for targeted therapy

Translational Perspective from the Bayesian Diabetes Prediction Project

This project further demonstrates the translational potential of Bayesian modeling in clinical decision-making and public health strategy. 

By using patient-level predictors such as age, BMI, sex, and race to estimate the probability of diabetes, the model moves beyond descriptive statistics toward individualized risk prediction. 

The translational move lies in converting these probabilistic outputs into actionable thresholds‚Äîsuch as identifying the BMI or age at which the predicted risk of diabetes exceeds a clinically meaningful level (e.g., 30%).

Such insights can guide early screening, personalized lifestyle interventions, and targeted prevention programs for populations at higher risk. This approach embodies precision public health‚Äîbridging data science and medical decision-making to deliver tailored, evidence-based strategies that can ultimately improve diabetes prevention and management outcomes.

‚ÄúWhat changes in modifiable predictors would lower diabetes risk?‚Äù

Translational Research Implications:
- We now use the model to guide prevention or intervention.
- Only BMI is a modifiable risk factor here
- ‚ÄúWhat must change in BMI, behavior, or lifestyle to achieve a lower risk threshold?‚Äù

In practice, we hold non modifiable predictors as constant (sex, race).
Vary modifiable predictors (BMI) until the model predicts the desired probability.

```{r}
#| label: prediction for one patient
#| eval: false
#| include: false

# Use the first participant 
# using multiple covariates to select someone
participant1_data  <- adult[1, ]


# Example: predicted probabilities for patient 1
phat1 <- posterior_linpred(bayes_fit, newdata = participant1_data, transform = TRUE)
# 'transform = TRUE' gives probabilities for logistic regression

# Store in a data frame for plotting
post_pred_df <- data.frame(pred = phat1)

# Compute 95% credible interval
ci_95_participant1 <- quantile(phat1, c(0.025, 0.975))

# Plot

ggplot(post_pred_df, aes(x = pred)) + 
  geom_density(color='darkblue', fill='lightblue') +
  geom_vline(xintercept = ci_95_participant1[1], color='red', linetype='dashed') +
  geom_vline(xintercept = ci_95_participant1[2], color='red', linetype='dashed') +
  xlab('Probability of being diabetic (Outcome=1)') +
  ggtitle('Posterior Predictive Distribution 95% Credible Interval') +
  theme_bw()


```

```{r}
#| label: participant2
#| eval: false
#| include: false

participant2_data  <- adult[2, ]


# Example: predicted probabilities for patient 1
phat2 <- posterior_linpred(bayes_fit, newdata = participant2_data, transform = TRUE)
# 'transform = TRUE' gives probabilities for logistic regression

# Store in a data frame for plotting
post_pred_df2 <- data.frame(pred = phat2)

# Compute 95% credible interval
ci_95_participant2 <- quantile(phat2, c(0.025, 0.975))

# Plot

ggplot(post_pred_df2, aes(x = pred)) + 
  geom_density(color='darkblue', fill='lightblue') +
  geom_vline(xintercept = ci_95_participant2[1], color='red', linetype='dashed') +
  geom_vline(xintercept = ci_95_participant2[2], color='red', linetype='dashed') +
  xlab('Probability of being diabetic (Outcome=1)') +
  ggtitle('Posterior Predictive Distribution 95% Credible Interval') +
  theme_bw()

```


```{r}
#| label: new participant
#| eval: false
#| include: false
library(ggplot2)

new_participant <- data.frame(
  age_c = 40,
  bmi_c = 25,
  sex   = "Female",
  race  = "Mexican American"
)

# Posterior predicted probabilities
phat_new <- posterior_linpred(bayes_fit, newdata = new_participant, transform = TRUE)

# Convert to numeric vector
phat_vec <- as.numeric(phat_new)

# Check the range to see if all values are similar
range(phat_vec)

# Store in a data frame
post_pred_df_new <- data.frame(pred = phat_vec)

# Compute 95% credible interval from the vector
ci_95_new_participant <- quantile(phat_vec, c(0.025, 0.975))

# Plot
ggplot(post_pred_df_new, aes(x = pred)) + 
  geom_density(color='darkblue', fill='lightblue', alpha = 0.6) +
  geom_vline(xintercept = ci_95_new_participant[1], color='red', linetype='dashed') +
  geom_vline(xintercept = ci_95_new_participant[2], color='red', linetype='dashed') +
  xlim(0, 1) +  # ensures you see the curve even if values are close
  xlab('Probability of being diabetic (Outcome=1)') +
  ggtitle('Posterior Predictive Distribution (95% Credible Interval)') +
  theme_bw()



```




```{r}
#| label: predict BMI_targeted therapy
#| eval: false
#| include: false

# Grid of possible BMI values (centered if model used bmi_c)
bmi_seq <- seq(18, 40, by = 0.5)

newdata_grid <- data.frame(
  age_c = 40,
  bmi_c = bmi_seq,
  sex   = "Female",
  race  = "Mexican American"
)

# Posterior mean predicted probabilities
pred_probs <- posterior_linpred(bayes_fit, newdata = newdata_grid, transform = TRUE)
# Average over posterior draws to get the mean predicted probability per BMI
prob_mean <- colMeans(pred_probs)

# Combine with BMI values
pred_df <- cbind(newdata_grid, prob_mean)

target_prob <- 0.3
closest <- pred_df[which.min(abs(pred_df$prob_mean - target_prob)), ]

closest

ggplot(pred_df, aes(x = bmi_c, y = prob_mean)) +
  geom_line(color = "darkblue", linewidth = 1.2) +
  geom_hline(yintercept = target_prob, color = "red", linetype = "dashed") +
  geom_vline(xintercept = closest$bmi_c, color = "red", linetype = "dotted") +
  annotate("text", x = closest$bmi_c, y = target_prob + 0.05,
           label = paste0("Target BMI ‚âà ", round(closest$bmi_c, 1)),
           color = "red", hjust = -0.1) +
  labs(
    x = "BMI (centered or raw, depending on model)",
    y = "Predicted Probability of Diabetes",
    title = "Inverse Prediction: BMI Needed for Target Diabetes Risk"
  ) +
  theme_bw()


```




# Implications

-   age and BMI as robust and independent predictors of diabetes,
    underscore the importance of early targeted interventions in
    mitigating diabetes risk.
-   Longitudinal studies and combining other statistical analytical
    methods with Bayesian can further enhance and provide better
    informed precision prevention strategies.

## References
