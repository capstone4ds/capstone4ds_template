---
title: "Bayesian Logistic Regression - Application in Probability Prediction of disease (Diabetes) "
subtitle: "CapStone Project_2025"
author: "Namita Mishra, Autumn Wilcox, Ecil Teodoro (Advisor: Dr. Ashraf Cohen)"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

Slides: [slides.html](slides.html){target="_blank"} ( Go to `slides.qmd`
to edit)


------------------------------------------------------------------------

## Introduction

A major public health concern**Diabetes mellitus (DM)** is associated
with obesity, age, race, and gender and identifying associated risk
factor is crucial for targeted intervention.**Logistic Regression**
estimates the association between risk factors and binary outcomes
(presence or absence of diabetes). Standard analytical approaches are
insufficient in analyzing the complexity of healthcare data (DNA
sequences, imaging, patient-reported outcomes, electronic health records
(EHRs), longitudinal health measurements, diagnoses, and treatments.
(Zeger et al., 2020). However, classical maximum likelihood estimation
(MLE) yields unstable results in small samples with missing data, or
quasi- and complete separation. The Bayesian hierarchical model with
Markov Chain Monte Carlo (MCMC) implemented on multivariate longitudinal
healthcare data by integrating prior knowledge predict health status
(Zeger et al., 2020). Model with two levels of data structure: (1)
repeated measures over time within individuals and (2) individuals
nested within a population, with added exogenous covariates (e.g., age,
clinical history), and endogenous covariates (e.g., current treatment),
yield posterior distributions and marginal distributions from MCMC
estimation of parameters provide risk prediction (pneumonia, prostate
cancer, and mental disorders). The model's limitation is its parametric
nature.

Application of **Bayesian Inference** @Chatzimichail2023, comparing
parametric (with a fixed set of parameters) and non-parametric
distributions (which do not make a priori assumptions) on National
Health and Nutrition Examination Survey data from two separate
diagnostic tests on both diseased and non-diseased populations, and
provides posterior probability classifying diseases. Clinical criteria
and fixed numerical thresholds in conventional and the dichotomous
method (overlap of probability distributions between the diseased and
nondiseased groups) fails to capture the intricate relationship between
diagnostic tests and the prevalence of the diseases, the complexity and
heterogeneity across diverse populations and its applicability in
skewed, bimodality, or multimodality data is critiqued. Bayesian
nonparametric (vs parametric) is a flexible, adaptable, versatile, and
robust approach, capturing complex data patterns, producing multimodal
probability patterns vs the bimodal, double-sigmoidal curves in
parametric models. Integrating priors, combined with multiple diagnostic
tests, improves diagnostic accuracy and precision. Model applicability
is limited by access to scholarly publications and over-dependence on
priors. Combining with other statistical and computational techniques
enhances diagnostic capabilities @Chatzimichail2023 to overcome systemic
bias, unrepresentative, incomplete, and non-normal datasets.

**Bayesian methodology** by @VandeSchoot2021 emphasizes the importance
of priors, data modeling, inferences, model checking, sampling from a
posterior distribution, variational inferences, and variable selection
for applicability across social sciences, ecology, genetics, and
medicine. The variable selection is crucial as multicollinearity,
insufficient sampling, and overfitting result in poor predictive
performance and difficult interpretation. Informative, weakly
informative, and diffuse prior incorporation depending on (un)certainty
in (hyperparameters), where a larger variance representing greater
uncertainty. Prior elicitation (experts, generic experts, data-based,
and sample data using maximum likelihood or sample statistics), and
prior sensitivity analysis of the likelihood assesses how the priors and
the likelihood align. Prior provides data-informed shrinkage,
regularization, or influence algorithms, providing a high-density
region, improving estimation. Specifying prior information in small and
less informative samples, strengthens the observed data with unknown
parameters having varied values, observed data having fixed values, and
the likelihood function generate a range of possible values and
integrating the MCMC algorithm for sampled values from a given
distribution through computer simulations provide empirical estimates of
the posterior distribution (BRMS and Blavaan in R). The frequentist
method does not consider the probability of the unknown parameters and
considers them as fixed, while likelihood is based on the conditional
probability distribution. Spatial and temporal Bayesian models has
applicability in large-scale cancer genomic data, identifying novel
molecular-level changes, interactions between mutated genes, capturing
mutational signatures, allowing genomic-based patient stratification in
clinical trials, and targeted treatments and in understanding cancer
evolution. The Bayesian model is reproducible, but is limited by
autocorrelation in the temporal model and by subjectivity in prior
elicitation.

Prior elicitation, analytical posteriors, robustness checks in
**Bayesian Normal linear regression, and parametric (conjugate) model
incorporating Normal‚ÄìInverse-Gamma prior** have been demonstrated in
metrology @Klauenberg2015 to calibrate instruments. In Gaussian, errors
are independent and identically distributed, the variance is unknown,
the relationship between X and Y is statistical, with noise and model
uncertainty, and the regression can not be treated as a measurement
function. Likelihood, Bayesian, bootstrap, etc., account for
uncertainty, prior information, and observables (data) and unobservables
(parameters and auxiliary variables) are unknown and random, and the
assigned probability distributions update prior knowledge about the
unobservables, enhance interpretation and robustify analyses. The
Normal-Inverse Gamma (NIG) distribution from the same family as the
conjugate prior with unknown mean and variance can specify vague or
non-informative priors. Hierarchical prior add an additional layer of
distributions, accounting for uncertainty to be more flexibly modeled.

**Bayesian Hierarchical / meta-analytic linear regression** model
@DeLeeuw2012 augments data by incorporating both exchangeable and
unexchangeable information on parameters addressing issues associated
with multiple testing with low statistical power, and the issues of
conducting separate significance tests across studies with different
predictors, and the need for larger samples. Linear regression produce
smaller, unreliable estimates vulnerable to sample variations. Priors
from meta-analysis in Bayesian regression addresses the challenge of
small sample size and unavailability of previous articles, resolving the
limitations of univariate analyses, and the relationship issues among
multiple regression parameters within a study. Priors based on previous
data and current data are categorized (1) Exchangeable when the current
data and previous studies share the same set of predictors, and (2)
Unexchangeable when the predictors differ. The probability density
function for the data (using the Gibbs sampler), and the likelihood
function reflect prior assumptions about the model. The hierarchical
unexchangeable model provide applicability is in studying differences in
studies, enabling explicit testing of the exchangeability assumption.
Application is limited due to the correlation between identical set of
predictors. (DeLeeuw, 2012).

**Bayesian logistic regression (Bayesian GLM)\*\***- A sequential
clinical reasoning model. Liu (2013) demonstrated its applicability in
screening adults (20‚Äì79 years, Taiwan) addressing the limited
availability of molecular information and as an alternative method
leveraging routinely collected biological markers classifying diseases.
Sequential adding of predictors in three models: (1) demographic
features (basic model), (2) six metabolic components (metabolic score
model), and (3) conventional risk factors (enhanced model),
incorporating priors, and emulating a clinician‚Äôs evaluation process,
the model assumes normally distributed regression coefficients, accounts
for uncertainty in clinical weights, and averages credible intervals for
predicted risk estimates. The posterior distributions produced in
Enhanced model showed that patient background significantly contributed
to baseline risk estimation by integrating individual characteristics
capturing ecological heterogeneity. The model applicability is limited
by potential interactions between predictors and external
cross-validation.

**Bayesian multiple imputation with logistic regression, @Austin 2021**,
addresses missing data in clinical research. Analyzing causes of missing
values (i) patients refusing to answer specific questions, (ii) loss to
follow-up, (iii) investigator or mechanical errors, or (iv) physicians
choosing not to order certain investigations and understanding
missingness: missing at random (MAR), missing not at random (MNAR), or
missing completely at random (MCAR) is crucial. Multiple imputation (MI)
using R, SAS, or Stata provide plausible values creating multiple
completed datasets while simultaneously conducting identical statistical
analyses across them, robustify estimates through pooled results in
classifying patients health status and mortality rates.

**Aims**

The present study aims to apply Bayesian logistic regression to predict
diabetes status and estimate the association between body mass index
(BMI), age (\>20 years age group), gender, and race as predictors and
diabetes as the response variable using a retrospective dataset
(2013‚Äì2014 NHANES survey). NHANES is a complex survey, and is not sampled uniformly: It uses stratification, clustering, and oversampling of certain groups in the population. We use Bayesian analytic approach to evaluates the
challenges of dataset anomalies (missingness, complete case analysis and separation) for
which traditional logistic regression is not efficient in predicting
health status. 

## Method and Data Preparation

**Statistical Tool**: R, R packages, and libraries are used to import data,
data wrangling and analysis.

**Data collection**: is from NHANES 2-year cross-sectional weighted data
(2013-2014 year). We imported three datasets (demographics, exam,
questionnaire) with selected variable of interest
@CenterforHealthStatistics1999. Haven package coverted .XPT files in R to dataframe (df).

**Modeling** - to estimate the association between BMI, age, sex, and race/ethnicity and
doctor-diagnosed diabetes (`DIQ010`). 


```{r}
#| label: Libraries
#| echo: true


# loading packages 
options(repos = c(CRAN = "https://cloud.r-project.org"))
install.packages("nhanesA")
library ("nhanesA")    

library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
library(Hmisc)
library(dplyr)
library(tidyr)
library(forcats)
library(ggplot2)
library(classpackage)
library(janitor)
install.packages("gt")   
library(gt)
library(survey)
library(DataExplorer)
library(logistf)


```

**Method**

**Data pre-processing and cleaning**
Subsets are created from the original weighted datasets (demographics,
exam, questionnaire) with selected variables are merged using ID to
create a single dataframe.

1.  Response Variable: Binary, **type 2 / diagnosed diabetes**
    (excluding gestational diabetes) -‚ÄúDoctor told you have diabetes?‚Äù DIQ010
    combined with `DIQ050` a **secondary variable** describing
    **treatment status (insulin use) to exclude those cases**
2.  Predictor Variables (Body Mass Index, factor, 4 levels are analyzed
    after standardization).\
    o Underweight (\<5th percentile)\
    o Normal (5th‚Äì\<85th)\
    o Overweight (85th‚Äì\<95th) o Obese (‚â•95th percentile)\
    o Missing We kept it as it is as categorization provides clinically
    interpretable groups
3.  Covariates:
    1.  Gender (factor, 2 levels): Male: Female
    2.  Ethnicity (factor, 5 levels): Mexican American, Non-Hispanic,
        White Non-Hispanic, Black Other Hispanic, Other Race - Including
        multi-racial
    3.  Age (number, continuous)


```{r message=FALSE, warning=FALSE}
#| label: nhanes tables and merged data
#| include: false

            

library ("nhanesA")     
 # imported datasets 
                       nhanesTables('EXAM', 2013)
                       nhanesTables('QUESTIONNAIRE', 2013)
                       nhanesTables('DEMOGRAPHICS', 2013)
         
                                     
 # codebook for variable details

nhanesCodebook("DEMO_H",'RIDRETH1')
nhanesCodebook("DEMO_H",'RIAGENDR')
nhanesCodebook("DEMO_H",'RIDAGEYR')
nhanesCodebook("DIQ_H","DIQ010, DIQ050")
nhanesCodebook("BMX_H",'BMDBMIC')

  #  .xpt files read ( 2013‚Äì2014)                      
                      bmx_h <- nhanes("BMX_H")         #Exam
                      demo_h <- nhanes("DEMO_H")       #Demo
                      diq_h <- nhanes("DIQ_H")         #diabetes


# variables of interest

library(dplyr)

exam_sub <- bmx_h %>% 
  select(SEQN, BMDBMIC) %>%
  rename(
    ID = SEQN,
    BMI = BMDBMIC
  )


need_demo <- c("SEQN","RIDAGEYR","RIAGENDR","RIDRETH1","SDMVPSU","SDMVSTRA","WTMEC2YR")
stopifnot(all(c("SEQN","BMXBMI") %in% names(bmx_h)))
stopifnot(all(need_demo %in% names(demo_h)))
if (!("DIQ010" %in% names(diq_h))) {
  stop("DIQ010 is not in DIQ_H. Check the cycle name 'DIQ_H' and nhanesA version.")
}


      # ---- Select only needed variables ----
exam_sub <- bmx_h  %>% select(SEQN, BMXBMI)
demo_sub <- demo_h %>% select(all_of(need_demo))
diq_sub  <- diq_h  %>% select(SEQN, DIQ010, dplyr::any_of("DIQ050"))

# merged dataframe

merged_data <- demo_sub %>%
  left_join(exam_sub, by = "SEQN") %>%
  left_join(diq_sub,  by = "SEQN")

names(merged_data)
saveRDS(merged_data, "data/nhanes2013_2014_prepared.rds")


```

Merged dataset created, cleaned and visualized for any anomalies and ata Exploration.

```{r}
#| label: data structure and missing data
#| echo: true

# weighted means of each variable                       
str(merged_data)
plot_str(merged_data)
introduce(merged_data)

p1 <- plot_intro(merged_data, title="Figure 1 (Merged dataset). Structure of variables and missing observations.")
p2 <- plot_missing(merged_data, title="Figure 2(Merged dataset). Breakdown of missing observations.")


# Save it as a PNG file
ggsave("Figure1_MergedDataset.png", plot = p1, width = 8, height = 6, dpi = 300)


ggsave("Figure2_MergedDataset.png", plot = p2, width = 8, height = 6, dpi = 300)

```


```{r message=FALSE, warning=FALSE}
#| label: merged dataframe and character handling
#| echo: true


# print(glimpse(merged_data))
print(table(merged_data$BMDBMIC, useNA = "ifany"))
print(table(merged_data$DIQ010,  useNA = "ifany"))

# ---- Coercion helpers (handle labelled/character) ----
to_num <- function(x) {
  if (is.numeric(x)) return(x)
  xc <- as.character(x)
  n <- suppressWarnings(readr::parse_number(xc))
  if (mean(is.na(n)) > 0.80) {
    xlow <- tolower(trimws(xc))
    n <- dplyr::case_when(
      xlow %in% c("1","yes","yes, told") ~ 1,
      xlow %in% c("2","no","no, not told") ~ 2,
      xlow %in% c("3","borderline") ~ 3,
      xlow %in% c("7","refused") ~ 7,
      xlow %in% c("9","don't know","dont know","unknown") ~ 9,
      TRUE ~ NA_real_
    )
  }
  as.numeric(n)
}

merged_data <- merged_data %>%
  mutate(
    DIQ010   = to_num(DIQ010),
    DIQ050   = to_num(if (!"DIQ050" %in% names(.)) NA_real_ else DIQ050),
    BMXBMI   = suppressWarnings(as.numeric(BMXBMI)),
    RIDAGEYR = suppressWarnings(as.numeric(RIDAGEYR)),
    RIAGENDR = suppressWarnings(as.numeric(RIAGENDR)),
    RIDRETH1 = suppressWarnings(as.numeric(RIDRETH1)),
    SDMVPSU  = suppressWarnings(as.numeric(SDMVPSU)),
    SDMVSTRA = suppressWarnings(as.numeric(SDMVSTRA)),
    WTMEC2YR = suppressWarnings(as.numeric(WTMEC2YR))
  )

# ---- Diagnostics BEFORE save ----
cat("DIQ010 counts BEFORE save:\n")
print(table(merged_data$DIQ010, useNA = "ifany"))
cat("Count with DIQ010 in {1,2}:", sum(merged_data$DIQ010 %in% c(1,2), na.rm = TRUE), "\n")

# ---- Save to file for reuse ----
dir.create("data", showWarnings = FALSE)
# ---- Save ----
dir.create("data", showWarnings = FALSE, recursive = TRUE)
saveRDS(merged_data, "data/merged_2013_2014.rds")
message("Saved: data/merged_2013_2014.rds")


```
**EDA**

-   Used library(survey) to get weighted means and sd of the variables.
    The BMI and age were standardized.
-   Age was recoded into different variabless, including only \>20 years
    in the analysis.
-   BMI is recoded and categorized as-"18.5,18.5‚Äì\<25,25‚Äì\<30,30‚Äì\<35,35‚Äì\<40,‚â•40          years).
-   Ethnicity is recoded as "Mexican American" = "1", "Other Hispanic" =
    "2", "NH White" = "3", "NH Black" = "4", "Other/Multi" = "5"
-   Since special codes are not random, cannot be dropped; the
    informative missingness if ignored (MAR or MNAR) could introduce
    bias. We transformed special codes (3,7,) to NA and included all NAs in
    the analysis. Visulaization of missing data presented below.
-   A final analytic dataset was created ('adult') with "NH White" and "Male" as
    the reference group

```{r}
#| label: Basic Exploration (adults)
## 
# ---------------- Basic Exploration (adults) ----------------

# Keep adults only and build analysis variables
adult <- merged_data %>%
  dplyr::filter(RIDAGEYR >= 20) %>%
  dplyr::transmute(
    # --- keep survey design variables so svydesign() can see them ---
    SDMVPSU, SDMVSTRA, WTMEC2YR,

    # --- outcome: DIQ010 (1 yes, 2 no; 3/7/9 -> NA) ---
    diabetes_dx = dplyr::case_when(
      DIQ010 == 1 ~ 1,
      DIQ010 == 2 ~ 0,
      DIQ010 %in% c(3, 7, 9) ~ NA_real_,
      TRUE ~ NA_real_
    ),

    # --- predictors (raw) ---
    bmi  = BMXBMI,
    age  = RIDAGEYR,

    # sex (1=Male, 2=Female)
    sex  = forcats::fct_recode(factor(RIAGENDR), Male = "1", Female = "2"),

    # race (5-level)
    race = forcats::fct_recode(
      factor(RIDRETH1),
      "Mexican American" = "1",
      "Other Hispanic"   = "2",
      "NH White"         = "3",
      "NH Black"         = "4",
      "Other/Multi"      = "5"
    ),

    # keep DIQ050 so we can safely reference it (may be absent/NA in some rows)
    
    DIQ050 = DIQ050
  ) %>%
  # standardize continuous predictors
  dplyr::mutate(
    age_c = as.numeric(scale(age)),
    bmi_c = as.numeric(scale(bmi)),
    bmi_cat = cut(
      bmi,
      breaks = c(-Inf, 18.5, 25, 30, 35, 40, Inf),
      labels = c("<18.5","18.5‚Äì<25","25‚Äì<30","30‚Äì<35","35‚Äì<40","‚â•40"),
      right = FALSE
    )
  ) %>%
  # adjust outcome: if female & DIQ050==1 ("only when pregnant"), set to 0 (not diabetes)
  dplyr::mutate(
    diabetes_dx = ifelse(sex == "Female" & !is.na(DIQ050) & DIQ050 == 1, 0, diabetes_dx)
  )

# Make NH White the reference level for race (clearer interpretation)
adult <- adult %>%
  dplyr::mutate(
    race = forcats::fct_relevel(race, "NH White")
  )

# --- sanity checks ---
cat("Adults n =", nrow(adult), "\n")



```
 
```{r}
#| label: Data exploration
# data exploration

print(table(adult$diabetes_dx, useNA = "ifany"))
print(table(adult$sex, useNA = "ifany"))
print(table(adult$race, useNA = "ifany"))

if (sum(!is.na(adult$diabetes_dx)) == 0) {
  stop("Too few non-missing outcomes for modeling (n = 0). Check DIQ010 upstream.")
}

# (optional plots omitted for brevity)

# save for downstream
if (!dir.exists("data")) dir.create("data", recursive = TRUE)
saveRDS(adult, "data/adult_cleaned_2013_2014.rds")

```
 
 
 
 
```{r}
#| label: survey design
# survey design
# ---------------- Survey Design ----------------
# Use exam weights because BMI (BMXBMI) is an MEC variable
nhanes_design_adult <- survey::svydesign(
  id = ~SDMVPSU,
  strata = ~SDMVSTRA,
  weights = ~WTMEC2YR,
  nest = TRUE,
  data = adult
)

# quick weighted checks
survey::svymean(~age, nhanes_design_adult, na.rm = TRUE)
survey::svymean(~diabetes_dx, nhanes_design_adult, na.rm = TRUE)


```

** Statistical Methods**

## Modeling

**Multiple Logistic regression** on survey weighted dataset

-We conducted frequentist method **Multiple Logistic regression** on a
survey-weighted dataset, for complete case analysis and data exploration

**Multivariate Imputation by Chained Equations (MICE)**
-   We then conducted MICE to manage missiging data 
-   Considering the small sample size, Multivariate Imputation by
    Chained Equations (MICE) was conducted as an alternative to the
    Bayesian Approach @JSSv045i03

-   Multiple imputation (MI) is an alternative analytic approach for
    small dataset with missingness.

-   Flatness of the density, heavy tails, non-zero peakedness, skewness
    and multimodality do not hamper the good performance of multiple
    imputation for the mean structure in samples n \> 400 even for high
    percentages (75%) of missing data in one variable
    \@@van2012flexible.

-   Multiple Imputation (MI) can be performed using mice package in R
    and adds sampling variability to the imputations.

-   Iterative MICE imputes missing values of one variable at a time,
    using regression models based on the other variables in the dataset.

-   In the chain process, each imputed variable become a predictor for
    the subsequent imputation, and the entire process is repeated
    multiple times to create several complete datasets, each reflecting
    different possibilities for the missing data.

**Bayesian Logistic Regression**

Bayesian statistics is about updating beliefs with evidence:

Posterior ‚àù Likelihood √ó Prior

- Prior (p(Œ∏)): Your initial belief about a parameter before seeing the data.
- Likelihood (p(y|Œ∏)): How probable the observed data are given the parameters.
This is derived from the model (e.g., logistic regression likelihood).
- Posterior (p(Œ∏|y)): Your updated belief about the parameter after seeing the data.

-   We selected Bayesian Logistic Regression since our study response variable is a binary outcome       (Diabetes:yes/no)
-   Bayesian Logistic Regression is based on binomial probability Bayes'
    rules, and predicts probability of disease outcome
-   Bayes analyzes linear relation between the predictor (Age, Race,
    BMI, Gender) and outcome response variable (Diabetes).
-   it considers that predictors and response variables are independent.
-   Regression a of a discrete variable (0 or 1) is a Bernoulli
    probability model that classifies categorical response variables -
    predicting Diabetes.
-   Logit link provides probabilities for the response variable.
-   We use Weakly informative priors Normal (0, 2.5) for logistic regression coefficients and            intercept, Normal(0, 10), allows a wide range of baseline log-odds and helps with convergence        and avoids extreme estimates. Good default for most applications in social, health, or               epidemiological studies.

- In Bayesian statistics, every unknown parameter (like a regression coefficient, mean, or variance) is treated as a random variable with a probability distribution that reflects uncertainty.

- we then summarize in Bayesian results (posterior mean, credible intervals, etc.).

## Model Equation

**Bayesian Logistic Regression model**

$$ \text{logit}(P(Y_i=1)) = \beta_0 + \beta_1 \cdot Age_i + \beta_2 \cdot BMI_i + \beta_3 \cdot Race_i + \beta_4 \cdot Gender_i $$

**Linear Regression equation:**

$$ y_i = \beta_0 + \beta_1 X_1 +\varepsilon_i $$

**Diagnostics performed before Bayesian Regression**

(1) Modeling assumption check
(2) Correlation matrix, Cook's distance, influential points
(3) Model plot


```{r}
#| label: modeling-Survey-weighted complete-case 

# Modeling

library(broom)
library(mice)
library(brms)
library(posterior)
library(bayesplot)
library(knitr)

# --- Guardrails for modeling ---
n_outcome <- sum(!is.na(adult$diabetes_dx))
if (n_outcome == 0) stop("Too few non-missing outcomes for modeling. n = 0")

# Ensure factors and >=2 observed levels among complete outcomes
adult <- adult %>%
  dplyr::mutate(
    sex  = if (!is.factor(sex))  factor(sex)  else sex,
    race = if (!is.factor(race)) factor(race) else race
  )

if (nlevels(droplevels(adult$sex[!is.na(adult$diabetes_dx)]))  < 2)
  stop("sex has <2 observed levels after filtering; check data availability.")
if (nlevels(droplevels(adult$race[!is.na(adult$diabetes_dx)])) < 2)
  stop("race has <2 observed levels after filtering; check Data Prep.")

# ------------------------- 1) Survey-weighted complete-case -------------------------
# Build a logical filter on the original adult data (same length as design$data)
keep_cc <- with(
  adult,
  !is.na(diabetes_dx) & !is.na(age_c) & !is.na(bmi_c) &
  !is.na(sex) & !is.na(race)
)

# Subset the survey design using the logical vector (same length as original)
des_cc <- subset(nhanes_design_adult, keep_cc)

# Corresponding complete-case data (optional)
cc <- adult[keep_cc, ] |> droplevels()
cat("\nComplete-case N for survey-weighted model:", nrow(cc), "\n")

print(table(cc$race))
print(table(cc$diabetes_dx))
print(table(cc$sex))

form_cc <- diabetes_dx ~ age_c + bmi_c + sex + race
svy_fit <- survey::svyglm(formula = form_cc, design = des_cc, family = quasibinomial())
summary(svy_fit)

plot(residuals(svy_fit, type='deviance'))


# Survey-weighted OR table (no intercept)
svy_or <- broom::tidy(svy_fit, conf.int = TRUE) %>%
  dplyr::mutate(OR = exp(estimate), LCL = exp(conf.low), UCL = exp(conf.high)) %>%
  dplyr::select(term, OR, LCL, UCL, p.value) %>%
  dplyr::filter(term != "(Intercept)")
knitr::kable(svy_or, caption = "Survey-weighted odds ratios (per 1 SD)")

```
The residual plot looks okay and does not show any pattern.

```{r}
#| label: MICE
#| include: false
 
# ------------------------- 2) Multiple Imputation (predictors only) 
mi_dat <- adult %>%
  dplyr::select(diabetes_dx, age, bmi, sex, race, WTMEC2YR, SDMVPSU, SDMVSTRA)

meth <- mice::make.method(mi_dat)
pred <- mice::make.predictorMatrix(mi_dat)

# Do not impute outcome
meth["diabetes_dx"] <- ""
pred["diabetes_dx", ] <- 0
pred[,"diabetes_dx"] <- 1

# Imputation methods
meth["age"]  <- "norm"
meth["bmi"]  <- "pmm"
meth["sex"]  <- "polyreg"
meth["race"] <- "polyreg"

# Survey design vars as auxiliaries only
meth[c("WTMEC2YR","SDMVPSU","SDMVSTRA")] <- ""
pred[, c("WTMEC2YR","SDMVPSU","SDMVSTRA")] <- 1

imp <- mice::mice(mi_dat, m = 5, method = meth, predictorMatrix = pred, seed = 123)


```

```{r}
#| label: fit mutated model
#| echo: true
fit_mi <- with(imp, {
  age_c <- as.numeric(scale(age))
  bmi_c <- as.numeric(scale(bmi))
  glm(diabetes_dx ~ age_c + bmi_c + sex + race, family = binomial())
})
pool_mi <- pool(fit_mi)
summary(pool_mi)

## table 

mi_or <- summary(pool_mi, conf.int = TRUE, exponentiate = TRUE) %>%
  dplyr::rename(
    term = term, OR = estimate, LCL = `2.5 %`, UCL = `97.5 %`, p.value = p.value
  ) %>%
  dplyr::filter(term != "(Intercept)")
knitr::kable(mi_or, caption = "MI pooled odds ratios (per 1 SD)")



```

```{r}
#| label: Bayesian model and summary
library(gt)

# 3) Bayesian Logistic Regression (formula weights) 
adult_imp1 <- complete(imp, 1) %>%
  dplyr::mutate(
    age_c  = as.numeric(scale(age)),
    bmi_c  = as.numeric(scale(bmi)),
    wt_norm = WTMEC2YR / mean(WTMEC2YR, na.rm = TRUE),
    # ensure factor refs match survey/MICE:
    race = forcats::fct_relevel(race, "NH White"),
    sex  = forcats::fct_relevel(sex,  "Male")
  ) %>%
  dplyr::filter(!is.na(diabetes_dx), !is.na(age_c), !is.na(bmi_c),
                !is.na(sex), !is.na(race)) %>%
  droplevels()

stopifnot(all(is.finite(adult_imp1$wt_norm)))

priors <- c(
  set_prior("normal(0, 2.5)", class = "b"),
  set_prior("student_t(3, 0, 10)", class = "Intercept")
)

bayes_fit <- brm(
  formula = diabetes_dx | weights(wt_norm) ~ age_c + bmi_c + sex + race,
  data    = adult_imp1,
  family  = bernoulli(link = "logit"),
  prior   = priors,
  chains  = 4, iter = 2000, seed = 123,
  control = list(adapt_delta = 0.95),
  refresh = 0   # quiet Stan output
)

summary(bayes_fit)

prior_summary(bayes_fit)

###
library(ggplot2)

# Example: priors for two coefficients
prior_draws <- tibble(
  term = rep(c("Age (per 1 SD)", "BMI (per 1 SD)"), each = 4000),
  value = c(rnorm(4000, 0, 2.5), rnorm(4000, 0, 2.5))
)

ggplot(prior_draws, aes(x = value, fill = term)) +
  geom_density(alpha = 0.5) +
  theme_minimal() +
  labs(title = "Prior Distributions for Coefficients",
       x = "Coefficient Value", y = "Density") +
  scale_fill_manual(values = c("skyblue", "orange"))

###

library(dplyr)
# install.packages("remotes")  # if you don't have remotes
remotes::install_github("mjskay/tidybayes")


# posterior draws from fitted model
library(posterior)
library(dplyr)
library(tidyr)

# Convert posterior draws to tibble
posterior_draws <- as_draws_df(bayes_fit) %>%   # from posterior package
  select(b_age_c, b_bmi_c) %>%
  as_tibble() %>%
  mutate(draw = row_number()) %>%
  pivot_longer(cols = c(b_age_c, b_bmi_c),
               names_to = "term",
               values_to = "value") %>%
  mutate(type = "Posterior")


# prior draws (simulate)
prior_draws <- tibble(
  term = rep(c("b_age_c", "b_bmi_c"), each = 4000),
  value = c(rnorm(4000, 0, 2.5), rnorm(4000, 0, 2.5)),
  type = "Prior"
)

prior_summary(bayes_fit)

# combine
combined <- bind_rows(prior_draws, posterior_draws)

# plot
ggplot(combined, aes(x = value, fill = type)) +
  geom_density(alpha = 0.5) +
  facet_wrap(~term, scales = "free") +
  theme_minimal() +
  labs(title = "Prior vs Posterior Distributions for Coefficients",
       x = "Coefficient Value", y = "Density")


# Diabetes vs BMI

library(ggplot2)

# Create the plot
p3 <- ggplot(adult_imp1, aes(x = factor(diabetes_dx), y = bmi, fill = factor(diabetes_dx))) +
  geom_boxplot(alpha = 0.7) +
  scale_x_discrete(labels = c("0" = "No Diabetes", "1" = "Diabetes")) +
  labs(
    x = "Diabetes Diagnosis",
    y = "BMI",
    title = "BMI Distribution by Diabetes Status"
  ) +
  theme_minimal() +
  theme(legend.position = "none")

# Save the plot as an image file (PNG)
ggsave("BMI_Distribution_by_Diabetes_Status.png", plot = p3, width = 7, height = 5, dpi = 300)


# logistic regression curve
p4 <- ggplot(adult_imp1, aes(x = bmi, y = diabetes_dx)) +
  geom_point(aes(y = diabetes_dx), alpha = 0.2, position = position_jitter(height = 0.02)) +
  geom_smooth(method = "glm", method.args = list(family = "binomial"), se = TRUE, color = "blue") +
  labs(
    x = "BMI",
    y = "Probability of Diabetes",
    title = "Predicted Probability of Diabetes vs BMI"
  ) +
  theme_minimal()

ggsave("Predicted Probability of Diabetes vs BMI.png", plot = p4, width = 7, height = 5, dpi = 300)







```

Once we get posterior draws, we study
Summary stats	Mean, median, 95% credible intervals	summary(bayes_fit) or posterior_summary(bayes_fit)
- Visualization	Distribution shape	mcmc_hist(posterior, pars = c("b_age")) or mcmc_areas()
- Pairwise plots	Correlations between parameters	mcmc_pairs(posterior)
- Posterior predictive checks	Compare model predictions vs observed data	pp_check(bayes_fit)
- Model comparison	Using LOO or WAIC	loo(bayes_fit) or waic(bayes_fit)

Assumptions for Bayesian logistic regression 
- posterior check
- plots for linearity
- mcmc trace plots for convergence 
- bayes_R2 for model fit

```{r}
#| label: Assumptions (Bayesian)

summary(bayes_fit)
p5 <- plot(bayes_fit)          # Posterior distributions

p6 <- pp_check(bayes_fit)      # Posterior predictive checks
mcmc_trace(bayes_fit)    # Convergence (optional)
bayes_R2(bayes_fit)      # Model fit
    # Leave-one-out cross-validation

ggsave("bayes_fit_plot.png", plot = p5, width = 8, height = 6, dpi = 300)


ggsave("pp_check.png", plot = p6, width = 8, height = 6, dpi = 300)




```

Below are reported odds ratio from the posterior predicted values and the Bayesian regression summary


```{r}
#| label: Posterior ORs and tables
#| echo: true
#| 
# Posterior ORs (drop intercept, clean labels)

bayes_or <- posterior_summary(bayes_fit, pars = "^b_") %>%
  as.data.frame() %>%
  tibble::rownames_to_column("raw") %>%
  dplyr::mutate(
    term = gsub("^b_", "", raw),
    term = gsub("race", "race:", term),
    term = gsub("sex",  "sex:",  term),
    term = gsub("OtherDMulti", "Other/Multi", term),
    term = gsub("OtherHispanic", "Other Hispanic", term),
    OR   = exp(Estimate),
    LCL  = exp(Q2.5),
    UCL  = exp(Q97.5)
  ) %>%
  dplyr::select(term, OR, LCL, UCL) %>%
  dplyr::filter(term != "Intercept")

knitr::kable(
  bayes_or %>%
    dplyr::mutate(dplyr::across(c(OR,LCL,UCL), ~round(.x, 2))),
  digits = 2,
  caption = "Bayesian posterior odds ratios (95% CrI) ‚Äî reference: NH White (race), Male (sex)"
)

```

```{r}
# Combined table

if (!dir.exists("outputs")) dir.create("outputs", recursive = TRUE)
saveRDS(svy_fit,   "outputs/svy_fit.rds")
saveRDS(pool_mi,   "outputs/pool_mi.rds")
saveRDS(bayes_fit, "outputs/bayes_fit.rds")
saveRDS(svy_or,    "outputs/survey_OR_table.rds")
saveRDS(mi_or,     "outputs/mi_OR_table.rds")
saveRDS(bayes_or,  "outputs/bayes_OR_table.rds")
```

```{r}
# Results

# ---- Build compact results table (BMI & Age only) ----
library(dplyr); 
library(tidyr); 
library(knitr); 
library(stringr)

# pretty "OR (LCL‚ÄìUCL)" string

fmt_or <- function(or, lcl, ucl, digits = 2) {
  paste0(
    formatC(or,  format = "f", digits = digits), " (",
    formatC(lcl, format = "f", digits = digits), "‚Äì",
    formatC(ucl, format = "f", digits = digits), ")"
  )
}

# guardrails: require these to exist from Modeling
stopifnot(exists("svy_or"), exists("mi_or"), exists("bayes_or"))
for (nm in c("svy_or","mi_or","bayes_or")) {
  if (!all(c("term","OR","LCL","UCL") %in% names(get(nm)))) {
    stop(nm, " must have columns: term, OR, LCL, UCL")
  }
}

svy_tbl   <- svy_or   %>% mutate(Model = "Survey-weighted MLE")
mi_tbl    <- mi_or    %>% mutate(Model = "MICE pooled")
bayes_tbl <- bayes_or %>% mutate(Model = "Bayesian")

all_tbl <- bind_rows(svy_tbl, mi_tbl, bayes_tbl) %>%
  mutate(term = case_when(
    str_detect(term, "bmi_c|\\bBMI\\b") ~ "BMI (per 1 SD)",
    str_detect(term, "age_c|\\bAge\\b") ~ "Age (per 1 SD)",
    TRUE ~ term
  )) %>%
  filter(term %in% c("BMI (per 1 SD)", "Age (per 1 SD)")) %>%
  mutate(OR_CI = fmt_or(OR, LCL, UCL, digits = 2)) %>%
  select(Model, term, OR_CI) %>%
  arrange(
    factor(Model, levels = c("Survey-weighted MLE","MICE pooled","Bayesian")),
    factor(term,  levels = c("BMI (per 1 SD)","Age (per 1 SD)"))
  )

res_wide <- all_tbl %>%
  pivot_wider(names_from = term, values_from = OR_CI) %>%
  rename(
    `BMI (per 1 SD) OR (95% CI)` = `BMI (per 1 SD)`,
    `Age (per 1 SD) OR (95% CI)` = `Age (per 1 SD)`
  )

kable(
  res_wide,
  align = c("l","c","c"),
  caption = "Odds ratios (per 1 SD) with 95% CIs across models"
)


```

```{r}
# Posterior predictive draws

#Posterior predictive checks (binary outcome)
pp_samples <- posterior_predict(bayes_fit, ndraws = 500)  # 500 draws

# Check dimensions
dim(pp_samples)  # rows = draws, cols = observations

# Plot overlay of observed vs predicted counts (duplicate image)
ppc_dens_overlay(y = adult_imp1$diabetes_dx, yrep = pp_samples[1:50, ]) +
  labs(title = "Posterior Predictive Check: Density Overlay") +
  theme_minimal()

ppc_bars(y = adult_imp1$diabetes_dx, yrep = pp_samples[1:50, ])


# Alternative PP plots (histogram / barplot) for binary outcome (bar chart preferred being discrete outcome)

p7 <- ppc_bars(y = adult_imp1$diabetes_dx, yrep = pp_samples[1:50, ]) +
  labs(title = "Posterior Predictive Check: Barplot of Counts") +
  theme_minimal()

ggsave("ppc_bars.png", plot = p7, width = 8, height = 6, dpi = 300)

#PP check for proportions (useful for binary) # mean comparison
## to check if the simulated means match the observed mean

## mean
p8 <- ppc_stat(y = adult_imp1$diabetes_dx, yrep = pp_samples[1:100, ], stat = "mean") +
  labs(title = "Posterior Predictive Check: Mean of Replicates") +
  theme_minimal()
ggsave("ppc_stat_mean.png", plot = p8, width = 8, height = 6, dpi = 300)

## sd
p9 <- ppc_stat(y = adult_imp1$diabetes_dx, yrep = pp_samples[1:100, ], stat = "sd") +
  labs(title = "PPC: Standard Deviation of Replicates") +
  theme_minimal()
ggsave("ppc_stat_sd.png", plot = p9, width = 8, height = 6, dpi = 300)

# PP checks with bayesplot options
color_scheme_set("blue")
p10 <- ppc_scatter_avg(y = adult_imp1$diabetes_dx, yrep = pp_samples[1:100, ]) +
  labs(title = "Observed vs Predicted (Avg) Posterior Predictive")


ggsave("ppc_scatter_avg.png", plot = p10, width = 8, height = 6, dpi = 300)
```
```{r}
#| label: posterior Visualization


library(brms)
library(dplyr)

# Posterior summary

post_sum <- posterior_summary(bayes_fit)
colnames(post_sum)

post_logodds <- posterior_summary(bayes_fit) %>%
  as.data.frame() %>%
  rownames_to_column(var = "term") %>%
  filter(term != "Intercept") %>%  # drop intercept if desired
  select(term,"Estimate","Est.Error","Q2.5","Q97.5")

post_logodds

# View table
library(ggplot2)

ggplot(post_logodds, aes(x = term, y = Estimate)) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = Q2.5, ymax = Q97.5), width = 0.2) +
  coord_flip() +
  labs(title = "Posterior log-odds for predictors",
       y = "Log-odds", x = "Predictor") +
  theme_minimal()

library(posterior)
library(bayesplot)

# Extract posterior draws as a draws_df # simulate posterior outcomes
post <- as_draws_df(bayes_fit)

# Check parameter names
colnames(post)


# Density overlay for age and bmi
p11 <- mcmc_areas(post, pars = c( "b_age_c","b_bmi_c","b_sexFemale","b_raceMexicanAmerican", "b_raceOtherHispanic","b_raceNHBlack","b_raceOtherDMulti" ))

ggsave("mcmc_areas.png", plot = p11, width = 8, height = 6, dpi = 300)


# Convert to long format
pp_long <- as.data.frame(pp_samples) %>%
  mutate(draw = row_number()) %>%
  pivot_longer(cols = -draw, names_to = "obs", values_to = "predicted") %>%
  mutate(obs = as.integer(obs))

# Add BMI and observed outcome from original dataset
pp_long <- pp_long %>%
  left_join(
    adult_imp1 %>% mutate(obs = row_number()) %>% select(obs, bmi, diabetes_dx),
    by = "obs"
  )

###

predicted <- fitted(bayes_fit, summary = TRUE)
observed <- adult_imp1[, c("bmi", "age")]

# Plot for bmi (obs vs pred)
library(ggplot2)
p12 <- ggplot(data = NULL, aes(x = observed$bmi, y = predicted[, "Estimate"])) +
  geom_point() +
  geom_errorbar(aes(ymin = predicted[, "Q2.5"], ymax = predicted[, "Q97.5"])) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  xlab("Observed bmi") + ylab("Predicted bmi")

ggsave("plot_bmi_obs_pred.png", plot = p12, width = 8, height = 6, dpi = 300)

###

library(ggplot2)
library(dplyr)

# Combine observed and predicted into one data frame
plot_data <- adult_imp1 %>%
  mutate(
    predicted_bmi = predicted[, "Estimate"],
    lower_ci = predicted[, "Q2.5"],
    upper_ci = predicted[, "Q97.5"],
    obs_index = 1:nrow(adult_imp1)  # index for x-axis
  )

# Line plot
p13 <- ggplot(plot_data, aes(x = obs_index)) +
  geom_line(aes(y = bmi, color = "Observed")) +               # observed BMI
  geom_line(aes(y = predicted_bmi, color = "Predicted")) +   # predicted BMI
  geom_ribbon(aes(ymin = lower_ci, ymax = upper_ci), alpha = 0.2) +  # uncertainty
  labs(x = "Observation", y = "BMI", color = "Legend") +
  theme_minimal()

ggsave("Line plot_bmi_obs_pred.png", plot = p13, width = 8, height = 6, dpi = 300)


###
summary(adult_imp1$bmi)
summary(plot_data$bmi_c)



```
# Results 

1.  **Multiple Linear Regression**

    Model**:**
    `svyglm(formula = form_cc, design = des_cc, family = quasibinomial())`

All predictors are significant: **age (p \< 0.001)** and **BMI (p \<
0.001)** show strong positive associations with the outcome, while
**being female (p = 0.0004)** is negatively associated. Other
significant associations include **raceMexican American (p = 0.0008)**,
**raceOther Hispanic (p = 0.0087)**, **raceNH Black (p = 0.0117)**, and
**raceOther/Multi (p = 0.0014)**.

2.  **MICE**

    All predictors are statistically significant.

    Positive associations: age (p \< 0.001), BMI (p \< 0.001), and all
    race categories compared to reference.

    Negative association: being female (p \< 0.001)

<!-- -->

3.  **Bayesian Regression**

    Sampling**:** NUTS (4 chains, 2000 iterations each; 1000 warmup,
    4000 post-warmup draws)

    Convergence & Diagnostics

    -   Rhat = 1.00 for all parameters ‚Üí excellent convergence

    -   Bulk_ESS / Tail_ESS: Large values (\>2000) ‚Üí high effective
        sample sizes, reliable posterior estimates.

        Interpretation

    -   Strong predictors: Age and BMI are strongly positively
        associated with diabetes risk.

    -   Sex effect: Females have a lower probability of diabetes
        compared to males
    -   R¬≤ = 0.13 shows 13% of the variance in the outcome (diabetes_dx) is explained by your predictors          (age, BMI, sex, race), 95% Credible Interval: 0.106‚Äì0.156, indicates that, given your model and           data, the true proportion of variance explained is plausibly between 10.6% and 15.6% and shows            uncertainty in the explained variance, which is natural for probabilistic models.
    -   Est.Error = 0.0127, reflects the standard error of the R¬≤ estimate across posterior samples. The          small SE indicates that the R¬≤ estimate is fairly precise.

    <!-- -->

    -   Race/ethnicity: Mexican American, NH Black, and ‚ÄúOther/Diverse‚Äù
        groups have higher odds of diabetes. Other Hispanic group has a
        less certain effect.

    <!-- -->

    -   age_c-Each 1-unit increase in centered age increases the
        log-odds of diabetes by 1.09. Strong positive effect.

    -   bmi_c-Higher BMI is associated with higher diabetes risk.
        sexFemale-Females have lower odds of diabetes compared to males.

    -   raceMexicanAmerican-Higher odds of diabetes vs. reference race
        (likely NH White)

    -   raceOtherHispanic-Slightly higher odds vs reference, but
        interval crosses zero ‚Üí uncertain effect.

    -   raceNHBlack-Significantly higher odds of diabetes compared to
        reference. raceOtherDMulti-Higher odds of diabetes vs reference
        group.

**Posterior distribution** of all parameters in the model.
(1)Density plot of posterior samples each parameter (e.g., intercept, slope) into a smoothed density curve, showing most of the posterior probability mass lies for best estimates and uncertainty.

**Posterior Predictive Distribution** - generated from posterior predictive draws:
ùë¶rep‚àºùëù(ùë¶new‚à£ùúÉ)yrep‚Äã‚àºp(ynew‚Äã‚à£Œ∏)
simulate the data given posterior parameter estimates.Posterior predictive checks (PPC) compare these simulations to real data to assess model fit.

**Incorporating Uncertainty** two sources of uncertainty:
Parameter uncertainty: captured in the posterior distributions
Predictive uncertainty: captured in posterior predictive draws

Combining the two provide credible intervals for predictions, not just point predictions and specifies - Given the BMI, the probability of diabetes is likely between 40‚Äì55%.‚Äù

**Comparing Models**

-   All three models (survey-weighted MLE, multiple imputation,
    Bayesian) agree closely on the direction and magnitude of the
    effects of BMI and age.

-   Age is a stronger predictor than BMI, nearly tripling the odds per 1
    SD.

-   BMI significantly increases diabetes risk (\~1.7‚Äì1.9√ó per 1 SD).

-   Differences between models are minor, indicating robust and reliable
    findings despite missing data or modeling approach.

# Conclusion

-   Across multiple modeling approaches‚Äîsurvey-weighted maximum
    likelihood, multiple imputation, and Bayesian regression‚Äîboth age
    and BMI were consistently strong predictors of diabetes. Each
    standard deviation increase in age nearly tripled the odds of
    diabetes, while a similar increase in BMI elevated the odds by
    approximately 1.7‚Äì1.9 times. The consistency of these results across
    models highlights the robustness of the associations and underscores
    the importance of age and BMI as key risk factors for diabetes in
    this population.
    
Effect stability: point estimates in rhe Bayesian model‚Äôs closely aligned with those from the frequentist,  indicating that prior regularization stabilized the estimates in the presence of modest missingness.

Uncertainty quantification: Bayesian credible intervals of odds ration were slightly narrower yet overlapped the frequentist confidence intervals, suggest comparable inferential precision while offering improved interpretability.

Design considerations: 
# Survey-weighted MLE (Maximum Likelihood Estimator) 
- incorporates each observation weighted according to its survey weight.
- provide estimates that reflect the population-level parameters, not just the sample- produces population-representative estimates.
# Bayesian model with normalized weights- 
- instead of fully modeling the survey design, it used normalized sampling weights as importance weights
- the scaled weights that sum to the sample size approximates the effect of survey weights, but does not fully account for: Stratification, clustering, design-based variance adjustments.
- Bayesian inference treats the weighted likelihood as from a regular model, ignoring some survey design features.


# Discussions

The use of multiple imputation allowed for robust analysis despite
missing data, increasing power and reducing bias. Comparison of
frequentist and Bayesian models demonstrated consistency in significant
predictors, while Bayesian approaches provided the advantage of
posterior distributions and probabilistic interpretation. The =

Across all models, both age and BMI emerged as strong and consistent
predictors of diabetes. The consistency across modeling approaches
strengthens the validity of these findings Multiple imputation accounted
for potential biases due to missing data, and Bayesian modeling provided
robust credible intervals that closely matched frequentist estimates.
align with previous epidemiological research indicating that increasing
age and higher BMI are among the most important determinants of type 2
diabetes risk.Cumulative exposure to metabolic and lifestyle risk factors over time,
and the role of excess adiposity and insulin related effects account for
diabetes.

Survey weighted dataset strenghthens ensuring population
representativeness, multiple imputation to handle missing data, and
rigorous Bayesian estimation provided high effective sample sizes and RÃÇ ‚âà 1.00 across parameters confirmed excellent model convergence. Bayesian logistic regression provided inference statistically  consistent and interpretable achieving the aim of this study. In future research hierarchical model using NHANES cycles or adding variables (lab tests) could assess nonlinear effects of metabolic risk factors.

# Limitations 

Our study was a cross-sectional study design - precludes potential residual confounding
from unmeasured factors such as diet, physical activity, and genetic
predisposition

# Implications
- age and BMI as robust and independent predictors of
diabetes, underscore the importance of early targeted interventions in
mitigating diabetes risk. 
- Longitudinal studies and combining other statistical
analytical methods with Bayesian can further enhance and provide better
informed precision prevention strategies.

## 

```{r}
#| label: posterior EDA



prior_summary(bayes_fit)
prior_draws <- tibble(
  term = rep(c("BMI (per 1 SD)", "Age (per 1 SD)"), each = 4000),
  estimate = c(rnorm(4000, 0, 1), rnorm(4000, 0, 1)),
  type = "Prior"
)


summary(prior_draws)



post
head(post)
names(prior_draws)
names(post_logodds)


library(brms)
library(dplyr)
library(tidyr)

# Extract posterior draws
post <- as_draws_df(bayes_fit) %>%      # bayes_fit = your brms model
  select(b_bmi_c, b_age_c) %>%               # select your coefficient columns
  pivot_longer(
    everything(),
    names_to = "term",
    values_to = "estimate"
  ) %>%
  mutate(
    term = case_when(
      term == "b_bmi_c" ~ "BMI (per 1 SD)",
      term == "b_age_c" ~ "Age (per 1 SD)"
    ),
    type = "Posterior"
  )


## visualization of prior and predicted draws
combined_draws <- bind_rows(prior_draws, post) 

library(ggplot2)

ggplot(combined_draws, aes(x = estimate, fill = type)) +
  geom_density(alpha = 0.4) +
  facet_wrap(~ term, scales = "free", ncol = 2) +
  theme_minimal(base_size = 13) +
  labs(
    title = "Prior vs Posterior Distributions",
    x = "Coefficient estimate",
    y = "Density",
    fill = ""
  )


## till now we compared prior and posterior distribution and have posterior summaries calculated


```

```{r}
#| label: Post predicited outcome







```















