---
title: "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013â€“2014)"
subtitle: "Capstone Presentation"
author:
  - "Namita Mishra"
  - "Autumn Wilcox"
advisor: "Dr. Ashraf Cohen"
format:
  revealjs:
    theme: simple
    slide-number: true
    transition: slide
    css: docs/slides.css
    incremental: false
    code-fold: true
    code-summary: "Show the code"
bibliography: references.bib
execute:
  warning: false
  message: false
  echo: false
---

```{=html}
<style>
.reveal {
  font-size: 34px; /* Change all slide text */
}
.reveal h1 {
  font-size: 40px; /* Title font */
}
</style>
```

### Aim

-   Early identification of risk factors is key to diabetes diagnosis and prevention.

-   Predict diabetes using Bayesian logistic regression.

### Frequentist Methods

-   Maximum likelihood estimation is unstable with missing data, quasi-separation, or small samples.

-   Probability is interpreted as long-run relative frequency.

-   To the frequentist, parameter is an unknown constant.

------------------------------------------------------------------------

### Bayesian Approach

-   Flexible and regularizes estimates.
-   Quantifies uncertainty under missingness or imputation (Baldwin & Larson, 2017; Kruschke & Liddell, 2017).
-   Incorporates priors and provides credible intervals via MCMC to predict patient health outcomes.
-   Don't know the value of the parameter, it's a random variable
-   We model our uncertainty with a probability distribution ![](images/clipboard-2228883614.png){width="37" height="20"}, called the prior distribution.
-   Probability represents a degree of belief.
-   It represents the statisticians belief about before observing the data.
-   Supports model checking, variable selection, and uncertainty quantification

------------------------------------------------------------------------

### Bayesian Model

::: panel-tabset
### Bayesâ€™ theorem

**Bayesâ€™ theorm** is based on conditional probability.

### Bayesian Inference

Bayesian inference estimates the **posterior probability** of a parameter:

$$
P(\theta \mid D) = \frac{P(D \mid \theta)\, P(\theta)}{P(D)}
$$

where:

-   ( P(\theta \mid D) ): Posterior â€” probability of the parameter given data\
-   ( P(D \mid \theta) ): Likelihood â€” probability of observing data given the parameter\
-   ( P(\theta) ): Prior â€” belief about the parameter before seeing data\
-   ( P(D) ): Marginal likelihood (normalizing constant)
:::

------------------------------------------------------------------------

**Bayesian regression and Coefficient estimation**

-   Logistic link function used for binary outcomes.
-   logit (ğ‘ƒ(ğ‘Œ=1))=ğ›½0+ğ›½1age ğ‘ + ğ›½ 2 bmi ğ‘ + ğ›½ 3 sex + ğ›½ 4 race
-   Intercept prior: student_t(3, 0, 10) â€” heavy tails for flexibility (Van de Schoot et al., 2013).
-   Regression coefficients prior: normal(0, 2.5) â€” weakly informative, constraining extreme values (Van de Schoot et al., 2021).

------------------------------------------------------------------------

### Data Source

::: panel-tabset
### Data

National Health and Nutrition Examination Survey (NHANES) 2013â€“2014\
**Design:** Complex multistage probability sampling; survey weights applied. **Response Variable** - **diabetes_dx (binary)**\
- Based on **DIQ010**: â€œHas a doctor told you that you have diabetes?â€\
- Excluded **DIQ050 (insulin use)** to avoid clinical confounding. **Predictor Variables** - **RIDAGEYR â€” Age (continuous)** - adults 20â€“80 years (per 1 SD)\
- **BMXBMI â€” Body Mass Index (continuous)** 1 SD increase\
- **RIAGENDR â€” Sex (factor)**\
- **RIDRETH1 â€” Race/Ethnicity (factor)**\
- *Mexican American*, *Other Hispanic*, *Non-Hispanic White*,\
*Non-Hispanic Black*, *Other/Multi-racial*

### Data view

![](images/clipboard-3274096863.png){width="795"}
:::

------------------------------------------------------------------------

### Missing data analysis {.panel-tabset}

::: panel-tabset
### Plot

![](images/clipboard-2873800077.png){width="427"}

### Abnormalities:

-   Missing Data Assessment - Overall missingness â‰ˆ 4%
-   No variable is completely missing
-   Missingness likely MAR (Missing At Random)
-   Clustered mainly in BMI (4.3%) and diabetes_dx (3.1%).
:::

------------------------------------------------------------------------

### Multiple Imputation by Chained Equations (MICE)

:::: nonincremental
*van Buuren & Groothuis-Oudshoorn, 2011; van Buuren, 2012*

Bayesian models assume complete data. **Multivariate Imputation by Chained Equations (MICE)** provides multiple realistic versions of missing data. Pooling across posterior distributions yields the correct full posterior:

Posterior (parameters + missing data uncertainty)

::: panel-tabset
### MICE

-   Iteratively imputes incomplete variables using regression models.
-   PMM for continuous; logistic regression for binary variables.
-   MICE procedure ran 5 iterations for each of the 5 imputed datasets to ensure stability and convergence.
-   Estimates pooled using Rubinâ€™s rules.

### Fit logistic regression model

``` r
fit_mi <- with(data = imp, exp = glm(diabetes_dx ~ age_c + bmi_c + sex + race, family = binomial()))
pool_mi <- pool(fit_mi)
```
:::
::::

------------------------------------------------------------------------

**Bayesian Logistic Regression**

A Bayesian logistic regression model was then fitted to each imputed dataset in **R** using brms **(Stan backend)**, 2000 itirations

-   Prior: **normal(0, 2.5)** for coefficients **student_t(3, 0, 10)** for Intercept

``` r
library(gt)
priors <- c(
  set_prior("normal(0, 2.5)", class = "b"),
  set_prior("student_t(3, 0, 10)", class = "Intercept")
)

bayes_fit <- brm(
  formula = diabetes_dx | weights(wt_norm) ~ age_c + bmi_c + sex + race,
  data    = adult_imp1,
  family  = bernoulli(link = "logit"),
  prior   = priors,
  chains  = 4, iter = 2000, seed = 123,
  control = list(adapt_delta = 0.95),
  refresh = 0
)
```

------------------------------------------------------------------------

### MCMC (Markov Chain Monte Carlo)

-   Algorithms used to draw samples from a probability distribution.
-   The Gibbs sampler produces a sequence of random vectors ![](images/clipboard-1640325740.png){width="90" height="35"}. Each one depends on the past only through the most recent one. It's a Markov process. The samples are not just random but are drawn in a way that the long-term distribution of the samples matches the target posterior distribution.
-   It uses a burn in period. The random vectors are sequentially dependent. Retain one parameter vector every n iterations, and discard the rest.

------------------------------------------------------------------------

### Posterior distribution

Posterior draws = 4000 (4 chains Ã— 1000 iterations) for posterior predeictive checks for -

-   The distribution of predictors after seeing the data.
-   The posterior is the conditional distribution of the parameter given the data
-   Subjectivity is present when it is based on the prior distribution which influences the conclusions.
-   The influence of the prior goes to zero as the sample size increases.

------------------------------------------------------------------------

### Bayesian Model Diagnostics

:::: panel-tabset
### MCMC

-   Trace plots indicate convergence (no drift across iterations).
-   Rhat â‰ˆ 1 â†’ good chain mixing.
-   Effective sample size is adequate across parameters.

### Posterior Estimates

::: nonincremental
-   Intercept -2.66 \[-2.84, -2.50\] Baseline log-odds
-   Age_c 1.09 \[0.97, 1.22\] â†‘ Age increases diabetes risk
-   BMI_c 0.88 \[0.76, 1.01\] Higher BMI predicts diabetes
-   All predictors show positive associations.
-   Narrow credible intervals â†’ precise estimates (Predictor Estimate 95% CI )
-   CIs exclude zero â†’ statistically significant effects.
:::
::::

------------------------------------------------------------------------

### Posterior Predictive Distribution

::: panel-tabset
### Code

``` r
library(ggplot2)

ggplot(combined_draws, aes(x = estimate, fill = type)) +
  geom_density(alpha = 0.4) +
  facet_wrap(~ term, scales = "free", ncol = 2) +
  theme_minimal(base_size = 13) +
  labs(
    title = "Prior vs Posterior Distributions",
    x = "Coefficient estimate",
    y = "Density",
    fill = ""
  )
```

### Plot

Prior vs Posterior Distributions ![](images/clipboard-1978292710.png){width="466"}

### Interpretation

-   Strong positive relationship between age, BMI, and diabetes probability.

-   **Posterior predictive checks** confirm good model fit.

-   Imputation reduced bias and improved robustness.

### R Square

```         
Model fit: bayes_R2(bayes_fit)
```

```         
    Estimate        Est.Error       Q2.5        Q97.5 
    R2 0.1313342    0.01265055    0.1064607     0.156078
```
:::

------------------------------------------------------------------------

![](images/clipboard-4013951629.png){width="576"}

![](images/clipboard-3433503019.png){width="466"}

------------------------------------------------------------------------

**Posterior pairwise correlation plot**

::: panel-tabset
### Code

``` r

# Extract posterior draws
post <- as_draws_df(bayes_fit)

# Select numeric parameters of interest
post_subset <- post %>% 
  dplyr::select(b_age_c, b_bmi_c, b_sexFemale, 
                b_raceMexicanAmerican, b_raceNHBlack)

# Compute correlation matrix
cor_matrix <- cor(post_subset)

# Visualize
mcmc_pairs(as_draws_array(bayes_fit), 
           pars = c("b_age_c", "b_bmi_c", "b_sexFemale"),
           off_diag_args = list(size = 1.5, alpha = 0.4))
           
```

### Plot

![](images/clipboard-1737198687.png)

### Interpretation

-   The absence of strong linear relationships among parameters suggests that age, BMI, and sex independently contributed to the prediction of diabetes status.
-   The smooth, unimodal histograms along the diagonals confirmed stable model convergence and well-behaved posterior samples.
-   A mild negative tilt between b_age_c and b_sexFemale: a slight negative correlation, meaning as the posterior estimate for age increases, the effect of being female slightly decreases.
-   However, this pattern is weak â€” confirming that both predictors contribute distinct information to the diabetes outcome.
:::

------------------------------------------------------------------------

### Autocorrelation for age and bmi

::: panel-tabset
### Plot

![](images/clipboard-4119356386.png){width="489"}

### Code

``` r
# Convert fitted model to draws array
post_array <- as_draws_array(bayes_fit)  # draws x chains x parameters

# Plot autocorrelation for age and bmi
mcmc_acf(post_array, pars = c("b_age_c", "b_bmi_c"))
```

### Interpretation

-   Plot of age and BMI coefficients - assess chain mixing and convergence, showing correlation of each draw with its lagged values across iterations.
-   Rapid decay towards zero: Markov chains are mixing well, and successive draws are relatively independent.
-   Both age and BMI coefficients exhibited low autocorrelation after a few lags,
-   Bayesian model sampling was adequate and stable, with reliable and valid inference from the posterior distributions.
:::

------------------------------------------------------------------------

### Model Check: Comparison of Odds Ratios Across Models

![](images/clipboard-329828759.png){width="363"}

-   Age and BMI are consistently strong risk factors for diabetes in this population.
-   The Bayesian model complements frequentist approaches by providing stable, interpretable, and uncertainty-quantified estimates, while broadly reproducing population-level prevalence.
-   The Bayesian model used normalized weights, which approximates the effect of survey weights but does not fully account for stratification, clustering, or design-based variance adjustments.

------------------------------------------------------------------------

### Assumptions for Bayesian Logistic Regression

::: nonincremental
-   Data Binary outcome
-   Independent observations
-   Relationship Linear in logit
-   No perfect collinearity
-   Priors Properly chosen: informative enough
-   Posterior Proper and convergent Fit
-   No complete separation
-   Good predictive checks
:::


---------------------------------------------------------------------

### Targeted Therapy


### Translational Perspective 
-  This project further demonstrates the translational potential of Bayesian modeling in clinical decision-making and public health strategy
-  By using patient-level predictors such as age, BMI, sex, and race to estimate the probability of diabetes, the model moves beyond descriptive statistics toward individualized risk prediction
- The translational move lies in converting these probabilistic outputs into actionable thresholdsâ€”such as identifying the BMI or age at which the predicted risk of diabetes exceeds a clinically meaningful level (e.g., 30%).
-  Only BMI is a modifiable risk factor
We can make changes in BMI (behavior or lifestyle) to achieve a lower risk threshold
we hold non modifiable predictors as constant (sex, race).


### 
---------------------------
---------------------------------------------------------------------------
### Internal validation

::: panel-tabset
### Personalized risk estimation
-  To illustrate personalized risk estimation using the Bayesian model, we computed the posterior predicted probability of diabetes for a representative participant.
-  We selected one participant from the dataset (adult[1, ]) including all relevant covariates (age, BMI, sex, race).
Used posterior_linpred with transform = TRUE to obtain predicted probabilities for logistic regression
-  Extracted posterior draws computed 95% credible interval from the posterior draws.

### Code
``` r
# Use the first participant 
# using multiple covariates to select someone
participant1_data  <- adult[1, ]
# predicted probabilities for patient 1
phat1 <- posterior_linpred(bayes_fit, newdata = participant1_data, transform = TRUE)
# 'transform = TRUE' gives probabilities for logistic regression
# Store in a data frame for plotting
post_pred_df <- data.frame(pred = phat1)
# Compute 95% credible interval
ci_95_participant1 <- quantile(phat1, c(0.025, 0.975))
# Plot
ggplot(post_pred_df, aes(x = pred)) + 
  geom_density(color='darkblue', fill='lightblue') +
  geom_vline(xintercept = ci_95_participant1[1], color='red', linetype='dashed') +
  geom_vline(xintercept = ci_95_participant1[2], color='red', linetype='dashed') +
  xlab('Probability of being diabetic (Outcome=1)') +
  ggtitle('Posterior Predictive Distribution 95% Credible Interval') +
  theme_bw()
```

### Interpretation
-  Plot shows the posterior predictive distribution for the probability that a specific individual in your dataset is diabetic (Outcome = 1)

(1) Median/mean predicted probability â‰ˆ 0.25
-  The peak of the density curve is around 25%: participant has a 1 in 4 chance of being diabetic.

(2) 95% credible interval â‰ˆ 0.20 to 0.31
-  The red dashed lines represent the 95% credible interval (CI):

Lower bound â‰ˆ 0.20
Upper bound â‰ˆ 0.31

Given the model and the participantâ€™s characteristics, there is a 95% probability that this participantâ€™s true diabetes risk lies between 20% and 31%.

:::
------------------------------------------------------------------------

### Limitations

::: nonincremental
-   **Cross-sectional** NHANES design limits causal inference.

-   Multiple **imputation** assumes MAR; MNAR may bias results.

-   Residual **confounding** from unmeasured factors (diet, activity, SES, genetics).

-   Bayesian estimates may be influenced by **prior choices**.

-   **BMI measured once**; may not capture long-term exposure.

-   **Self-reported** variables may introduce recall/reporting bias.

-   **Interaction effects** (e.g., age Ã— BMI) not tested.
:::

------------------------------------------------------------------------

### Conclusion

Bayesian logistic regression effectively models uncertainty.

MICE improved data completeness and reliability.

Posterior predictions provide interpretable probabilities of diabetes risk.

Framework adaptable to other outcomes (e.g., hypertension, obesity).

------------------------------------------------------------------------

### References

van Buuren, S. & Groothuis-Oudshoorn, K. (2011). MICE: Multivariate Imputation by Chained Equations in R.

Gelman, A. & Hill, J. (2007). Data Analysis Using Regression and Multilevel/Hierarchical Models.

McElreath, R. (2020). Statistical Rethinking: A Bayesian Course with Examples in R and Stan.

NHANES Data Documentation (CDC).

Van de Schoot, R. et al. (2013, 2021). Weakly Informative Priors in Bayesian Regression.

------------------------------------------------------------------------

### **Thank You**

**Dr. Ashraf Cohen, PhD, MS**

-   University of West Florida
-   Department of Mathematics and Statistics

------------------------------------------------------------------------
