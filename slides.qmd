---
title: "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013‚Äì2014)"
subtitle: "Capstone Presentation"
author:
  - "Namita Mishra"
  - "Autumn Wilcox"
advisor: "Dr. Ashraf Cohen"
format:
  revealjs:
    theme: simple
    slide-number: true
    transition: slide
    css: docs/slides.css
    incremental: false
    code-fold: true
    code-summary: "Show the code"
bibliography: references.bib
execute:
  warning: false
  message: false
  echo: false
---

```{=html}
<style>
.reveal {
  font-size: 32px; /* Change all slide text */
}
.reveal h1 {
  font-size: 40px; /* Title font */
}
</style>
```

### Aim

-   Early identification of risk factors is key to diabetes diagnosis and prevention.

-   Predict diabetes using Bayesian logistic regression.

### Frequentist Methods

-   Maximum likelihood estimation is unstable with missing data, quasi-separation, or small samples.

-   Probability is interpreted as long-run relative frequency.

-   To the frequentist, parameter is an unknown constant.

------------------------------------------------------------------------

### Bayesian Approach

-   Flexible and regularizes estimates.
-   Quantifies uncertainty under missingness or imputation (Baldwin & Larson, 2017; Kruschke & Liddell, 2017).
-   Incorporates priors and provides credible intervals via MCMC to predict patient health outcomes.
-   Don't know the value of the parameter, it's a random variable
-   We model our uncertainty with a probability distribution ![](images/clipboard-2228883614.png){width="37" height="20"}, called the prior distribution.
-   Probability represents a degree of belief.
-   It represents the statisticians belief about before observing the data.
-   Supports model checking, variable selection, and uncertainty quantification

------------------------------------------------------------------------

### Bayesian Model

::: panel-tabset
### Bayes‚Äô Theorem

**Bayes‚Äô theorm** is based on conditional probability.

### Bayesian Inference

Bayesian inference estimates the **posterior probability** of a parameter:

$$
P(\theta \mid D) = \frac{P(D \mid \theta)\, P(\theta)}{P(D)}
$$

where:

-   ( P(\theta \mid D) ): Posterior ‚Äî probability of the parameter given data\
-   ( P(D \mid \theta) ): Likelihood ‚Äî probability of observing data given the parameter\
-   ( P(\theta) ): Prior ‚Äî belief about the parameter before seeing data\
-   ( P(D) ): Marginal likelihood (normalizing constant)
:::

------------------------------------------------------------------------

**Bayesian Regression and Coefficient Estimation**

-   Logistic link function used for binary outcomes.
-   logit (ùëÉ(ùëå=1))=ùõΩ0+ùõΩ1age ùëê + ùõΩ 2 bmi ùëê + ùõΩ 3 sex + ùõΩ 4 race
-   Intercept prior: student_t(3, 0, 10) ‚Äî heavy tails for flexibility (Van de Schoot et al., 2013).
-   Regression coefficients prior: normal(0, 2.5) ‚Äî weakly informative, constraining extreme values (Van de Schoot et al., 2021).

------------------------------------------------------------------------

### Data Source

::: panel-tabset
### Data

National Health and Nutrition Examination Survey (NHANES) 2013‚Äì2014\
**Design:** Complex multistage probability sampling; survey weights applied. **Response Variable** - **diabetes_dx (binary)**\
- Based on **DIQ010**: ‚ÄúHas a doctor told you that you have diabetes?‚Äù\
- Excluded **DIQ050 (insulin use)** to avoid clinical confounding. **Predictor Variables** - **RIDAGEYR ‚Äî Age (continuous)** - adults 20‚Äì80 years (per 1 SD)\
- **BMXBMI ‚Äî Body Mass Index (continuous)** 1 SD increase\
- **RIAGENDR ‚Äî Sex (factor)**\
- **RIDRETH1 ‚Äî Race/Ethnicity (factor)**\
- *Mexican American*, *Other Hispanic*, *Non-Hispanic White*,\
*Non-Hispanic Black*, *Other/Multi-racial*

### Data view

![](images/clipboard-4180967765.png){width="660"}
:::

------------------------------------------------------------------------

### Missing Data Analysis {.panel-tabset}

::: panel-tabset
### Plot

![](images/clipboard-2873800077.png){width="427"}

### Abnormalities

-   Missing Data Assessment - Overall missingness ‚âà 4%
-   No variable is completely missing
-   Missingness likely MAR (Missing At Random)
-   Clustered mainly in BMI (4.3%) and diabetes_dx (3.1%).

### Complete Case Analysis

#### Survey-Weighted Diabetes Risk (U.S. Adults)

**diabetes_dx‚àºage_c + bmi_c + sex + race**
Interpretation
-   model was estimated using svyglm(),**quasibinomial family** for binary outcome for NHANES design.
-  Design-adjusted coefficient estimates, standard errors, and odds ratios reflect population-representative associations for U.S. adults.
-   Age (per 1 SD ‚Üë): OR ‚âà 3.03 ‚Üí Older adults \~3√ó higher odds
-   BMI (per 1 SD ‚Üë): OR ‚âà 1.88 ‚Üí Higher BMI increases risk
-   Sex (Female vs Male): OR ‚âà 0.53 ‚Üí Females \~47% lower odds
-   Race (vs NH White):
-   Mexican American: OR ‚âà 2.04
-   Other Hispanic: OR ‚âà 1.59
-   NH Black: OR ‚âà 1.67
-   Other/Multi-racial: OR ‚âà 2.33

Age and BMI are strongest predictors Females have lower risk Certain racial/ethnic groups have higher odds

### Code

``` r
form_cc <- diabetes_dx ~ age_c + bmi_c + sex + race
svy_fit <- survey::svyglm(formula = form_cc, design = des_cc, family = quasibinomial())
```

:::

------------------------------------------------------------------------

### Multiple Imputation by Chained Equations (MICE)

:::: nonincremental
*van Buuren & Groothuis-Oudshoorn, 2011; van Buuren, 2012*

Bayesian models assume complete data. **Multivariate Imputation by Chained Equations (MICE)** provides multiple realistic versions of missing data. Pooling across posterior distributions yields the correct full posterior:

Posterior (parameters + missing data uncertainty)

::: panel-tabset
### MICE

-   Iteratively imputes incomplete variables using regression models.
-   PMM for continuous; logistic regression for binary variables.
-   MICE procedure ran 5 iterations for each of the 5 imputed datasets to ensure stability and convergence.
-   Estimates pooled using Rubin‚Äôs rules.


:::
::::

------------------------------------------------------------------------

**Bayesian Logistic Regression**

A Bayesian logistic regression model was then fitted to each imputed dataset in **R** using brms **(Stan backend)**, 2000 itirations

Prior

-  **normal(0, 2.5)** for coefficients 
-  **student_t(3, 0, 10)** for Intercept

``` r
library(gt)
priors <- c(
  set_prior("normal(0, 2.5)", class = "b"),
  set_prior("student_t(3, 0, 10)", class = "Intercept")
)

bayes_fit <- brm(
  formula = diabetes_dx | weights(wt_norm) ~ age_c + bmi_c + sex + race,
  data    = adult_imp1,
  family  = bernoulli(link = "logit"),
  prior   = priors,
  chains  = 4, iter = 2000, seed = 123,
  control = list(adapt_delta = 0.95),
  refresh = 0
)
```

------------------------------------------------------------------------

### MCMC (Markov Chain Monte Carlo)

::: panel-tabset
### Markov Process

-   Gibbs sampler produces a sequence of random vectors ![](images/clipboard-1640325740.png){width="90" height="35"}
-   each depends on the past only through the most recent one.
-   The samples are not just random but are drawn in a way that the long-term distribution of the samples matches the target posterior distribution.
-   It uses a burn in period.
-   The random vectors are sequentially dependent.
-   Retain one parameter vector every n iterations, and discard the rest.

### Code

``` r
post <- posterior_summary(bayes_fit, robust = FALSE) %>%  as_tibble(rownames = "term")
# Extract convergence diagnostics
diag <- rstan::monitor(as.array(bayes_fit$fit), print = FALSE) %>%
  as_tibble(rownames = "term") %>%
  select(term, Rhat, n_eff = Bulk_ESS)

bayes_results <- post %>%  left_join(diag, by = "term") %>%  mutate(
    OR       = exp(Estimate),
    OR_LCL   = exp(Q2.5),
    OR_UCL   = exp(Q97.5)
  ) %>%  select(term,    Estimate,Est.Error,Q2.5, Q97.5,Rhat, n_eff, OR,  OR_LCL,  OR_UCL )
```

### Results

![](images/clipboard-1727290712.png){width="443"}

### Diagnostics

-   R-hat ‚âà 1 ‚Üí good chain mixing
-   Effective sample sizeTrace plots
-   Trace plots indicate convergence (no drift across iterations).
-   Autocorrelation (mcmc_acf())

### Trace Plots

![](images/clipboard-2853565372.png){width="313"}

### Autocorrelation Plot

![](images/clipboard-3030610644.png){width="344"}

### Interpretation

-   Assess chain mixing and convergence showing correlation of each draw with its lagged values across iterations.
-   Rapid decay of autocorrelation toward zero indicates: chains are mixing well and successive draws are relatively independent.
-   Age and BMI coefficients exhibited low autocorrelation after a few lags, supporting the reliability of posterior estimates.

This diagnostic confirms that the Bayesian model sampling was adequate and stable, ensuring valid inference from the posterior distributions reflecting MCMC chain mixing and efficiency, not relationships between parameters
:::

------------------------------------------------------------------------

### Posterior Draws

Posterior draws = 4000 (4 chains √ó 1000 iterations) for posterior predictive checks

-   The distribution of predictors after seeing the data.
-   The posterior is the conditional distribution of the parameter given the data
-   Subjectivity is present when it is based on the prior distribution which influences the conclusions.
-   The influence of the prior goes to zero as the sample size increases.

------------------------------------------------------------------------

### Posterior Results

::: panel-tabset
### Posterior Estimates

![](images/clipboard-1727290712.png)

### Posterior Distribution

![](images/clipboard-2417602744.png)

### Interpretation

**Posterior Coefficients**

Age (per 1 SD): strong positive effect (‚âà +1.1, 80% CrI ‚âà 0.9‚Äì1.3).

BMI (per 1 SD): positive association (‚âà +0.6, 80% CrI ‚âà 0.45‚Äì0.75).

**Female: slightly protective (‚âà ‚Äì0.2, CrI spans slightly below zero).**

Mexican American: small positive effect (‚âà +0.25, wide CrI).

Other Hispanic: moderate positive effect (‚âà +0.35, wide CrI).

Non-Hispanic Black: clear positive effect (‚âà +0.75, CrI ‚âà 0.6‚Äì0.9).

Other/Multi: small positive effect (‚âà +0.3, wide CrI).
:::

------------------------------------------------------------------------

### Prior and Posterior Predictive Distribution (age and BMI)

::: panel-tabset
### Code

``` r
library(ggplot2)

ggplot(combined_draws, aes(x = estimate, fill = type)) +
  geom_density(alpha = 0.4) +
  facet_wrap(~ term, scales = "free", ncol = 2) +
  theme_minimal(base_size = 13) +
  labs(
    title = "Prior vs Posterior Distributions",
    x = "Coefficient estimate",
    y = "Density",
    fill = ""
  )
```

### Plot

Prior vs Posterior Distributions ![](images/clipboard-1978292710.png){width="466"}

### Interpretation

-   Strong positive relationship between age, BMI, and diabetes probability.

-   **Posterior predictive checks** confirm good model fit.

-   Imputation reduced bias and improved robustness.

### R Square

```         
Model fit: bayes_R2(bayes_fit)
```

```         
    Estimate        Est.Error       Q2.5        Q97.5 
    R2 0.1313342    0.01265055    0.1064607     0.156078
```
:::

------------------------------------------------------------------------

### Proportion of Diabetes Prevalence (Survey weighted NHANES vs Predicted)

![](images/clipboard-4013951629.png){width="576"}

![](images/clipboard-3433503019.png){width="466"}

------------------------------------------------------------------------

### Posterior Predictive Checks

500 draws: predicted outcomes for each observation compare ( observed data vs predeicted outcomes: binary outcome with 0 or 1 simulated

::: panel-tabset
### Code

``` r
pp_samples <- posterior_predict(bayes_fit, ndraws = 500)  # 500 draws

# Check dimensions
dim(pp_samples)  # rows = draws, cols = observations
ppc_bars(y = adult_imp1$diabetes_dx, yrep = pp_samples[1:500, ])
```

### Compare Outcome

**Predicted vs observed outcome**

![](images/clipboard-2418526955.png){width="274"}

### Proportion

![](images/clipboard-122565288.png){width="273"}

### Mean

![](images/clipboard-2726352750.png){width="340"}

### SD

![](images/clipboard-1023519991.png){width="331"}
:::

------------------------------------------------------------------------

### Observed vs. Predicted Average diabetes outcome across individuals

::: panel-tabset
### Code

``` r
# PP checks with bayesplot options
color_scheme_set("blue")
ppc_scatter_avg(y = adult_imp1$diabetes_dx, yrep = pp_samples[1:100, ]) +
labs(title = "Observed vs Predicted (Avg) Posterior Predictive at individual level")
```

### Plot

![](images/clipboard-2608056459.png){width="287"}

-   Each point = one observation (an individual)
-   **x-axis = observed value** (`0` or `1` in your binary diabetes variable)
-   **y-axis = average predicted posterior probability** for that same individual across simulated datasets
-   Points near (0, 0) or (1, 1) ‚Üí good prediction

### Plot

![](images/clipboard-2167517988.png){width="340"}

-   Each point represents an individual‚Äôs observed BMI versus the model‚Äôs predicted mean.
-   Error bars indicate the 95% credible intervals of the predictions.
-   The plot demonstrates that the model‚Äôs predictions generally align with the observed data,
:::

------------------------------------------------------------------------

### Model Diagnostics

#### Model Comparison Across methods

(1) **Survey-weighted maximum likelihood estimation (MLE)**
(2) **Bayesian regression**

``` r
svy_tbl   <- svy_or   %>% mutate(Model = "Survey-weighted MLE")
bayes_tbl <- bayes_or %>% mutate(Model = "Bayesian")
```

![](images/clipboard-329828759.png){width="363"}

-   Age and BMI - strong risk factors for both models for diabetes in this population.
-   The Bayesian model complements frequentist approaches by providing stable, interpretable, and uncertainty-quantified estimates, while broadly reproducing population-level prevalence.
-   The Bayesian model used normalized weights, which approximates the effect of survey weights but does not fully account for stratification, clustering, or design-based variance adjustments.

------------------------------------------------------------------------

### Model Diagnostics

::: panel-tabset
### Plot pairwise correlation

![](images/clipboard-1737198687.png)

### Code

``` r
# Extract posterior draws
post <- as_draws_df(bayes_fit)
# Select numeric parameters of interest
post_subset <- post %>% 
  dplyr::select(b_age_c, b_bmi_c, b_sexFemale, 
                b_raceMexicanAmerican, b_raceNHBlack)
# Compute correlation matrix
cor_matrix <- cor(post_subset)
# Visualize
mcmc_pairs(as_draws_array(bayes_fit), 
           pars = c("b_age_c", "b_bmi_c", "b_sexFemale"),
           off_diag_args = list(size = 1.5, alpha = 0.4))
```

### Interpretation

-   The absence of strong linear relationships between age, BMI, and sex
-   Independently contribute to the prediction of diabetes status.
-   The smooth, unimodal histograms along the diagonals confirmed stable model convergence and well-behaved posterior samples.
-   A mild negative tilt between b_age_c and b_sexFemale: a slight negative correlation:
-   As the posterior estimate for age increases, the effect of being female slightly decreases.
-   Weak correlation: confirms both predictors contribute distinct information to the diabetes outcome.
:::

------------------------------------------------------------------------

### Model Diagnostics

#### Predicted Diabetes Prevalence vs. NHANES Prevalence

::: panel-tabset
### Plot

![](images/clipboard-2142249375.png){width="329"}

### Interpretation

-   We compared the Bayesian posterior-predicted diabetes prevalence with the NHANES survey-weighted estimate (8.9%, SE = 0.0048).
-   The model‚Äôs posterior mean was 10.95%, with a 95% credible interval of 8.5%‚Äì12.8%. Although the model predicts slightly higher prevalence, the credible interval overlaps the NHANES estimate.
-   This indicates that the model is reasonably well-calibrated, with NHANES falling near the lower end of the posterior distribution but still within a plausible range.

### Code

``` r
# pp_proportion <- rowMeans(pp_samples)  # if not already done
known_prev <- 0.089   # NHANES prevalence
posterior_mean <- mean(pp_proportion)
posterior_ci <- quantile(pp_proportion, c(0.025, 0.975))  # 95% credible interval
pp_df <- tibble(proportion = pp_proportion)
ggplot(pp_df, aes(x = proportion)) +
  geom_histogram(binwidth = 0.005, fill = "skyblue", color = "black") +
  geom_vline(xintercept = known_prev, color = "red", linetype = "dashed", size = 1) +
  geom_vline(xintercept = posterior_mean, color = "blue", linetype = "solid", size = 1) +
  labs(
    title = "Posterior Predicted Diabetes Proportion vs NHANES Prevalence",
    subtitle = paste0("Red dashed = NHANES prevalence (", known_prev, 
                      "), Blue solid = Posterior mean (", round(posterior_mean,3), ")"),
    x = "Proportion of Diabetes = 1",
    y = "Frequency"
  ) +
  theme_minimal()
```
:::

------------------------------------------------------------------------

### Assumptions for Bayesian Logistic Regression

::: nonincremental
-   Data Binary outcome
-   Independent observations
-   Relationship Linear in logit
-   No perfect collinearity
-   Priors Properly chosen: informative enough
-   Posterior Proper and convergent Fit
-   No complete separation
-   Good predictive checks
:::

------------------------------------------------------------------------

### Translational Perspective

::: panel-tabset
### Internal Validation

#### Personalized Risk Estimation

-   Model enables individualized diabetes risk prediction using age, BMI, sex, race and outputs can guide actionable thresholds (e.g., risk \>30%)
-   BMI is modifiable- interventions can lower risk while sex and race remain constant.
-   Selected one participant (adult\[1, \]) with all covariates (age, BMI, sex, race) obtain predicted probabilities from the logistic regression model. **posterior_linpred(transform = TRUE)**
-   Posterior draws: calculate the 95% credible interval for the predicted probability

### Code

``` r
participant1_data  <- adult[1, ]
phat1 <- posterior_linpred(bayes_fit, newdata = participant1_data, transform = TRUE)
# Store in a data frame for plotting
post_pred_df <- data.frame(pred = phat1)
ci_95_participant1 <- quantile(phat1, c(0.025, 0.975))

ggplot(post_pred_df, aes(x = pred)) + 
  geom_density(color='darkblue', fill='lightblue') +
  geom_vline(xintercept = ci_95_participant1[1], color='red', linetype='dashed') +
  geom_vline(xintercept = ci_95_participant1[2], color='red', linetype='dashed') +
  xlab('Probability of being diabetic (Outcome=1)') +
  ggtitle('Posterior Predictive Distribution 95% Credible Interval') +
  theme_bw()
  
```

### Interpretation

Posterior predictive distribution for a specific individual with Outcome = 1

-   Median/mean predicted probability ‚âà 0.25
-   The peak of the density curve is around 25%: participant has a 1 in 4 chance of being diabetic.
-   95% credible interval ‚âà 0.20 to 0.31
-   The red dashed lines represent the 95% credible interval (CI): Lower bound ‚âà 0.20 Upper bound ‚âà 0.31

**Given the model and the participant‚Äôs characteristics, there is a 95% probability that this participant‚Äôs true diabetes risk lies between 20% and 31%**
:::

------------------------------------------------------------------------

### Limitations

-   **Cross-sectional** NHANES design limits causal inference.

-   Multiple **imputation** assumes MAR; MNAR may bias results.

-   Residual **confounding** from unmeasured factors (diet, activity, SES, genetics).

-   Bayesian estimates may be influenced by **prior choices**.

-   **BMI measured once**; may not capture long-term exposure.

-   **Self-reported** variables may introduce recall/reporting bias.

-   **Interaction effects** (e.g., age √ó BMI) not tested. :::

------------------------------------------------------------------------

### Conclusion

Bayesian logistic regression effectively models uncertainty.

MICE improved data completeness and reliability.

Posterior predictions provide interpretable probabilities of diabetes risk.

Framework adaptable to other outcomes (e.g., hypertension, obesity).

------------------------------------------------------------------------

### References

van Buuren, S. & Groothuis-Oudshoorn, K. (2011). MICE: Multivariate Imputation by Chained Equations in R.

Gelman, A. & Hill, J. (2007). Data Analysis Using Regression and Multilevel/Hierarchical Models.

McElreath, R. (2020). Statistical Rethinking: A Bayesian Course with Examples in R and Stan.

NHANES Data Documentation (CDC).

Van de Schoot, R. et al. (2013, 2021). Weakly Informative Priors in Bayesian Regression.

------------------------------------------------------------------------

### **Thank You**

**Dr. Ashraf Cohen, PhD, MS**

-   University of West Florida
-   Department of Mathematics and Statistics

------------------------------------------------------------------------
