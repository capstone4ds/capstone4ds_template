---
title: "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013â€“2014)"
subtitle: "Capstone Presentation"
author:
  - "Namita Mishra"
  - "Autumn Wilcox"
advisor: "Dr. Ashraf Cohen"
format:
  revealjs:
    theme: simple
    slide-number: true
    transition: slide
    css: docs/slides.css
    incremental: true
    code-fold: true
    code-summary: "Show the code"
bibliography: references.bib
execute:
  warning: false
  message: false
  echo: false
---

```{=html}
<style>
.reveal {
  font-size: 34px; /* Change all slide text */
}
.reveal h1 {
  font-size: 48px; /* Title font */
}
</style>
```

### Aim

-   Early identification of risk factors is key to diabetes diagnosis and prevention.

-   Predict diabetes using Bayesian logistic regression.

### Frequentist Methods

-   Maximum likelihood estimation is unstable with missing data, quasi-separation, or small samples.

-   Probability is interpreted as long-run relative frequency.

------------------------------------------------------------------------

### Bayesian Approach

-   Flexible and regularizes estimates.

-   Quantifies uncertainty under missingness or imputation (Baldwin & Larson, 2017; Kruschke & Liddell, 2017).

-   Incorporates priors and provides credible intervals via MCMC to predict patient health outcomes.

-   Supports model checking, variable selection, and uncertainty quantification.

-   Probability represents a degree of belief.

------------------------------------------------------------------------

### Bayesian Model

::: panel-tabset
### Bayesâ€™ theorem

**Bayesâ€™ theorem** is based on conditional probability.

Coefficients estimated using posterior means and 95% credible intervals.

Logistic link function used for binary outcomes.

### Bayesian Logistic Regression

logit (ğ‘ƒ(ğ‘Œ=1))=ğ›½0+ğ›½1age ğ‘ + ğ›½ 2 bmi ğ‘ + ğ›½ 3 sex + ğ›½ 4 race

Intercept prior: student_t(3, 0, 10) â€” heavy tails for flexibility (Van de Schoot et al., 2013).

Regression coefficients prior: normal(0, 2.5) â€” weakly informative, constraining extreme values (Van de Schoot et al., 2021).
:::

------------------------------------------------------------------------

**Bayesian Logistic Regression**

In **R** using brms **(Stan backend)**

Posterior draws = 4000 (4 chains Ã— 1000 iterations).

``` r
library(gt)
priors <- c(
  set_prior("normal(0, 2.5)", class = "b"),
  set_prior("student_t(3, 0, 10)", class = "Intercept")
)

bayes_fit <- brm(
  formula = diabetes_dx | weights(wt_norm) ~ age_c + bmi_c + sex + race,
  data    = adult_imp1,
  family  = bernoulli(link = "logit"),
  prior   = priors,
  chains  = 4, iter = 2000, seed = 123,
  control = list(adapt_delta = 0.95),
  refresh = 0
)
```

------------------------------------------------------------------------

### Data Source

### **NHANES 2013â€“2014 (Survey weighted)**

:::: panel-tabset
### Exploratory data analysis

::: nonincremental
-   Missing Data Assessment
-   Overall missingness â‰ˆ 4%
-   No variable is completely missing.
-   Missingness likely MAR (Missing At Random).
-   Clustered mainly in BMI (4.3%) and diabetes_dx (3.1%).
:::

### Missing data analysis {.panel-tabset}

![](images/clipboard-2873800077.png)
::::

------------------------------------------------------------------------

### Multiple Imputation by Chained Equations (MICE)

::: nonincremental
Handling Missing Data *(van Buuren & Groothuis-Oudshoorn, 2011; van Buuren, 2012*)

**Fit logistic regression model**

MICE performed on imputed datasets and pool estimates:

``` r
fit_mi <- with(data = imp, exp = glm(diabetes_dx ~ age_c + bmi_c + sex + race, family = binomial()))
pool_mi <- pool(fit_mi)
```

-   Iteratively imputes incomplete variables using regression models.
-   PMM for continuous; logistic regression for binary variables.
-   5 imputations Ã— 10 iterations ensure stability and convergence.
-   Estimates pooled using Rubinâ€™s rules.
:::

------------------------------------------------------------------------

### Bayesian Model Diagnostics

:::: panel-tabset
### MCMC

-   Trace plots indicate convergence (no drift across iterations).
-   Rhat â‰ˆ 1 â†’ good chain mixing.
-   Effective sample size is adequate across parameters.

### Posterior Estimates

::: nonincremental
-   Intercept -2.66 \[-2.84, -2.50\] Baseline log-odds
-   Age_c 1.09 \[0.97, 1.22\] â†‘ Age increases diabetes risk
-   BMI_c 0.88 \[0.76, 1.01\] Higher BMI predicts diabetes
-   All predictors show positive associations.
-   Narrow credible intervals â†’ precise estimates (Predictor Estimate 95% CI )
-   CIs exclude zero â†’ statistically significant effects.
:::
::::

------------------------------------------------------------------------

### Posterior Predictive Distribution

::::: panel-tabset
### code

``` r
library(ggplot2)

ggplot(combined_draws, aes(x = estimate, fill = type)) +
  geom_density(alpha = 0.4) +
  facet_wrap(~ term, scales = "free", ncol = 2) +
  theme_minimal(base_size = 13) +
  labs(
    title = "Prior vs Posterior Distributions",
    x = "Coefficient estimate",
    y = "Density",
    fill = ""
  )
```

### Plot

![](images/clipboard-1978292710.png){width="368"}

### Interpretation

:::: nonincremental
-   Strong positive relationship between age, BMI, and diabetes probability.

-   **Posterior predictive checks** confirm good model fit.

-   Imputation reduced bias and improved robustness.

::: nonincremental
```         
Model fit: bayes_R2(bayes_fit)
```
:::

```         
        Estimate        Est.Error       Q2.5        Q97.5 
        R2 0.1313342    0.01265055    0.1064607     0.156078
```
::::
:::::

------------------------------------------------------------------------

### Assumptions for Bayesian Logistic Regression

::: nonincremental
-   Data Binary outcome
-   independent observations
-   Relationship Linear in logit
-   no perfect collinearity
-   Priors Properly chosen: informative enough
-   Posterior Proper and convergent Fit
-   No complete separation
-   good predictive checks
:::

------------------------------------------------------------------------

### Limitations

:::: nonincremental
::: nonincremental
-   NHANES is cross-sectional â†’ cannot infer causality

-   Role of Potential unmeasured confounding factors (diet, activity)

-   Limited predictors simplify model structure

-   Imputation assumes MAR â†’ violation may bias estimates
:::
::::

------------------------------------------------------------------------

### Conclusion

Bayesian logistic regression effectively models uncertainty.

MICE improved data completeness and reliability.

Posterior predictions provide interpretable probabilities of diabetes risk.

Framework adaptable to other outcomes (e.g., hypertension, obesity).

------------------------------------------------------------------------

### References

::: nonincremental
van Buuren, S. & Groothuis-Oudshoorn, K. (2011). MICE: Multivariate Imputation by Chained Equations in R.

-   Gelman, A. & Hill, J. (2007). Data Analysis Using Regression and Multilevel/Hierarchical Models.

-   McElreath, R. (2020). Statistical Rethinking: A Bayesian Course with Examples in R and Stan.

-   NHANES Data Documentation (CDC).

-   Van de Schoot, R. et al. (2013, 2021). Weakly Informative Priors in Bayesian Regression.
:::

------------------------------------------------------------------------

:::: panel-tabset
### **Thank You**

### Dr. Ashraf Cohen, PhD, MS

::: nonincremental
-   University of West Florida
-   Department of Mathematics and Statistics
-   Thanks for the guidance
:::

------------------------------------------------------------------------
::::
