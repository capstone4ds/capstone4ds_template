---
title: "Bayesian Logistic Regression for Diabetes Risk (NHANES 2013â€“2014)"
subtitle: "Capstone Presentation"
author:
  - "Namita Mishra"
  - "Autumn Wilcox"
advisor: "Dr. Ashraf Cohen"
format:
  revealjs:
    theme: simple
    slide-number: true
    transition: slide
    css: docs/slides.css
    incremental: false
    code-fold: true
    code-summary: "Show the code"
bibliography: references.bib
execute:
  warning: false
  message: false
  echo: false
---

```{=html}
<style>
.reveal {
  font-size: 34px; /* Change all slide text */
}
.reveal h1 {
  font-size: 48px; /* Title font */
}
</style>
```

### Aim

-   Early identification of risk factors is key to diabetes diagnosis and prevention.

-   Predict diabetes using Bayesian logistic regression.

### Frequentist Methods

-   Maximum likelihood estimation is unstable with missing data, quasi-separation, or small samples.

-   Probability is interpreted as long-run relative frequency.

-   To the frequentist, parameter is an unknown constant.

------------------------------------------------------------------------

### Bayesian Approach

-   Flexible and regularizes estimates.
-   Quantifies uncertainty under missingness or imputation (Baldwin & Larson, 2017; Kruschke & Liddell, 2017).
-   Incorporates priors and provides credible intervals via MCMC to predict patient health outcomes.
-   Don't know the value of the parameter, it's a random variable
-   We model our uncertainty with a probability distribution ![](images/clipboard-2228883614.png){width="37" height="20"}, called the prior distribution.
-   Probability represents a degree of belief.
-   It represents the statisticians belief about before observing the data.
-   Supports model checking, variable selection, and uncertainty quantification

------------------------------------------------------------------------

### Bayesian Model

::: panel-tabset
### Bayesâ€™ theorem

**Bayesâ€™ theorm** is based on conditional probability.

### Bayesian Inference

Bayesian inference estimates the **posterior probability** of a parameter:

$$
P(\theta \mid D) = \frac{P(D \mid \theta)\, P(\theta)}{P(D)}
$$

where:

-   ( P(\theta \mid D) ): Posterior â€” probability of the parameter given data\
-   ( P(D \mid \theta) ): Likelihood â€” probability of observing data given the parameter\
-   ( P(\theta) ): Prior â€” belief about the parameter before seeing data\
-   ( P(D) ): Marginal likelihood (normalizing constant)
:::

------------------------------------------------------------------------

**Bayesian regression and Coefficient estimation**

-   Logistic link function used for binary outcomes.
-   logit (ğ‘ƒ(ğ‘Œ=1))=ğ›½0+ğ›½1age ğ‘ + ğ›½ 2 bmi ğ‘ + ğ›½ 3 sex + ğ›½ 4 race
-   Intercept prior: student_t(3, 0, 10) â€” heavy tails for flexibility (Van de Schoot et al., 2013).
-   Regression coefficients prior: normal(0, 2.5) â€” weakly informative, constraining extreme values (Van de Schoot et al., 2021).

------------------------------------------------------------------------

### Data Source

::: panel-tabset
### Data

National Health and Nutrition Examination Survey (NHANES) 2013â€“2014\
**Design:** Complex multistage probability sampling; survey weights applied. **Response Variable** - **diabetes_dx (binary)**\
- Based on **DIQ010**: â€œHas a doctor told you that you have diabetes?â€\
- Excluded **DIQ050 (insulin use)** to avoid clinical confounding. **Predictor Variables** - **RIDAGEYR â€” Age (continuous)** - adults 20â€“80 years (per 1 SD)\
- **BMXBMI â€” Body Mass Index (continuous)** 1 SD increase\
- **RIAGENDR â€” Sex (factor)**\
- **RIDRETH1 â€” Race/Ethnicity (factor)**\
- *Mexican American*, *Other Hispanic*, *Non-Hispanic White*,\
*Non-Hispanic Black*, *Other/Multi-racial*

### Data view

![](images/clipboard-3274096863.png){width="795"}
:::

------------------------------------------------------------------------

### Missing data analysis {.panel-tabset}

::: panel-tabset
### Plot

![](images/clipboard-2873800077.png){width="427"}

### Abnormalities:

-   Missing Data Assessment - Overall missingness â‰ˆ 4%
-   No variable is completely missing
-   Missingness likely MAR (Missing At Random)
-   Clustered mainly in BMI (4.3%) and diabetes_dx (3.1%).
:::

------------------------------------------------------------------------

### Multiple Imputation by Chained Equations (MICE)

:::: nonincremental
*van Buuren & Groothuis-Oudshoorn, 2011; van Buuren, 2012*

Bayesian models assume complete data. **Multivariate Imputation by Chained Equations (MICE)** provides multiple realistic versions of missing data. Pooling across posterior distributions yields the correct full posterior:

Posterior (parameters + missing data uncertainty)

::: panel-tabset
### MICE

-   Iteratively imputes incomplete variables using regression models.
-   PMM for continuous; logistic regression for binary variables.
-   MICE procedure ran 5 iterations for each of the 5 imputed datasets to ensure stability and convergence.
-   Estimates pooled using Rubinâ€™s rules.

### Fit logistic regression model

``` r
fit_mi <- with(data = imp, exp = glm(diabetes_dx ~ age_c + bmi_c + sex + race, family = binomial()))
pool_mi <- pool(fit_mi)
```
:::
::::

------------------------------------------------------------------------

**Bayesian Logistic Regression**

A Bayesian logistic regression model was then fitted to each imputed dataset in **R** using brms **(Stan backend)**, 2000 itirations

-   Prior: **normal(0, 2.5)** for coefficients **student_t(3, 0, 10)** for Intercept

``` r
library(gt)
priors <- c(
  set_prior("normal(0, 2.5)", class = "b"),
  set_prior("student_t(3, 0, 10)", class = "Intercept")
)

bayes_fit <- brm(
  formula = diabetes_dx | weights(wt_norm) ~ age_c + bmi_c + sex + race,
  data    = adult_imp1,
  family  = bernoulli(link = "logit"),
  prior   = priors,
  chains  = 4, iter = 2000, seed = 123,
  control = list(adapt_delta = 0.95),
  refresh = 0
)
```

------------------------------------------------------------------------

### MCMC (Markov Chain Monte Carlo)

-   Algorithms used to draw samples from a probability distribution.
-   The Gibbs sampler produces a sequence of random vectors ![](images/clipboard-1640325740.png){width="90" height="35"}. Each one depends on the past only through the most recent one. It's a Markov process. The samples are not just random but are drawn in a way that the long-term distribution of the samples matches the target posterior distribution.
-   It uses a burn in period. The random vectors are sequentially dependent. Retain one parameter vector every n iterations, and discard the rest.

------------------------------------------------------------------------

### Posterior distribution

Posterior draws = 4000 (4 chains Ã— 1000 iterations) for posterior predeictive checks for -

-   The distribution of predictors after seeing the data.
-   The posterior is the conditional distribution of the parameter given the data
-   Subjectivity is present when it is based on the prior distribution which influences the conclusions.
-   The influence of the prior goes to zero as the sample size increases.

------------------------------------------------------------------------

### Bayesian Model Diagnostics

:::: panel-tabset
### MCMC

-   Trace plots indicate convergence (no drift across iterations).
-   Rhat â‰ˆ 1 â†’ good chain mixing.
-   Effective sample size is adequate across parameters.

### Posterior Estimates

::: nonincremental
-   Intercept -2.66 \[-2.84, -2.50\] Baseline log-odds
-   Age_c 1.09 \[0.97, 1.22\] â†‘ Age increases diabetes risk
-   BMI_c 0.88 \[0.76, 1.01\] Higher BMI predicts diabetes
-   All predictors show positive associations.
-   Narrow credible intervals â†’ precise estimates (Predictor Estimate 95% CI )
-   CIs exclude zero â†’ statistically significant effects.
:::
::::

------------------------------------------------------------------------

### Posterior Predictive Distribution

::: panel-tabset
### Code

``` r
library(ggplot2)

ggplot(combined_draws, aes(x = estimate, fill = type)) +
  geom_density(alpha = 0.4) +
  facet_wrap(~ term, scales = "free", ncol = 2) +
  theme_minimal(base_size = 13) +
  labs(
    title = "Prior vs Posterior Distributions",
    x = "Coefficient estimate",
    y = "Density",
    fill = ""
  )
```

### Plot

Prior vs Posterior Distributions ![](images/clipboard-1978292710.png){width="466"}

### Interpretation

-   Strong positive relationship between age, BMI, and diabetes probability.

-   **Posterior predictive checks** confirm good model fit.

-   Imputation reduced bias and improved robustness.

### R Square

```         
Model fit: bayes_R2(bayes_fit)
```

```         
    Estimate        Est.Error       Q2.5        Q97.5 
    R2 0.1313342    0.01265055    0.1064607     0.156078
```
:::

------------------------------------------------------------------------

**Posterior pairwise correlation plot**

::: panel-tabset
### Code

``` r

# Extract posterior draws
post <- as_draws_df(bayes_fit)

# Select numeric parameters of interest
post_subset <- post %>% 
  dplyr::select(b_age_c, b_bmi_c, b_sexFemale, 
                b_raceMexicanAmerican, b_raceNHBlack)

# Compute correlation matrix
cor_matrix <- cor(post_subset)

# Visualize
mcmc_pairs(as_draws_array(bayes_fit), 
           pars = c("b_age_c", "b_bmi_c", "b_sexFemale"),
           off_diag_args = list(size = 1.5, alpha = 0.4))
           
```

### Plot

![](images/clipboard-1737198687.png)

### Interpretation

The absence of strong linear relationships among parameters suggests that age, BMI, and sex independently contributed to the prediction of diabetes status. The smooth, unimodal histograms along the diagonals confirmed stable model convergence and well-behaved posterior samples.
:::

------------------------------------------------------------------------

### Model Check: Comparison of Odds Ratios Across Models

![](images/clipboard-329828759.png){width="363"}

-   Age and BMI are consistently strong risk factors for diabetes in this population.
-   The Bayesian model complements frequentist approaches by providing stable, interpretable, and uncertainty-quantified estimates, while broadly reproducing population-level prevalence.

------------------------------------------------------------------------

### Assumptions for Bayesian Logistic Regression

::: nonincremental
-   Data Binary outcome
-   Independent observations
-   Relationship Linear in logit
-   No perfect collinearity
-   Priors Properly chosen: informative enough
-   Posterior Proper and convergent Fit
-   No complete separation
-   Good predictive checks
:::

----------------------------------------------

### Limitations

::::: nonincremental
-   **Cross-sectional** NHANES design limits causal inference.

-   Multiple **imputation** assumes MAR; MNAR may bias results.

-   Residual **confounding** from unmeasured factors (diet, activity, SES, genetics).

-   Bayesian estimates may be influenced by **prior choices**.

-   **BMI measured once**; may not capture long-term exposure.

-   **Self-reported** variables may introduce recall/reporting bias.

-   **Interaction effects** (e.g., age Ã— BMI) not tested.

:::::


------------------------------------------------------------------------

### Conclusion

Bayesian logistic regression effectively models uncertainty.

MICE improved data completeness and reliability.

Posterior predictions provide interpretable probabilities of diabetes risk.

Framework adaptable to other outcomes (e.g., hypertension, obesity).

------------------------------------------------------------------------

### References

van Buuren, S. & Groothuis-Oudshoorn, K. (2011). MICE: Multivariate Imputation by Chained Equations in R.

Gelman, A. & Hill, J. (2007). Data Analysis Using Regression and Multilevel/Hierarchical Models.

McElreath, R. (2020). Statistical Rethinking: A Bayesian Course with Examples in R and Stan.

NHANES Data Documentation (CDC).

Van de Schoot, R. et al. (2013, 2021). Weakly Informative Priors in Bayesian Regression.

------------------------------------------------------------------------

### **Thank You**

**Dr. Ashraf Cohen, PhD, MS**

-   University of West Florida
-   Department of Mathematics and Statistics

------------------------------------------------------------------------

